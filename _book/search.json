[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"book self study second edition book Introduction Statistical Learning Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.","code":""},{"path":"chapter-1.html","id":"chapter-1","chapter":"1 Introduction","heading":"1 Introduction","text":"Chapter 1 provides brief overview statistical learning, refers vast set tools understanding data. tools statistical learning can classified either supervised unsupervised.Supervised statistical learning involves building statistical model predicting output (called response dependent variable) based one inputs (called features, predictors, independent variables). typical supervised learning setting access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations, response \\(Y\\) also measured \\(n\\) observations. goal predict \\(Y\\) using \\(X_1, X_2, \\dots , X_p\\). response \\(Y\\) can quantitative (continuous, numerical) qualitative (categorical, non-numerical). goal predict numerical value, call regression problem. goal predict non-numerical value call classification problem. Supervised statistical learning can used exploratory confirmatory data analysis.Unsupervised statistical learning involves cases one inputs used learn relationships structure data absence output. typical unsupervised learning setting () access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations. goal discover interesting things measurements \\(X_1, X_2, \\dots , X_p\\). goal partition observations subgroups based (dis)similarity, call clustering problem. Unsupervised statistical learning typically used exploratory data analysis; possible check work unsupervised statistical learning (donâ€™t know true answer since problem unsupervised), used confirmatory data analysis.Semi-supervised statistical learning involves building statistical model predicting output based one inputs subset observations predictor measurements response measurement, wish incorporate observations model. typical semi-supervised learning setting access set \\(n\\) observations. \\(m\\) observations, \\(m < n\\), predictor measurements response measurement; remaining \\(n - m\\) observations predictor measurements response measurement. goal predict \\(Y\\) incorporating \\(m\\) observations response measurements available well \\(n - m\\) observations . topic beyond scope book explored .","code":""},{"path":"chapter-1.html","id":"notation-and-simple-matrix-algebra","chapter":"1 Introduction","heading":"1.1 Notation and Simple Matrix Algebra","text":"Chapter 1 also discusses notation conventions used textbook, starting Page 9. Briefly:\\(n\\) represents number observations sample\\(p\\) represents number variables available data set\\(x_{ij}\\) represents value \\(j\\)th variable \\(\\)th observation, \\(= 1, 2, \\dots, n\\) \\(j = 1, 2, \\dots, p\\).\\(\\mathbf X\\) represents \\(n \\times p\\) matrix whose \\((,j)\\)th element \\(x_{ij}\\)\\(x_i\\) represents rows matrix \\(\\mathbf X\\) \\(x_i\\) vector length \\(p\\) containing \\(p\\) variable measurements \\(\\)th observation.\\(\\mathbf x_j\\) represents columns matrix \\(\\mathbf X\\) \\(\\mathbf x_j\\) vector length \\(n\\) containing \\(n\\) observation measurements \\(j\\)th variable.\\(y_i\\) represents \\(\\)th observation response variable.\\(\\mathbf y\\) represents vector length \\(n\\) containing set response variable measurements predictions.","code":""},{"path":"chapter-1.html","id":"data-sets-used-in-labs-and-exercises","chapter":"1 Introduction","heading":"1.2 Data Sets Used in Labs and Exercises","text":"book uses data sets ISLR2 package (available CRAN) one data set included part base R distribution labs exercises.","code":""},{"path":"chapter-1.html","id":"book-website","chapter":"1 Introduction","heading":"1.3 Book Website","text":"website Introduction Statistical Learning located https://www.statlearning.org. contains number additional resources may useful.","code":""},{"path":"chapter-2.html","id":"chapter-2","chapter":"2 Statistical Learning","heading":"2 Statistical Learning","text":"Chapter 2 formalizes concept statistical learning introducing general statistical model used modelling relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), can written \\[\nY = f(X) + \\epsilon.\n\\tag{2.1}\n\\]:\\(Y\\) represents response variable data set\\(X\\) represents set variables data set\\(X_p\\) represents \\(p\\)th variable data set\\(f(\\dots)\\) represents fixed unknown function input(s)\\(\\epsilon\\) represents random error term independent \\(X\\) mean zeroThe goal statistical learning estimate \\(f\\). two main reasons estimating \\(f\\): prediction inference. Depending whether ultimate goal prediction, inference, combination two, different methods estimating \\(f\\) may appropriate. general, trade-prediction accuracy model interpretability. Models make accurate predictions tend less interpretable, models interpretable tend make less accurate predictions (although always case, due potential overfitting highly flexible models).methods use estimate \\(f\\) can characterized either parametric non-parametric. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly.","code":""},{"path":"chapter-2.html","id":"prediction","chapter":"2 Statistical Learning","heading":"2.1 Prediction","text":"error term \\(\\epsilon\\) averages zero, general statistical model predicting \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\) can written \\[\n\\hat Y = \\hat f(X).\n\\tag{2.2}\n\\]:\\(\\hat Y\\) represents resulting prediction \\(Y\\)\\(\\hat f\\) represents estimate \\(f\\)goal predict, typically need concern exact form \\(\\hat f\\) provided accurately predicts \\(Y\\). accuracy \\(\\hat Y\\) prediction \\(Y\\) depends two sources error: reducible error irreducible error. error model attributable \\(\\hat f\\) reducible can potentially improve accuracy \\(\\hat f\\) estimating \\(f\\) using appropriate statistical learning technique. However, error model attributable \\(\\epsilon\\) irreducible \\(Y\\) also function \\(\\epsilon\\), \\(\\epsilon\\) independent \\(X\\), matter well estimate \\(f\\), variability associated \\(\\epsilon\\) still present model. variability may come unmeasured variables useful predicting \\(Y\\), unmeasurable variation. Irreducible error places (often unknowable) upper bound accuracy prediction \\(Y\\).","code":""},{"path":"chapter-2.html","id":"inference","chapter":"2 Statistical Learning","heading":"2.2 Inference","text":"goal understand relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), need concern exact form \\(\\hat f\\). form \\(\\hat f\\) can used identify:predictors associated responsethe direction (positive negative) form (simple complex) relationship response predictor","code":""},{"path":"chapter-2.html","id":"assessing-model-accuracy","chapter":"2 Statistical Learning","heading":"2.3 Assessing Model Accuracy","text":"one statistical learning approach performs better approaches possible data sets. , care needs taken choose approach use given data set produce best results. number important concepts arise selecting statistical learning approach specific data set:Measuring quality fitThe bias-variance trade-","code":""},{"path":"chapter-2.html","id":"in-the-regression-setting","chapter":"2 Statistical Learning","heading":"2.3.1 In the Regression Setting","text":"regression setting, commonly used quality fit measure training data mean squared error \\(\\mathit{MSE}\\), given \\[\n\\mathit{MSE}_{\\mathrm{training}} = \\frac 1 n \\sum_{= 1}^n (y_i - \\hat f(x_i))^2,\n\\tag{2.3}\n\\]\\(\\hat f(x_i)\\) represents prediction \\(\\hat f\\) gives \\(\\)th observation. predicted responses close true responses \\(\\mathit{MSE}\\) small; predicted responses far true responses \\(\\mathit{MSE}\\) large. generally really care value accurately predicting data already seen particularly useful.goal assess accuracy predictions apply method previously unseen test data, can compute mean squared error test observations, given \\[\n\\mathit{MSE}_{\\mathrm{testing}} = \\frac 1 n \\sum_{= 1}^n (y_0 - \\hat f(x_0))^2,\n\\tag{2.4}\n\\]\\((x_0, y_0)\\) previously unseen test observation used train statistical learning model. want choose model gives lowest test \\(\\mathit{MSE}\\) minimizing distance \\(\\hat f(x_0)\\) \\(y_0\\). test data set available can simply evaluate Equation (2.4) choose statistical learning model \\(\\mathit{MSE}\\) smallest. test data set available can use cross-validation, method estimating test \\(\\mathit{MSE}\\) using training data set.expected test \\(\\mathit{MSE}\\) given value \\(x_0\\) can always decomposed sum three fundamental quantities: variance \\(\\hat f(x_0)\\), squared bias \\(\\hat f(x_0)\\), variance error term \\(\\epsilon\\), written \\[\nE \\bigl(y_0 - \\hat f(x_0) \\bigr)^2 = \\mathrm{Var}(\\hat f(x_0)) +\n                                     [\\mathrm{Bias(\\hat f(x_0))}]^2 +\n                                     \\mathrm{Var}(\\epsilon),\n\\tag{2.5}\n\\]notation \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) defines expected test \\(\\mathit{MSE}\\) \\(x_0\\). overall expected test \\(\\mathit{MSE}\\) given averaging \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) possible values \\(x_0\\) test data set.variance \\(\\hat f\\) refers amount \\(\\hat f\\) change estimated using different training set. bias \\(\\hat f\\) refers error introduced approximating real-life problem much simpler model. general, models become flexible, variance increase bias decrease. relative rate change variance bias determines whether test \\(\\mathit{MSE}\\) increases decreases. variance bias can change different rates different data sets, challenge lies finding model variance bias lowest.","code":""},{"path":"chapter-2.html","id":"in-the-classification-setting","chapter":"2 Statistical Learning","heading":"2.3.2 In the Classification Setting","text":"classification setting, commonly used quality fit measure training data error rate, given \\[\n\\frac 1 n \\sum_{= 1}^n (y_i \\ne \\hat y_i).\n\\tag{2.6}\n\\]:\\(\\hat y_i\\) predicted class label \\(\\)th observation using \\(\\hat f\\)\\((y_i \\ne \\hat y_i)\\) indicator variable equals one \\(y_i \\ne \\hat y_i\\) (incorrectly classified) zero \\(y_i = \\hat y_i\\) (correctly classified)test error rate set test observations \\((x_0, y_0)\\) given \\[\n\\frac 1 n \\sum_{= 1}^n (y_0 \\ne \\hat y_0),\n\\tag{2.7}\n\\]\\(\\hat y_0\\) predicted class label results applying classifier test observation predictor \\(x_0\\).bias-variance trade-present classification setting ; variance bias lowest test error rate smallest given data set.","code":""},{"path":"chapter-2.html","id":"exercises","chapter":"2 Statistical Learning","heading":"2.4 Exercises","text":"","code":""},{"path":"chapter-2.html","id":"conceptual","chapter":"2 Statistical Learning","heading":"Conceptual","text":"Exercise 2.1  parts () (d), indicate whether generally expect performance flexible statistical learning method better worse inflexible method. Justify answer.sample size \\(n\\) extremely large, number predictors \\(p\\) small.Answer. Better. flexible model generally able better estimate true \\(f\\) avoid overfitting extremely large sample size small number predictors. exception true \\(f\\) linear, inflexible model generally perform better; however, real world relationships linear, lower bias flexible model generally lead better quality fit.number predictors \\(p\\) extremely large, number observations \\(n\\) small.Answer. Worse. flexible model generally lead overfitting training data number predictors large sample size small. inflexible model less likely lead overfitting scenario, generally better job giving accurate predictions new observations flexible overfit model.relationship predictors response highly non-linear.Answer. Better. flexible model generally able fit highly non-linear relationship better inflexible model relative rate decrease bias tends much greater relative increase variance \\(f\\) highly non-linear. left right plots Figure 2.12 Page 36 book demonstrate nicely.variance error terms, .e.Â \\(\\sigma^2 = \\mathrm{Var}(\\epsilon)\\), extremely high.Answer. Worse. flexible model generally lead overfitting training data variance error terms extremely high. \\(Y\\) partly function \\(\\epsilon\\), variance error terms extremely high variance \\(Y\\) also extremely high, mainly due random error. flexible model tries find patterns noise likely pick patterns caused random chance rather true properties unknown function \\(f\\). bias inflexible model preferable situation, give stable predictions long run, likely preferable making essentially random predictions flexible model give circumstances.","code":""}]
