[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"book self study second edition book Introduction Statistical Learning Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.","code":""},{"path":"chapter-1.html","id":"chapter-1","chapter":"1 Introduction","heading":"1 Introduction","text":"Chapter 1 provides brief overview statistical learning, refers vast set tools understanding data. tools statistical learning can classified either supervised unsupervised.Supervised statistical learning involves building statistical model predicting output (called response dependent variable) based one inputs (called features, predictors, independent variables). typical supervised learning setting access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations, response \\(Y\\) also measured \\(n\\) observations. goal predict \\(Y\\) using \\(X_1, X_2, \\dots , X_p\\). response \\(Y\\) can quantitative (continuous, numerical) qualitative (categorical, non-numerical). goal predict numerical value, call regression problem. goal predict non-numerical value call classification problem. Supervised statistical learning can used exploratory confirmatory data analysis.Unsupervised statistical learning involves cases one inputs used learn relationships structure data absence output. typical unsupervised learning setting () access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations. goal discover interesting things measurements \\(X_1, X_2, \\dots , X_p\\). goal partition observations subgroups based (dis)similarity, call clustering problem. Unsupervised statistical learning typically used exploratory data analysis; possible check work unsupervised statistical learning (donâ€™t know true answer since problem unsupervised), used confirmatory data analysis.Semi-supervised statistical learning involves building statistical model predicting output based one inputs subset observations predictor measurements response measurement, wish incorporate observations model. typical semi-supervised learning setting access set \\(n\\) observations. \\(m\\) observations, \\(m < n\\), predictor measurements response measurement; remaining \\(n - m\\) observations predictor measurements response measurement. goal predict \\(Y\\) incorporating \\(m\\) observations response measurements available well \\(n - m\\) observations . topic beyond scope book explored .","code":""},{"path":"chapter-1.html","id":"notation-and-simple-matrix-algebra","chapter":"1 Introduction","heading":"1.1 Notation and Simple Matrix Algebra","text":"Chapter 1 also discusses notation conventions used textbook, starting Page 9. Briefly:\\(n\\) represents number observations sample\\(p\\) represents number variables available data set\\(x_{ij}\\) represents value \\(j\\)th variable \\(\\)th observation, \\(= 1, 2, \\dots, n\\) \\(j = 1, 2, \\dots, p\\).\\(\\mathbf X\\) represents \\(n \\times p\\) matrix whose \\((,j)\\)th element \\(x_{ij}\\)\\(x_i\\) represents rows matrix \\(\\mathbf X\\) \\(x_i\\) vector length \\(p\\) containing \\(p\\) variable measurements \\(\\)th observation.\\(\\mathbf x_j\\) represents columns matrix \\(\\mathbf X\\) \\(\\mathbf x_j\\) vector length \\(n\\) containing \\(n\\) observation measurements \\(j\\)th variable.\\(y_i\\) represents \\(\\)th observation response variable.\\(\\mathbf y\\) represents vector length \\(n\\) containing set response variable measurements predictions.","code":""},{"path":"chapter-1.html","id":"data-sets-used-in-labs-and-exercises","chapter":"1 Introduction","heading":"1.2 Data Sets Used in Labs and Exercises","text":"book uses data sets ISLR2 package (available CRAN) one data set included part base R distribution labs exercises.","code":""},{"path":"chapter-1.html","id":"book-website","chapter":"1 Introduction","heading":"1.3 Book Website","text":"website Introduction Statistical Learning located https://www.statlearning.org. contains number additional resources may useful.","code":""},{"path":"chapter-2.html","id":"chapter-2","chapter":"2 Statistical Learning","heading":"2 Statistical Learning","text":"Chapter 2 formalizes concept statistical learning introducing general statistical model used modelling relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), can written \\[\nY = f(X) + \\epsilon.\n\\tag{2.1}\n\\]:\\(Y\\) represents response variable data set\\(X\\) represents set variables data set\\(X_p\\) represents \\(p\\)th variable data set\\(f(\\dots)\\) represents fixed unknown function input(s)\\(\\epsilon\\) represents random error term independent \\(X\\) mean zeroThe goal statistical learning estimate \\(f\\). two main reasons estimating \\(f\\): prediction inference. Depending whether ultimate goal prediction, inference, combination two, different methods estimating \\(f\\) may appropriate. general, trade-prediction accuracy model interpretability. Models make accurate predictions tend less interpretable, models interpretable tend make less accurate predictions (although always case, due potential overfitting highly flexible models).methods use estimate \\(f\\) can characterized either parametric non-parametric. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly.","code":""},{"path":"chapter-2.html","id":"prediction","chapter":"2 Statistical Learning","heading":"2.1 Prediction","text":"error term \\(\\epsilon\\) averages zero, general statistical model predicting \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\) can written \\[\n\\hat Y = \\hat f(X).\n\\tag{2.2}\n\\]:\\(\\hat Y\\) represents resulting prediction \\(Y\\)\\(\\hat f\\) represents estimate \\(f\\)goal predict, typically need concern exact form \\(\\hat f\\) provided accurately predicts \\(Y\\). accuracy \\(\\hat Y\\) prediction \\(Y\\) depends two sources error: reducible error irreducible error. error model attributable \\(\\hat f\\) reducible can potentially improve accuracy \\(\\hat f\\) estimating \\(f\\) using appropriate statistical learning technique. However, error model attributable \\(\\epsilon\\) irreducible \\(Y\\) also function \\(\\epsilon\\), \\(\\epsilon\\) independent \\(X\\), matter well estimate \\(f\\), variability associated \\(\\epsilon\\) still present model. variability may come unmeasured variables useful predicting \\(Y\\), unmeasurable variation. Irreducible error places (often unknowable) upper bound accuracy prediction \\(Y\\).","code":""},{"path":"chapter-2.html","id":"inference","chapter":"2 Statistical Learning","heading":"2.2 Inference","text":"goal understand relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), need concern exact form \\(\\hat f\\). form \\(\\hat f\\) can used identify:predictors associated responsethe direction (positive negative) form (simple complex) relationship response predictor","code":""},{"path":"chapter-2.html","id":"assessing-model-accuracy","chapter":"2 Statistical Learning","heading":"2.3 Assessing Model Accuracy","text":"one statistical learning approach performs better approaches possible data sets. , care needs taken choose approach use given data set produce best results. number important concepts arise selecting statistical learning approach specific data set:Measuring quality fitThe bias-variance trade-","code":""},{"path":"chapter-2.html","id":"in-the-regression-setting","chapter":"2 Statistical Learning","heading":"2.3.1 In the Regression Setting","text":"regression setting, commonly used quality fit measure training data mean squared error \\(\\mathit{MSE}\\), given \\[\n\\mathit{MSE}_{\\mathrm{training}} = \\frac 1 n \\sum_{= 1}^n (y_i - \\hat f(x_i))^2,\n\\tag{2.3}\n\\]\\(\\hat f(x_i)\\) represents prediction \\(\\hat f\\) gives \\(\\)th observation. predicted responses close true responses \\(\\mathit{MSE}\\) small; predicted responses far true responses \\(\\mathit{MSE}\\) large. generally really care value accurately predicting data already seen particularly useful.goal assess accuracy predictions apply method previously unseen test data, can compute mean squared error test observations, given \\[\n\\mathit{MSE}_{\\mathrm{testing}} = \\frac 1 n \\sum_{= 1}^n (y_0 - \\hat f(x_0))^2,\n\\tag{2.4}\n\\]\\((x_0, y_0)\\) previously unseen test observation used train statistical learning model. want choose model gives lowest test \\(\\mathit{MSE}\\) minimizing distance \\(\\hat f(x_0)\\) \\(y_0\\). test data set available can simply evaluate Equation (2.4) choose statistical learning model \\(\\mathit{MSE}\\) smallest. test data set available can use cross-validation, method estimating test \\(\\mathit{MSE}\\) using training data set.expected test \\(\\mathit{MSE}\\) given value \\(x_0\\) can always decomposed sum three fundamental quantities: variance \\(\\hat f(x_0)\\), squared bias \\(\\hat f(x_0)\\), variance error term \\(\\epsilon\\), written \\[\nE \\bigl(y_0 - \\hat f(x_0) \\bigr)^2 = \\mathrm{Var}(\\hat f(x_0)) +\n                                     [\\mathrm{Bias(\\hat f(x_0))}]^2 +\n                                     \\mathrm{Var}(\\epsilon),\n\\tag{2.5}\n\\]notation \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) defines expected test \\(\\mathit{MSE}\\) \\(x_0\\). overall expected test \\(\\mathit{MSE}\\) given averaging \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) possible values \\(x_0\\) test data set.variance \\(\\hat f\\) refers amount \\(\\hat f\\) change estimated using different training set. bias \\(\\hat f\\) refers error introduced approximating real-life problem much simpler model. general, models become flexible, variance increase bias decrease. relative rate change variance bias determines whether test \\(\\mathit{MSE}\\) increases decreases. variance bias can change different rates different data sets, challenge lies finding model variance bias lowest.","code":""},{"path":"chapter-2.html","id":"in-the-classification-setting","chapter":"2 Statistical Learning","heading":"2.3.2 In the Classification Setting","text":"classification setting, commonly used quality fit measure training data error rate, given \\[\n\\frac 1 n \\sum_{= 1}^n (y_i \\ne \\hat y_i).\n\\tag{2.6}\n\\]:\\(\\hat y_i\\) predicted class label \\(\\)th observation using \\(\\hat f\\)\\((y_i \\ne \\hat y_i)\\) indicator variable equals one \\(y_i \\ne \\hat y_i\\) (incorrectly classified) zero \\(y_i = \\hat y_i\\) (correctly classified)test error rate set test observations \\((x_0, y_0)\\) given \\[\n\\frac 1 n \\sum_{= 1}^n (y_0 \\ne \\hat y_0),\n\\tag{2.7}\n\\]\\(\\hat y_0\\) predicted class label results applying classifier test observation predictor \\(x_0\\).bias-variance trade-present classification setting ; variance bias lowest test error rate smallest given data set.","code":""},{"path":"chapter-2.html","id":"exercises","chapter":"2 Statistical Learning","heading":"2.4 Exercises","text":"","code":""},{"path":"chapter-2.html","id":"prerequisites","chapter":"2 Statistical Learning","heading":"Prerequisites","text":"access data sets functions used complete Chapter 2 exercises, load following packages.","code":"\n# library(ISLR2)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(patchwork)"},{"path":"chapter-2.html","id":"conceptual","chapter":"2 Statistical Learning","heading":"Conceptual","text":"Exercise 2.1  parts () (d), indicate whether generally expect performance flexible statistical learning method better worse inflexible method. Justify answer.sample size \\(n\\) extremely large, number predictors \\(p\\) small.Answer. Better. flexible model generally able better estimate true \\(f\\) avoid overfitting extremely large sample size small number predictors. exception true \\(f\\) linear, inflexible model generally perform better; however, real world relationships linear, lower bias flexible model generally lead better quality fit.number predictors \\(p\\) extremely large, number observations \\(n\\) small.Answer. Worse. flexible model generally lead overfitting training data number predictors large sample size small. inflexible model less likely lead overfitting scenario, generally better job giving accurate predictions new observations flexible overfit model.relationship predictors response highly non-linear.Answer. Better. flexible model generally able fit highly non-linear relationship better inflexible model relative rate decrease bias tends much greater relative increase variance \\(f\\) highly non-linear. left right plots Figure 2.12 Page 36 book demonstrate nicely.variance error terms, .e.Â \\(\\sigma^2 = \\mathrm{Var}(\\epsilon)\\), extremely high.Answer. Worse. flexible model generally lead overfitting training data variance error terms extremely high. \\(Y\\) partly function \\(\\epsilon\\), variance error terms extremely high variance \\(Y\\) also extremely high, mainly due random error. flexible model tries find patterns noise likely pick patterns caused random chance rather true properties unknown function \\(f\\). bias inflexible model preferable situation, give stable predictions long run, likely preferable making essentially random predictions flexible model give circumstances.Exercise 2.2  Explain whether scenario classification regression problem, indicate whether interested inference prediction. Finally, provide \\(n\\) \\(p\\).collect set data top 500 firms US. firm record profit, number employees, industry, CEO salary. interested understanding factors affect CEO salary.Answer. Regression, inference, \\(n = 500\\), \\(p = 3\\).considering launching new product wish know whether success failure. collect data 20 similar products previously launched. product recorded whether success failure, price charged product, marketing budget, competition price, ten variables.Answer. Classification, prediction, \\(n = 20\\), \\(p = 13\\).interested predicting % change USD/Euro exchange rate relation weekly changes world stock markets. Hence collect weekly data 2012. week record % change USD/Euro, % change US market, % change British market, % change German market.Answer. Regression, prediction, \\(n = 52\\), \\(p = 3\\).Exercise 2.3  now revisit bias-variance decomposition.Provide sketch typical (squared) bias, variance, training error, test error, Bayes (irreducible) error curves, single plot, go less flexible statistical learning methods towards flexible approaches. \\(x\\)-axis represent amount flexibility method, \\(y\\)-axis represent values curve. five curves. Make sure label one.Answer.Explain five curves shape displayed part ().Answer. value measure part () changes different rates different directions go less flexible statistical learning methods towards flexible approaches. Broadly, curve different shape. Specifically:Bias refers error introduced approximating real-life problem much simpler model. general, bias decrease models become flexible, flexibility lead better approximations real-life problem, reducing error.Variance refers amount \\(\\hat f\\) change estimated using different training set. general, variance increase models become flexible, flexible model pick patterns training data better may differ training sets.Training error refers quality fit model training data. general, training error steadily increase models become flexible, flexibility allow model closely follow training data.Test error refers quality fit trained model test data. general, test error U-shaped bias-variance trade-.Irreducible error refers random error independent \\(X\\). irreducible error independent \\(X\\), remains stable regardless model flexibility.Exercise 2.4  now think real-life applications statistical learning.Describe three real-life applications classification might useful. Describe response, well predictors. goal application inference prediction? Explain answer.Answer.Predicting whether customer likely purchase items discounted web store. predictor whether likely purchase items; predictors purchase history discounted full price items, purchase frequency. goal prediction order determine whether show discounts customer less frequently.Predicting whether customer likely purchase items discounted web store. predictor whether likely purchase items; predictors purchase history discounted full price items, purchase frequency. goal prediction order determine whether show discounts customer less frequently.Understanding demographic groups enjoy Star Trek. response whether someone Trekkie; predictors various demographic variables age, gender identity, ethnicity, . goal inference whether Star Trek appeals specific groups enjoyable diverse audience.Understanding demographic groups enjoy Star Trek. response whether someone Trekkie; predictors various demographic variables age, gender identity, ethnicity, . goal inference whether Star Trek appeals specific groups enjoyable diverse audience.Understanding kind people like raisin cookies. response whether someone likes raisin cookies; predictors levels psychopathy sadism. goal inference whether people like raisin cookies secretly evil.Understanding kind people like raisin cookies. response whether someone likes raisin cookies; predictors levels psychopathy sadism. goal inference whether people like raisin cookies secretly evil.Describe three real-life applications regression might useful. Describe response, well predictors. goal application inference prediction? Explain answer.Answer.Predicting temperature tomorrow live. response temperature degrees Celsius; predictors average temperature live past seven days, latitude, longitude, altitude live, month year. goal prediction order decide outfit might wear tomorrow.Predicting temperature tomorrow live. response temperature degrees Celsius; predictors average temperature live past seven days, latitude, longitude, altitude live, month year. goal prediction order decide outfit might wear tomorrow.Understanding aspects video game lead people playing longer. response minutes spent playing game; predictors game genre, whether game original sequel, whether game singleplayer, multiplayer, , game developer. goal inference determine makes game engaging.Understanding aspects video game lead people playing longer. response minutes spent playing game; predictors game genre, whether game original sequel, whether game singleplayer, multiplayer, , game developer. goal inference determine makes game engaging.Predicting long take injury heal. response time heal; predictors injury severity, age, indicators physical health. goal prediction order give patients idea time recovery.Predicting long take injury heal. response time heal; predictors injury severity, age, indicators physical health. goal prediction order give patients idea time recovery.Describe three real-life applications cluster analysis might useful.Answer.Identifying whether brain activity similar within individuals. predictor variable repeated measure functional connectivity. goal inference order see whether brain activity recordings cluster together individual.Identifying whether brain activity similar within individuals. predictor variable repeated measure functional connectivity. goal inference order see whether brain activity recordings cluster together individual.Identifying whether patients depression can classified subgroups. predictor variables responses depression inventory. goal inference order understand whether depression inventory measures multiple types depression.Identifying whether patients depression can classified subgroups. predictor variables responses depression inventory. goal inference order understand whether depression inventory measures multiple types depression.Identifying whether species families animals identified biologists aligns speciesâ€™ genomes. predictor variable species genome. goal understanding whether clusters identified observation align clusters identified genetics.Identifying whether species families animals identified biologists aligns speciesâ€™ genomes. predictor variable species genome. goal understanding whether clusters identified observation align clusters identified genetics.Exercise 2.5  advantages disadvantages flexible (versus less flexible) approach regression classification? circumstances might flexible approach preferred less flexible approach? might less flexible approach preferred?Answer. advantage flexible approach less bias, model better approximates reality, typically better prediction. disadvantage higher variance potential overfitting. flexible model might preferred goal statistical learning prediction. less flexible approach might preferred goal statistical learning inference, simpler models tend easier understand.Exercise 2.6  Describe differences parametric non-parametric statistical learning approach. advantages parametric approach regression classification (opposed non-parametric approach)? disadvantages?Answer. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. advantage easier estimate set parameters fit entirely arbitrary function \\(f\\). disadvantage model choose usually match true unknown form \\(f\\), can lead poor estimates far true \\(f\\).Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly. advantage becomes easier accurately fit wider range possible shapes \\(f\\). However, disadvantage accurately requires much larger number observations since problem estimating \\(f\\) small number parameters.Exercise 2.7  table provides training data set containing six observations, three predictors, one qualitative response variable.Suppose wish use data set make prediction \\(Y\\) \\(X1 = X2 = X3 = 0\\) using K-nearest neighbours.Compute Euclidean distance observation test point, \\(X1 = X2 = X3 = 0\\).Answer.equation Euclidean distance given \\[\nd(p, q) = \\sqrt{ \\sum_{= 1}^n (q_i - p_i)^2},\n\\tag{2.8}\n\\]\\(d(p, q)\\) represents distance points \\(p\\) \\(q\\) \\(n\\) dimensional Euclidean space, \\(q_i\\) \\(p_i\\) represent Euclidean vectors, starting origin \\(n\\) dimensional Euclidean space.can computed R like .prediction \\(K = 1\\)? ?Answer. \\(K = 1\\) prediction test point whatever training observation closest test point. Observation 5 (Green) closest prediction test point Green.prediction \\(K = 3\\)? ?Answer. \\(K = 3\\) prediction test point given class highest estimated probability based three points training data closest test point. observations 5 (Green), 6 (Red), 2 (Red) closest, results probabilities 1/3 Green class 2/3 Red class. prediction test point Red.Bayes decision boundary problem highly non-linear, expect best value \\(K\\) large small? ?Answer. expect smaller value K give best predictions. small. \\(K = 1\\) KNN decision boundary overly flexible, large value \\(K\\) sufficiently flexible.","code":"\neuclidean_distance <- function(q, p) sqrt(sum((q - p)^2))\n\nobs_test <- c( 0, 0, 0)\n\nobs_train <- list(\n  obs_01 = c( 0, 3, 0),\n  obs_02 = c( 2, 0, 0),\n  obs_03 = c( 0, 1, 3),\n  obs_04 = c( 0, 1, 2),\n  obs_05 = c(-1, 0, 0),\n  obs_06 = c( 1, 1, 1)\n)\n\nlapply(obs_train, function(x) euclidean_distance(obs_test, x))\n#> $obs_01\n#> [1] 3\n#> \n#> $obs_02\n#> [1] 2\n#> \n#> $obs_03\n#> [1] 3.162278\n#> \n#> $obs_04\n#> [1] 2.236068\n#> \n#> $obs_05\n#> [1] 1\n#> \n#> $obs_06\n#> [1] 1.732051"},{"path":"chapter-2.html","id":"applied","chapter":"2 Statistical Learning","heading":"Applied","text":"Exercise 2.8  exercise relates ISLR2::College data set. contains number \nvariables 777 different universities colleges US. help page ?ISLR2::College data set contains description data set 18 variables measures.Load ISLR2::College data set R.Explore college data.Produce numerical summary variables data set.Answer.Table 2.1: Data summaryVariable type: factorVariable type: numericProduce scatterplot matrix first ten columns variables data.Answer.Produce side--side boxplots Outstate versus Private.Answer.Create new qualitative variable, called Elite, binning Top10perc variable. going divide universities two groups based whether proportion students coming top 10% high school classes exceeds 50%. See many elite universities , produce side--side boxplots Outstate versus Elite.Answer.Produce histograms differing numbers bins quantitative variables.Answer.Continue exploring data, provide brief summary discover.Answer. Passing one.Exercise 2.9  exercise involves Auto data set.predictors quantitative, qualitative?Answer.variables name quantitative.range quantitative predictor?range quantitative predictor?mean standard deviation quantitative\npredictor?mean standard deviation quantitative\npredictor?Answer.Now remove 10th 85th observations. range, mean, standard deviation predictor subset data remains?Answer.Using full data set, investigate predictors graphically, using scatterplots tools choice. Create plots highlighting relationships among predictors. Comment findings.Answer.plots show relationships expect different variables based laws physics. interesting observations:appear two clusters cars based weight acceleration plotThe relationship weight mpg nonlinearMost cars even number cylindersSuppose wish predict gas mileage (mpg) basis variables. plots suggest variables might useful predicting mpg? Justify answer.Answer.plots suggest mpg nonlinear relationships least variables data set (didnâ€™t explore ). particular, weight horsepower show clear relationships might suitable making predictions, long can deal heteroskedasticity.Exercise 2.10  exercise involves Boston housing data set.begin, load Boston data set. many rows data set? many columns? rows columns represent?Answer.columns represent variables might related value houses area. row represents suburb Boston.Make pairwise scatterplots predictors (columns) \ndata set. Describe findings.Answer.explored scatterplots shown . general arenâ€™t many easily interpretable relationships variables data set. pairs clusters data, overall trend; others floor effects communities value zero (e.g., crime).predictors associated per capita crime rate?\n, explain relationship.Answer.plots illustrative relationship crime variables dataset. less, crime seems occur specific range variable associated .census tracts Boston appear particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment range predictor.Answer.plots already get features data. range three predictors show interesting patterns . Crime per capita wide range, almost none others lot comparison. Tax similarly wide range. Although pupil-teacher ratios skinnier range, differences minimum maximum quite meaningful classroomâ€”teacher minimum theoretically twice much time student compared teacher maximum.many census tracts data set bound Charles river?Answer.median pupil-teacher ratio among towns data set?Answer.census tract Boston lowest median value owner-occupied homes? values predictors census tract, values compare overall ranges predictors? Comment findings.Answer.suburb lowest median value owner-occupied homes values predictors listed .comparison range values:crim upper endzn minimumindus around two-thirds upThe suburb bound rivernox upper endrm near middleage maximumdis almost minimumrad maximumtax near maximumptratio near maximumlstat around two-thirds upmedv minimumIt unsurprising suburb lowest median value owner-occupied homes. older suburb small residential lots, high traffic density resultant increase pollution crime, land taken businesses, high taxes. characteristics attractive home buyers, likely isnâ€™t much demand homes suburb.data set, many census tracts average seven rooms per dwelling? eight rooms per dwelling? Comment census tracts average eight rooms per dwelling.Answer.Several communities seem near , given shared values many variables. low tax rates, low crime, smaller lots, medium high pollution, high number rooms per dwelling.","code":"\n# Make sure to preserve the row names with college names in them because they\n# might be useful later\ncollege <- as_tibble(ISLR2::College, rownames = NA)\nglimpse(college)\n#> Rows: 777\n#> Columns: 18\n#> $ Private     <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes~\n#> $ Apps        <dbl> 1660, 2186, 1428, 417, 193, 587, 353, ~\n#> $ Accept      <dbl> 1232, 1924, 1097, 349, 146, 479, 340, ~\n#> $ Enroll      <dbl> 721, 512, 336, 137, 55, 158, 103, 489,~\n#> $ Top10perc   <dbl> 23, 16, 22, 60, 16, 38, 17, 37, 30, 21~\n#> $ Top25perc   <dbl> 52, 29, 50, 89, 44, 62, 45, 68, 63, 44~\n#> $ F.Undergrad <dbl> 2885, 2683, 1036, 510, 249, 678, 416, ~\n#> $ P.Undergrad <dbl> 537, 1227, 99, 63, 869, 41, 230, 32, 3~\n#> $ Outstate    <dbl> 7440, 12280, 11250, 12960, 7560, 13500~\n#> $ Room.Board  <dbl> 3300, 6450, 3750, 5450, 4120, 3335, 57~\n#> $ Books       <dbl> 450, 750, 400, 450, 800, 500, 500, 450~\n#> $ Personal    <dbl> 2200, 1500, 1165, 875, 1500, 675, 1500~\n#> $ PhD         <dbl> 70, 29, 53, 92, 76, 67, 90, 89, 79, 40~\n#> $ Terminal    <dbl> 78, 30, 66, 97, 72, 73, 93, 100, 84, 4~\n#> $ S.F.Ratio   <dbl> 18.1, 12.2, 12.9, 7.7, 11.9, 9.4, 11.5~\n#> $ perc.alumni <dbl> 12, 16, 30, 37, 2, 11, 26, 37, 23, 15,~\n#> $ Expend      <dbl> 7041, 10527, 8735, 19016, 10922, 9727,~\n#> $ Grad.Rate   <dbl> 60, 56, 54, 59, 15, 55, 63, 73, 80, 52~\nskim(college)\ncollege %>% \n  select(1:10) %>% \n  ggpairs()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\ncollege %>% \n  ggplot(aes(y = Outstate)) +\n    geom_boxplot() +\n    facet_wrap(~Private) +\n    labs(title = \"Private universities receive more out of state tuition\")\ncollege %>% \n  mutate(Elite = if_else(Top10perc > 50, \"Yes\", \"No\") %>% as_factor()) %>% \n  ggplot(aes(y = Outstate)) +\n    geom_boxplot() +\n    facet_wrap(~Elite) +\n    labs(title = \"Elite universities receive more out of state tuition\")\ngghist <- function(x, binwidth) {\n  ggplot(college, aes(x = {{ x }})) +\n    geom_histogram(binwidth = binwidth)\n}\n\ngghist(Enroll, 300) + gghist(Enroll, 200) + gghist(Enroll, 100) +\n  plot_annotation(\n    title = \"College enrollment follows a power law distribution\"\n  )\n\ngghist(PhD, 20) + gghist(PhD, 10) + gghist(PhD, 5) +\n  plot_annotation(\n    title = \"The majority of college faculty have a PhD\"\n  )\n\ngghist(Books, 100) + gghist(Books, 50) + gghist(Books, 25) +\n  plot_annotation(\n    title = \"Books cost way too much at colleges\"\n  )\nauto <- as_tibble(ISLR2::Auto)\nglimpse(auto)\n#> Rows: 392\n#> Columns: 9\n#> $ mpg          <dbl> 18, 15, 18, 16, 17, 15, 14, 14, 14, 1~\n#> $ cylinders    <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8~\n#> $ displacement <dbl> 307, 350, 318, 304, 302, 429, 454, 44~\n#> $ horsepower   <int> 130, 165, 150, 150, 140, 198, 220, 21~\n#> $ weight       <int> 3504, 3693, 3436, 3433, 3449, 4341, 4~\n#> $ acceleration <dbl> 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9~\n#> $ year         <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~\n#> $ origin       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ name         <fct> chevrolet chevelle malibu, buick skyl~\nauto %>% \n  pivot_longer(!name, \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value),\n    mean = mean(value),\n    sd   = sd(value)\n  )\n#> # A tibble: 8 x 5\n#>   predictor      min    max    mean      sd\n#>   <chr>        <dbl>  <dbl>   <dbl>   <dbl>\n#> 1 acceleration     8   24.8   15.5    2.76 \n#> 2 cylinders        3    8      5.47   1.71 \n#> 3 displacement    68  455    194.   105.   \n#> 4 horsepower      46  230    104.    38.5  \n#> 5 mpg              9   46.6   23.4    7.81 \n#> 6 origin           1    3      1.58   0.806\n#> 7 weight        1613 5140   2978.   849.   \n#> 8 year            70   82     76.0    3.68\nauto %>% \n  slice(-10:-85) %>% \n  pivot_longer(!name, \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value),\n    mean = mean(value),\n    sd   = sd(value)\n  )\n#> # A tibble: 8 x 5\n#>   predictor       min    max    mean      sd\n#>   <chr>         <dbl>  <dbl>   <dbl>   <dbl>\n#> 1 acceleration    8.5   24.8   15.7    2.69 \n#> 2 cylinders       3      8      5.37   1.65 \n#> 3 displacement   68    455    187.    99.7  \n#> 4 horsepower     46    230    101.    35.7  \n#> 5 mpg            11     46.6   24.4    7.87 \n#> 6 origin          1      3      1.60   0.820\n#> 7 weight       1649   4997   2936.   811.   \n#> 8 year           70     82     77.1    3.11\nggscatter <- function(x, y) {\n  ggplot(auto, aes({{ x }}, {{ y }})) +\n    geom_point()\n}\n\nggscatter(weight, acceleration) + ggscatter(weight, mpg) +\n  plot_annotation(\n    title = \"Lighter cars are more fuel efficient and accelerate faster\"\n  )\n\nggscatter(cylinders, displacement) +\n  labs(\n    title = \"Cars with more cylinders have higher displacement\"\n  )\nggscatter(weight, mpg) + ggscatter(acceleration, mpg) +\n  ggscatter(horsepower, mpg) + ggscatter(displacement, mpg)\nboston <- as_tibble(ISLR2::Boston)\nglimpse(boston)\n#> Rows: 506\n#> Columns: 13\n#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.0690~\n#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5,~\n#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, ~\n#> $ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, ~\n#> $ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, ~\n#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, ~\n#> $ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.~\n#> $ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, ~\n#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 31~\n#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, ~\n#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43,~\n#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, ~\nggscatter <- function(x, y) {\n  ggplot(boston, aes({{ x }}, {{ y }})) +\n    geom_point()\n}\n\nggscatter(chas, crim) +\n  labs(\n    title = \"Crime occurs more often away from the Charles River\",\n    subtitle = \"Or, most suburbs are away from the Charles River\"\n  )\n\nggscatter(zn, nox) +\n  labs(\n    title = paste0(\n      \"Higher nitrogen oxide concentration in suburbs with all residential \\n\",\n      \"land zoning below 25,000 sq.ft.\")\n  )\n\nggscatter(ptratio, medv) +\n  labs(\n    title = paste0(\n      \"Higher pupil to teacher ratios are related to less \\n\",\n      \"valuable owner-occupied homes\"\n    )\n  )\nggscatter(crim, rad) +\n  labs(\n    title = \"Suburbs with better access to radial highways have more crime\"\n  )\n\nggscatter(crim, tax) +\n  labs(\n    title = \"Suburbs with higher property tax rates have more crime\"\n  )\nboston %>% \n  select(crim, tax, ptratio) %>% \n  pivot_longer(everything(), \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value)\n  )\n#> # A tibble: 3 x 3\n#>   predictor       min   max\n#>   <chr>         <dbl> <dbl>\n#> 1 crim        0.00632  89.0\n#> 2 ptratio    12.6      22  \n#> 3 tax       187       711\nboston %>% \n  mutate(bound = if_else(chas == 1, \"Yes\", \"No\")) %>% \n  group_by(bound) %>% \n  summarise(\n    n = n()\n  )\n#> # A tibble: 2 x 2\n#>   bound     n\n#>   <chr> <int>\n#> 1 No      471\n#> 2 Yes      35\nboston %>% \n  summarise(\n    median_ptratio = median(ptratio)\n  )\n#> # A tibble: 1 x 1\n#>   median_ptratio\n#>            <dbl>\n#> 1           19.0\nboston %>% \n  arrange(desc(medv)) %>% \n  tail(1) %>% \n  glimpse()\n#> Rows: 1\n#> Columns: 13\n#> $ crim    <dbl> 67.9208\n#> $ zn      <dbl> 0\n#> $ indus   <dbl> 18.1\n#> $ chas    <int> 0\n#> $ nox     <dbl> 0.693\n#> $ rm      <dbl> 5.683\n#> $ age     <dbl> 100\n#> $ dis     <dbl> 1.4254\n#> $ rad     <int> 24\n#> $ tax     <dbl> 666\n#> $ ptratio <dbl> 20.2\n#> $ lstat   <dbl> 22.98\n#> $ medv    <dbl> 5\nboston %>% \n  pivot_longer(everything(), \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value)\n  )\n#> # A tibble: 13 x 3\n#>    predictor       min     max\n#>    <chr>         <dbl>   <dbl>\n#>  1 age         2.9     100    \n#>  2 chas        0         1    \n#>  3 crim        0.00632  89.0  \n#>  4 dis         1.13     12.1  \n#>  5 indus       0.46     27.7  \n#>  6 lstat       1.73     38.0  \n#>  7 medv        5        50    \n#>  8 nox         0.385     0.871\n#>  9 ptratio    12.6      22    \n#> 10 rad         1        24    \n#> 11 rm          3.56      8.78 \n#> 12 tax       187       711    \n#> 13 zn          0       100\nboston %>% \n  filter(rm > 7) %>% \n  count()\n#> # A tibble: 1 x 1\n#>       n\n#>   <int>\n#> 1    64\nboston %>% \n  filter(rm > 8)\n#> # A tibble: 13 x 13\n#>      crim    zn indus  chas   nox    rm   age   dis   rad\n#>     <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <int>\n#>  1 0.121      0  2.89     0 0.445  8.07  76    3.50     2\n#>  2 1.52       0 19.6      1 0.605  8.38  93.9  2.16     5\n#>  3 0.0201    95  2.68     0 0.416  8.03  31.9  5.12     4\n#>  4 0.315      0  6.2      0 0.504  8.27  78.3  2.89     8\n#>  5 0.527      0  6.2      0 0.504  8.72  83    2.89     8\n#>  6 0.382      0  6.2      0 0.504  8.04  86.5  3.22     8\n#>  7 0.575      0  6.2      0 0.507  8.34  73.3  3.84     8\n#>  8 0.331      0  6.2      0 0.507  8.25  70.4  3.65     8\n#>  9 0.369     22  5.86     0 0.431  8.26   8.4  8.91     7\n#> 10 0.612     20  3.97     0 0.647  8.70  86.9  1.80     5\n#> 11 0.520     20  3.97     0 0.647  8.40  91.5  2.29     5\n#> 12 0.578     20  3.97     0 0.575  8.30  67    2.42     5\n#> 13 3.47       0 18.1      1 0.718  8.78  82.9  1.90    24\n#> # ... with 4 more variables: tax <dbl>, ptratio <dbl>,\n#> #   lstat <dbl>, medv <dbl>"},{"path":"chapter-3.html","id":"chapter-3","chapter":"3 Linear Regression","heading":"3 Linear Regression","text":"Chapter 3 (ordinary least squares polynomial) linear regression, parametric supervised statistical learning method. Many advanced statistical learning approaches generalizations extensions linear regression, good grasp linear regression important understanding later methods book.also discusses K-nearest neighbours regression (KNN regression), non-parametric supervised statistical learning method.","code":""},{"path":"chapter-3.html","id":"exercises-1","chapter":"3 Linear Regression","heading":"3.1 Exercises","text":"","code":""},{"path":"chapter-3.html","id":"prerequisites-1","chapter":"3 Linear Regression","heading":"Prerequisites","text":"access data sets functions used complete Chapter 3 exercises, load following packages.","code":"\nlibrary(ISLR2)\nlibrary(tidyverse)\nlibrary(tidymodels)\n# library(skimr)\n# library(GGally)\n# library(patchwork)"},{"path":"chapter-3.html","id":"conceptual-1","chapter":"3 Linear Regression","heading":"Conceptual","text":"Exercise 3.1  Describe null hypotheses p-values given Table 3.4 correspond. Explain conclusions can draw based p-values. explanation phrased terms sales, TV, radio, newspaper, rather terms coefficients linear model.Table 3.4Answer.null hypotheses p-values Table 3.4 correspond coefficient term equal zero, \\(\\beta_i = 0\\), use test whether term associated response variable. can use p-values table infer whether coefficients sufficiently far zero can confident non-zero, thus associated response variable.Based p-values Table 3.4, can conclude :$1000 increase TV advertising budget associated average increase product sales 46 units holding advertising budgets radio newspaper fixed.$1000 increase radio advertising budget associated average increase product sales 189 units holding advertising budgets TV newspaper fixed.$1000 increase newspaper advertising budget associated change product sales.Data note: Advertising data sales thousands units advertising budgets thousands dollars.Exercise 3.2  Carefully explain differences KNN classifier KNN regression methods.Answer.Given value \\(K\\) test observation \\(x_0\\), KNN classifier KNN regression first identify \\(K\\) training observations closest \\(x_0\\), represented \\(\\mathcal N_0\\). KNN classifier \\(x_0\\) discrete class, KNN regression \\(x_0\\) continuous value. methods diverge \\(\\mathcal N_0\\).KNN classifier estimates conditional probability test observation \\(x_0\\) belongs class \\(j\\) fraction training observations \\(\\mathcal N_0\\) whose response value \\(y_i\\) equals \\(j\\):\\[\n\\mathrm{Pr}(Y = j|X = x_0) =  \\frac{1}{K} \\sum_{\\\\mathcal N_0} (y_i = j).\n\\tag{3.1}\n\\]KNN regression, hand, estimates \\(f(x_0)\\) using average training observations \\(\\mathcal N_0\\):\\[\n\\hat f(x_0) = \\frac{1}{K} \\sum_{\\{\\mathcal N}_0} y_i.\n\\tag{3.2}\n\\]can seen Equations (3.1) (3.2), two main differences KNN classifier KNN regression methods:right side equation, KNN classifier uses indicator function \\((y_i = j)\\) determine whether response value \\(y_i\\) equals \\(j\\) (equals 1, 0). summed get numerator conditional probability class (denominator \\(K\\)). KNN regression, hand, simply uses response value \\(y_i\\) , since â€™s continuous value. also summed get numerator fraction (\\(K\\) denominator), although makes sense think average training observations \\(\\mathcal N_0\\).left hand side equation, KNN classifier estimate conditional probability test observation \\(x_0\\) belongs class \\(j\\). can use probability assign \\(x_0\\) class \\(j\\) highest probability, choose (although donâ€™t , probabilities equally useful). KNN regression estimate form \\(f(x_0)\\), corresponds prediction point test observation \\(x_0\\) since \\(\\hat Y = \\hat f(X)\\).Thus, main difference two methods KNN classifier method used predict class observation likely belongs based classes \\(K\\) nearest neighbours, whereas KNN regression method used predict value observation response variable based response values \\(K\\) nearest neighbours.Exercise 3.3  Suppose data set five predictors, \\(X_1 =\\) GPA, \\(X_2 =\\) IQ, \\(X_3 =\\) Level (1 College 0 High School), \\(X_4 =\\) Interaction GPA IQ, \\(X_5 =\\) Interaction GPA Level. response starting salary graduation (thousands dollars). Suppose use least squares fit model, get \\(\\hat \\beta_0 = 50\\), \\(\\hat \\beta_1 = 20\\), \\(\\hat \\beta_2 = 0.07\\), \\(\\hat \\beta_3 = 35\\), \\(\\hat \\beta_4 = 0.01\\), \\(\\hat \\beta_5 = âˆ’10\\).answer correct, ?fixed value IQ GPA, high school graduates earn , average, college graduates.fixed value IQ GPA, high school graduates earn , average, college graduates.fixed value IQ GPA, college graduates earn , average, high school graduates.fixed value IQ GPA, college graduates earn , average, high school graduates.fixed value IQ GPA, high school graduates earn , average, college graduates provided GPA high enough.fixed value IQ GPA, high school graduates earn , average, college graduates provided GPA high enough.fixed value IQ GPA, college graduates earn , average, high school graduates provided GPA high enough.fixed value IQ GPA, college graduates earn , average, high school graduates provided GPA high enough.Answer. fourth description correct. coefficient Level positive, included model (.e., Level zero) predictions starting salary higher; however, coefficient interaction GPA level negative, true provided college graduate GPA high enough.Predict salary college graduate IQ 110 GPA 4.0.Answer.predicted salary college graduate $137,100.\\[\n\\begin{align}\n\\hat y &= 50 + 20(4) + 0.07(110) + 35(1) + 0.01(4*110) - 10(4*1) \\\\\n       &= 50 + 80 + 7.7 + 35 + 4.4 - 40 \\\\\n       &= 137.1\n\\end{align}\n\\]True false: Since coefficient GPA/IQ interaction term small, little evidence interaction effect. Justify answer.Answer. False. size coefficient provide evidence interaction effect. use p-value associated coefficient determine whether sufficiently far zero can confident non-zero, thus associated response variable. estimates precise enough can find small coefficient confident non-zero. make decision based coefficient alone.Exercise 3.4  collect set data (n = 100 observations) containing single predictor quantitative response. fit linear regression model data, well separate cubic regression, .e.Â \\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\\).Suppose true relationship \\(X\\) \\(Y\\) linear,\n.e.Â \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Consider training residual sum squares (RSS) linear regression, also training RSS cubic regression. expect one lower , expect , enough information tell? Justify answer.Answer.Given true relationship \\(X\\) \\(Y\\) linear expect residual sum squares lower linear regression cubic regression :least squares line minimizes RSS, andThe linear regression matches true form \\(f(X)\\), soThe linear regression fit better thus smaller RSS.Answer () using test rather training RSS.Answer.expect outcome test RSS training RSS.Suppose true relationship \\(X\\) \\(Y\\) linear, donâ€™t know far linear. Consider training RSS linear regression, also training RSS cubic regression. expect one lower , expect , enough information tell? Justify answer.Answer.Given donâ€™t know far true relationship \\(X\\) \\(Y\\) linear, â€™s hard say sure. true relationship far linear expect training RSS lower cubic regression linear regression bias-variance trade poorer linear regression. expect true relationship cubic cubic regression match true form \\(f(X)\\). However, true relationship slightly nonlinear â€™s hard say lower training RSS, . likely situations one better ; although general cubic regression might overfit thus lower RSS.Answer (c) using test rather training RSS.Answer., â€™s hard say sure. true relationship \\(X\\) \\(Y\\) slightly nonlinear, might expect cubic regression overfit training data, bias linear regression may lead better predictions (lower RSS) test data. true relationship cubic cubic regression lower test RSS since match true form \\(f(X)\\). true relationship far linear expect cubic regression lower RSS since better bias-variance trade linear regression.Exercise 3.5  Consider fitted values result performing linear regression without intercept. setting, \\(\\)th fitted value takes form\\[\n\\hat y_i = \\hat \\beta x_i,\n\\]\\[\n\\hat \\beta = \\bigg(\\sum_{= 1}^n x_i y_i \\bigg) / \\bigg(\\sum_{' = 1}^n x_{'}^2 \\bigg).\n\\]Show can write\\[\n\\hat y_i = \\sum_{' = 1}^n a_{'} y_{'}.\n\\]\\(a_{'}\\)?Answer.can get solution :Plugging expression \\(\\hat \\beta\\) formula \\(\\)th fitted value.Moving constant \\(x_i\\) numerator summation expression changing numerator summation dummy variable \\('\\), since \\(x_i\\) independent \\('\\).Moving summation \\('\\) numerator.Moving \\(y_{'}\\) numerator.looks like\\[\n\\begin{align}\n\\hat y_i &= x_i \\hat \\beta \\\\\n         &= x_i \\frac{\\sum_{= 1}^n x_i y_i}{\\sum_{' = 1}^n x_{'}^2} \\\\\n         &= \\frac{\\sum_{' = 1}^n x_i x_{'} y_{'}}{\\sum_{'' = 1}^n x_{''}^2} \\\\\n         &= \\sum_{' = 1}^n \\frac{x_i x_{'} y_{'}}{\\sum_{'' = 1}^n x_{''}^2} \\\\\n         &= \\sum_{' = 1}^n \\frac{x_i x_{'}}{\\sum_{'' = 1}^n x_{''}^2} y_{'} \\\\\n         &= \\sum_{' = 1}^n a_{'} y_{'},\n\\end{align}\n\\]\\[\na_{'} = \\frac{x_i x_{'}}{\\sum_{'' = 1}^n x_{''}^2}.\n\\]Note: interpret result saying fitted values linear regression linear combinations response values.Exercise 3.6  Using least squares coefficient estimates equation, argue case simple linear regression, least squares line always passes point \\((\\bar x, \\bar y)\\).Answer.least squares coefficient estimates given \\[\n\\begin{align}\n\\hat \\beta_1 &= \\frac{\\sum_{= 1}^n (x_i - \\bar x)(y_i - \\bar y)}\n                     {\\sum_{= 1}^n (x_i - \\bar x)^2}, \\\\\n\\hat \\beta_0 &= \\bar y - \\hat \\beta_1 \\bar x.\n\\end{align}\n\\]least squares line given \\[\n\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x.\n\\]\\(x = \\bar x\\) \\(\\hat y = \\bar y\\) least squares line. can demonstrated substituting least squares coefficient estimate equation intercept least squares line equation,\\[\n\\begin{align}\n\\hat y &= \\hat \\beta_0 + \\hat \\beta_1 \\bar x \\\\\n       &= \\bar y - \\hat \\beta_1 \\bar x + \\hat \\beta_1 \\bar x \\\\\n       &= \\bar y.\n\\end{align}\n\\]Since least squares line passes values \\(X\\), always pass mean predictor \\(\\bar x\\), exists somewhere smallest largest values \\(x\\). shown , \\(x = \\bar x\\) \\(\\hat y = \\bar y\\) least squares line, thus least squares line always passes point \\((\\bar x, \\bar y)\\).Exercise 3.7  claimed text case simple linear regression \\(Y\\) onto \\(X\\), \\(R^2\\) statistic equal square correlation \\(X\\) \\(Y\\). Prove case. simplicity, may assume \\(\\bar x = \\bar y = 0\\).Answer.Skipping one.\n","code":""},{"path":"chapter-3.html","id":"applied-1","chapter":"3 Linear Regression","heading":"Applied","text":"Exercise 3.8  question involves use simple linear regression ISLR2::Auto\ndata set.Use lm() function perform simple linear regression mpg response horsepower predictor. Use summary() function print results. Comment output.Answer.output can see :relationship horsepower mpg one horsepower increase associated average decrease fuel economy 0.157845 miles per gallon.Horsepower explains 60.59% variability mpg.make predictions based common horsepower values different classes vehicles (small car, average car, SUV) can see model misspecified.vehicle 300 horsepower model predicts fuel economy falls somewhere -9.94281 mpg -4.892308 mpg. implies vehicles high horsepower produce fuel travel, rather consuming . â€™s nonsense statistic indicates model misspecified fairly severe way. perhaps data quality issue.Plot response predictor. Display least squares regression line using reference line (abline).Answer.plot gives good evidence model misspecified. relationship horsepower mpg clearly nonlinear data. also see range horsepower data limited comparison amount horsepower might interested making predictions .Produce diagnostic plots least squares regression fit. Comment problems see fit.Answer.diagnostic plot clearly shows problems fit:U-shape residuals gives strong indication nonlinearity (know nonlinearity data already).non-constant variance residuals sign heteroscedasticity.least couple outliers.Exercise 3.9  question involves use multiple linear regression Auto data set.Produce scatterplot matrix includes variables\ndata set.Answer.Compute matrix correlations variables.Answer.Use lm() function perform multiple linear regression mpg response variables except name origin predictors. Use summary() function print results. Comment output.Answer.output can see :F-statistic indicates least one predictor non-zero.American vehicles average fuel economy -17.9546021 miles per gallon variables 0. nonsense statistic, holding variables 0. Centring might help .European vehicles average fuel economy 2.63 miles per gallon higher American vehicles holding variables fixed.Japanese vehicles average fuel economy 2.85 miles per gallon higher American vehicles holding variables fixed.one year increase vehicle model associated average increase fuel economy 0.78 miles per gallon American vehicles. newer American vehicles expected higher fuel economy.predictors except cylinders, horsepower, acceleration, statistically significant relationship mpg.Produce diagnostic plots linear regression fit. Comment problems see fit. residual plots suggest unusually large outliers? leverage plot identify observations unusually high\nleverage?Answer.diagnostic plot clearly shows problems fit:U-shape residuals gives strong indication nonlinearity (know nonlinearity data already).non-constant variance residuals sign heteroscedasticity.leverage plot shows one two high leverage points. can also see possible outliers whose standardized residual value greater three.Use * : symbols fit linear regression models interaction effects. interactions appear statistically significant?Answer.model interactions predictors number statistically significant interactions (shown output really long).simpler models show interesting interactions. example, see average effect one horsepower increase fuel economy depends vehicleâ€™s country origin (although meaningful amount seems).Try different transformations variables, \\(\\mathrm{log}(X)\\), \\(\\sqrt X\\), \\(X^2\\). Comment findings.Answer.â€™ll fit model different transformations. chose predictors nonlinear relationship mpg. can seen output models, transformations affect aspects model fits parameter estimates.Exercise 3.10  question answered using Carseats data set.Fit multiple regression model predict Sales using Price, Urban, US.Answer.Provide interpretation coefficient model. carefulâ€”variables model qualitative!Answer.coefficients can interpreted follows:intercept make sense interpret since car seats never sold free.one dollar increase car seat price associated average decrease sales 54 units rural stores located outside US.Urban stores located outside US sell 22 fewer car seats average compared rural stores located outside US holding price fixed.Rural stores located US sell 1200 car seats average compared rural stores located outside US holding price fixed.Write model equation form, careful handle\nqualitative variables properly.Answer.\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i + \\beta_3 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 x_i + \\epsilon_i &\n  \\text{ith location rural outside US} \\\\\n\\beta_0 + \\beta_1 x_i + \\beta_2 x_i + \\epsilon_i &\n  \\text{ith location urban outside US} \\\\\n\\beta_0 + \\beta_1 x_i + \\beta_3 x_i + \\epsilon_i &\n  \\text{ith location rural inside US},\n\\end{cases}\n\\]\\[\n\\begin{align}\n\\beta_0 &= 13.043469 \\\\\n\\beta_1 &= -0.054459 \\\\\n\\beta_2 &= -0.021916 \\\\\n\\beta_3 &= 1.200573.\n\\end{align}\n\\]predictors can reject null hypothesis \\(H_0: \\beta_j = 0\\)?Answer. Intercept, Price, US.basis response previous question, fit smaller model uses predictors evidence association outcome.Answer.well models () (e) fit data?Answer.residual standard error \\(R^2\\) models , respectively, similar . likelihood ratio test two models also shows difference compatibility models data.Using model (e), obtain 95% confidence intervals coefficient(s).Answer.evidence outliers high leverage observations model (e)?Answer.can seen diagnostic plot , appear outliers, least high leverage points.","code":"\nauto_model <- lm(mpg ~ horsepower, data = Auto)\nsummary(auto_model)\n#> \n#> Call:\n#> lm(formula = mpg ~ horsepower, data = Auto)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -13.5710  -3.2592  -0.3435   2.7630  16.9240 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 39.935861   0.717499   55.66   <2e-16 ***\n#> horsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.906 on 390 degrees of freedom\n#> Multiple R-squared:  0.6059, Adjusted R-squared:  0.6049 \n#> F-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\nauto_model %>%\n  predict(\n    tibble(horsepower = c(98, 180, 300)),\n    type = \"response\",\n    interval = \"confidence\"\n  )\n#>         fit      lwr       upr\n#> 1 24.467077 23.97308 24.961075\n#> 2 11.523809 10.44983 12.597792\n#> 3 -7.417559 -9.94281 -4.892308\nggplot(Auto, aes(x = horsepower, y = mpg)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(\n    slope = coef(auto_model)[\"horsepower\"],\n    intercept = coef(auto_model)[\"(Intercept)\"],\n    colour = \"blue\"\n  ) +\n  coord_cartesian(ylim = c(0, 50))\nauto_model %>% \n  augment() %>% \n  ggplot(aes(x = .fitted, y = .resid)) +\n    geom_point()\nplot(Auto)\nAuto %>% \n  select(-name) %>% \n  cor()\n#>                     mpg  cylinders displacement horsepower\n#> mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268\n#> cylinders    -0.7776175  1.0000000    0.9508233  0.8429834\n#> displacement -0.8051269  0.9508233    1.0000000  0.8972570\n#> horsepower   -0.7784268  0.8429834    0.8972570  1.0000000\n#> weight       -0.8322442  0.8975273    0.9329944  0.8645377\n#> acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955\n#> year          0.5805410 -0.3456474   -0.3698552 -0.4163615\n#> origin        0.5652088 -0.5689316   -0.6145351 -0.4551715\n#>                  weight acceleration       year     origin\n#> mpg          -0.8322442    0.4233285  0.5805410  0.5652088\n#> cylinders     0.8975273   -0.5046834 -0.3456474 -0.5689316\n#> displacement  0.9329944   -0.5438005 -0.3698552 -0.6145351\n#> horsepower    0.8645377   -0.6891955 -0.4163615 -0.4551715\n#> weight        1.0000000   -0.4168392 -0.3091199 -0.5850054\n#> acceleration -0.4168392    1.0000000  0.2903161  0.2127458\n#> year         -0.3091199    0.2903161  1.0000000  0.1815277\n#> origin       -0.5850054    0.2127458  0.1815277  1.0000000\nauto_model <- Auto %>% \n  mutate(\n    across(origin, as_factor),\n    across(\n      origin,\n      ~ fct_recode(.x, American = \"1\", European = \"2\", Japanese = \"3\")\n    )\n  ) %>% \n  lm(\n    mpg ~ cylinders + displacement + horsepower + weight +\n    acceleration + year + origin,\n    data = .\n  )\nsummary(auto_model)\n#> \n#> Call:\n#> lm(formula = mpg ~ cylinders + displacement + horsepower + weight + \n#>     acceleration + year + origin, data = .)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.0095 -2.0785 -0.0982  1.9856 13.3608 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    -1.795e+01  4.677e+00  -3.839 0.000145 ***\n#> cylinders      -4.897e-01  3.212e-01  -1.524 0.128215    \n#> displacement    2.398e-02  7.653e-03   3.133 0.001863 ** \n#> horsepower     -1.818e-02  1.371e-02  -1.326 0.185488    \n#> weight         -6.710e-03  6.551e-04 -10.243  < 2e-16 ***\n#> acceleration    7.910e-02  9.822e-02   0.805 0.421101    \n#> year            7.770e-01  5.178e-02  15.005  < 2e-16 ***\n#> originEuropean  2.630e+00  5.664e-01   4.643 4.72e-06 ***\n#> originJapanese  2.853e+00  5.527e-01   5.162 3.93e-07 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.307 on 383 degrees of freedom\n#> Multiple R-squared:  0.8242, Adjusted R-squared:  0.8205 \n#> F-statistic: 224.5 on 8 and 383 DF,  p-value: < 2.2e-16\nauto_model %>% \n  augment() %>% \n  ggplot(aes(x = .fitted, y = .resid)) +\n    geom_point()\nauto_model %>% \n  augment() %>%\n  # The standardized residual returned by augment() is equivalent to the\n  # \"internally studentized residuals\", which the textbook refers to simply as\n  # \"studentized residuals\".\n  ggplot(aes(x = .hat, y = .std.resid)) +\n    geom_point() +\n    geom_text(aes(label = 1:nrow(Auto)), nudge_x = -0.007) +\n    labs(x = \"leverage\")\nAuto %>% \n  mutate(\n    across(origin, as_factor),\n    across(\n      origin,\n      ~ fct_recode(.x, American = \"1\", European = \"2\", Japanese = \"3\")\n    )\n  ) %>% \n  lm(\n    mpg ~ horsepower*origin,\n    data = .\n  ) %>% \n  summary()\n#> \n#> Call:\n#> lm(formula = mpg ~ horsepower * origin, data = .)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10.7415  -2.9547  -0.6389   2.3978  14.2495 \n#> \n#> Coefficients:\n#>                            Estimate Std. Error t value\n#> (Intercept)               34.476496   0.890665  38.709\n#> horsepower                -0.121320   0.007095 -17.099\n#> originEuropean            10.997230   2.396209   4.589\n#> originJapanese            14.339718   2.464293   5.819\n#> horsepower:originEuropean -0.100515   0.027723  -3.626\n#> horsepower:originJapanese -0.108723   0.028980  -3.752\n#>                           Pr(>|t|)    \n#> (Intercept)                < 2e-16 ***\n#> horsepower                 < 2e-16 ***\n#> originEuropean            6.02e-06 ***\n#> originJapanese            1.24e-08 ***\n#> horsepower:originEuropean 0.000327 ***\n#> horsepower:originJapanese 0.000203 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.422 on 386 degrees of freedom\n#> Multiple R-squared:  0.6831, Adjusted R-squared:  0.679 \n#> F-statistic: 166.4 on 5 and 386 DF,  p-value: < 2.2e-16\nauto_model_1 <- lm(\n  mpg ~ displacement + horsepower + weight,\n  data = Auto\n)\n\nauto_model_2 <- lm(\n  mpg ~ log(displacement) + log(horsepower) + log(weight),\n  data = Auto\n)\n\nauto_model_3 <- lm(\n  mpg ~ sqrt(displacement) + sqrt(horsepower) + sqrt(weight),\n  data = Auto\n)\n\nauto_model_4 <- lm(\n  mpg ~ I(displacement^2) + I(horsepower^2) + I(weight^2),\n  data = Auto\n)\n\nsummary(auto_model_1)\n#> \n#> Call:\n#> lm(formula = mpg ~ displacement + horsepower + weight, data = Auto)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -11.3347  -2.8028  -0.3402   2.2037  16.2409 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  44.8559357  1.1959200  37.507  < 2e-16 ***\n#> displacement -0.0057688  0.0065819  -0.876  0.38132    \n#> horsepower   -0.0416741  0.0128139  -3.252  0.00125 ** \n#> weight       -0.0053516  0.0007124  -7.513 4.04e-13 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.241 on 388 degrees of freedom\n#> Multiple R-squared:  0.707,  Adjusted R-squared:  0.7047 \n#> F-statistic:   312 on 3 and 388 DF,  p-value: < 2.2e-16\nsummary(auto_model_2)\n#> \n#> Call:\n#> lm(formula = mpg ~ log(displacement) + log(horsepower) + log(weight), \n#>     data = Auto)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -11.6539  -2.4232  -0.3147   2.0760  15.2014 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        161.495     11.898  13.574  < 2e-16 ***\n#> log(displacement)   -2.353      1.187  -1.982   0.0482 *  \n#> log(horsepower)     -6.929      1.263  -5.487 7.38e-08 ***\n#> log(weight)        -11.835      2.264  -5.228 2.81e-07 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.978 on 388 degrees of freedom\n#> Multiple R-squared:  0.7422, Adjusted R-squared:  0.7402 \n#> F-statistic: 372.3 on 3 and 388 DF,  p-value: < 2.2e-16\nsummary(auto_model_3)\n#> \n#> Call:\n#> lm(formula = mpg ~ sqrt(displacement) + sqrt(horsepower) + sqrt(weight), \n#>     data = Auto)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -11.4945  -2.6258  -0.3391   2.1942  15.6793 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        65.76610    2.36880  27.763  < 2e-16 ***\n#> sqrt(displacement) -0.26868    0.18126  -1.482    0.139    \n#> sqrt(horsepower)   -1.11790    0.25795  -4.334 1.87e-05 ***\n#> sqrt(weight)       -0.50813    0.08152  -6.233 1.19e-09 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.089 on 388 degrees of freedom\n#> Multiple R-squared:  0.7276, Adjusted R-squared:  0.7255 \n#> F-statistic: 345.4 on 3 and 388 DF,  p-value: < 2.2e-16\nsummary(auto_model_4)\n#> \n#> Call:\n#> lm(formula = mpg ~ I(displacement^2) + I(horsepower^2) + I(weight^2), \n#>     data = Auto)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -10.941  -3.323  -0.771   2.634  17.200 \n#> \n#> Coefficients:\n#>                     Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)        3.427e+01  6.017e-01  56.955   <2e-16\n#> I(displacement^2) -3.673e-08  1.483e-05  -0.002   0.9980\n#> I(horsepower^2)   -1.033e-04  5.632e-05  -1.834   0.0674\n#> I(weight^2)       -9.953e-07  1.018e-07  -9.778   <2e-16\n#>                      \n#> (Intercept)       ***\n#> I(displacement^2)    \n#> I(horsepower^2)   .  \n#> I(weight^2)       ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.596 on 388 degrees of freedom\n#> Multiple R-squared:  0.6559, Adjusted R-squared:  0.6532 \n#> F-statistic: 246.5 on 3 and 388 DF,  p-value: < 2.2e-16\ncarseats <- Carseats %>% \n  as_tibble() %>% \n  mutate(\n    across(where(is.character), as_factor),\n    across(ShelveLoc, ~ fct_relevel(.x, \"Medium\", after = 1))\n  )\n\ncarseats_model <- lm(Sales ~ Price + Urban + US, data = carseats)\nsummary(carseats_model)\n#> \n#> Call:\n#> lm(formula = Sales ~ Price + Urban + US, data = carseats)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -6.9206 -1.6220 -0.0564  1.5786  7.0581 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 13.043469   0.651012  20.036  < 2e-16 ***\n#> Price       -0.054459   0.005242 -10.389  < 2e-16 ***\n#> UrbanYes    -0.021916   0.271650  -0.081    0.936    \n#> USYes        1.200573   0.259042   4.635 4.86e-06 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.472 on 396 degrees of freedom\n#> Multiple R-squared:  0.2393, Adjusted R-squared:  0.2335 \n#> F-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16\ncarseats_model_2 <- lm(Sales ~ Price + US, data = carseats)\nsummary(carseats_model_2)\n#> \n#> Call:\n#> lm(formula = Sales ~ Price + US, data = carseats)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -6.9269 -1.6286 -0.0574  1.5766  7.0515 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 13.03079    0.63098  20.652  < 2e-16 ***\n#> Price       -0.05448    0.00523 -10.416  < 2e-16 ***\n#> USYes        1.19964    0.25846   4.641 4.71e-06 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.469 on 397 degrees of freedom\n#> Multiple R-squared:  0.2393, Adjusted R-squared:  0.2354 \n#> F-statistic: 62.43 on 2 and 397 DF,  p-value: < 2.2e-16\nanova(carseats_model, carseats_model_2, test = \"LRT\")\n#> Analysis of Variance Table\n#> \n#> Model 1: Sales ~ Price + Urban + US\n#> Model 2: Sales ~ Price + US\n#>   Res.Df    RSS Df Sum of Sq Pr(>Chi)\n#> 1    396 2420.8                      \n#> 2    397 2420.9 -1  -0.03979   0.9357\nconfint(carseats_model_2)\n#>                   2.5 %      97.5 %\n#> (Intercept) 11.79032020 14.27126531\n#> Price       -0.06475984 -0.04419543\n#> USYes        0.69151957  1.70776632\ncarseats_model_2 %>% \n  augment() %>%\n  # The standardized residual returned by augment() is equivalent to the\n  # \"internally studentized residuals\", which the textbook refers to simply as\n  # \"studentized residuals\".\n  ggplot(aes(x = .hat, y = .std.resid)) +\n    geom_point() +\n    geom_text(aes(label = 1:nrow(carseats)), nudge_x = -0.001) +\n    labs(x = \"leverage\")"}]
