[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"book self study second edition book Introduction Statistical Learning Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.","code":""},{"path":"chapter-1.html","id":"chapter-1","chapter":"1 Introduction","heading":"1 Introduction","text":"Chapter 1 provides brief overview statistical learning, refers vast set tools understanding data. tools statistical learning can classified either supervised unsupervised.Supervised statistical learning involves building statistical model predicting output (called response dependent variable) based one inputs (called features, predictors, independent variables). typical supervised learning setting access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations, response \\(Y\\) also measured \\(n\\) observations. goal predict \\(Y\\) using \\(X_1, X_2, \\dots , X_p\\). response \\(Y\\) can quantitative (continuous, numerical) qualitative (categorical, non-numerical). goal predict numerical value, call regression problem. goal predict non-numerical value call classification problem. Supervised statistical learning can used exploratory confirmatory data analysis.Unsupervised statistical learning involves cases one inputs used learn relationships structure data absence output. typical unsupervised learning setting () access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations. goal discover interesting things measurements \\(X_1, X_2, \\dots , X_p\\). goal partition observations subgroups based (dis)similarity, call clustering problem. Unsupervised statistical learning typically used exploratory data analysis; possible check work unsupervised statistical learning (don’t know true answer since problem unsupervised), used confirmatory data analysis.Semi-supervised statistical learning involves building statistical model predicting output based one inputs subset observations predictor measurements response measurement, wish incorporate observations model. typical semi-supervised learning setting access set \\(n\\) observations. \\(m\\) observations, \\(m < n\\), predictor measurements response measurement; remaining \\(n - m\\) observations predictor measurements response measurement. goal predict \\(Y\\) incorporating \\(m\\) observations response measurements available well \\(n - m\\) observations . topic beyond scope book explored .","code":""},{"path":"chapter-1.html","id":"notation-and-simple-matrix-algebra","chapter":"1 Introduction","heading":"1.1 Notation and Simple Matrix Algebra","text":"Chapter 1 also discusses notation conventions used textbook, starting Page 9. Briefly:\\(n\\) represents number observations sample\\(p\\) represents number variables available data set\\(x_{ij}\\) represents value \\(j\\)th variable \\(\\)th observation, \\(= 1, 2, \\dots, n\\) \\(j = 1, 2, \\dots, p\\).\\(\\mathbf X\\) represents \\(n \\times p\\) matrix whose \\((,j)\\)th element \\(x_{ij}\\)\\(x_i\\) represents rows matrix \\(\\mathbf X\\) \\(x_i\\) vector length \\(p\\) containing \\(p\\) variable measurements \\(\\)th observation.\\(\\mathbf x_j\\) represents columns matrix \\(\\mathbf X\\) \\(\\mathbf x_j\\) vector length \\(n\\) containing \\(n\\) observation measurements \\(j\\)th variable.\\(y_i\\) represents \\(\\)th observation response variable.\\(\\mathbf y\\) represents vector length \\(n\\) containing set response variable measurements predictions.","code":""},{"path":"chapter-1.html","id":"data-sets-used-in-labs-and-exercises","chapter":"1 Introduction","heading":"1.2 Data Sets Used in Labs and Exercises","text":"book uses data sets ISLR2 package (available CRAN) one data set included part base R distribution labs exercises.","code":""},{"path":"chapter-1.html","id":"book-website","chapter":"1 Introduction","heading":"1.3 Book Website","text":"website Introduction Statistical Learning located https://www.statlearning.org. contains number additional resources may useful.","code":""},{"path":"chapter-2.html","id":"chapter-2","chapter":"2 Statistical Learning","heading":"2 Statistical Learning","text":"Chapter 2 formalizes concept statistical learning introducing general statistical model used modelling relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), can written \\[\nY = f(X) + \\epsilon.\n\\tag{2.1}\n\\]:\\(Y\\) represents response variable data set\\(X\\) represents set variables data set\\(X_p\\) represents \\(p\\)th variable data set\\(f(\\dots)\\) represents fixed unknown function input(s)\\(\\epsilon\\) represents random error term independent \\(X\\) mean zeroThe goal statistical learning estimate \\(f\\). two main reasons estimating \\(f\\): prediction inference. Depending whether ultimate goal prediction, inference, combination two, different methods estimating \\(f\\) may appropriate. general, trade-prediction accuracy model interpretability. Models make accurate predictions tend less interpretable, models interpretable tend make less accurate predictions (although always case, due potential overfitting highly flexible models).methods use estimate \\(f\\) can characterized either parametric non-parametric. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly.","code":""},{"path":"chapter-2.html","id":"prediction","chapter":"2 Statistical Learning","heading":"2.1 Prediction","text":"error term \\(\\epsilon\\) averages zero, general statistical model predicting \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\) can written \\[\n\\hat Y = \\hat f(X).\n\\tag{2.2}\n\\]:\\(\\hat Y\\) represents resulting prediction \\(Y\\)\\(\\hat f\\) represents estimate \\(f\\)goal predict, typically need concern exact form \\(\\hat f\\) provided accurately predicts \\(Y\\). accuracy \\(\\hat Y\\) prediction \\(Y\\) depends two sources error: reducible error irreducible error. error model attributable \\(\\hat f\\) reducible can potentially improve accuracy \\(\\hat f\\) estimating \\(f\\) using appropriate statistical learning technique. However, error model attributable \\(\\epsilon\\) irreducible \\(Y\\) also function \\(\\epsilon\\), \\(\\epsilon\\) independent \\(X\\), matter well estimate \\(f\\), variability associated \\(\\epsilon\\) still present model. variability may come unmeasured variables useful predicting \\(Y\\), unmeasurable variation. Irreducible error places (often unknowable) upper bound accuracy prediction \\(Y\\).","code":""},{"path":"chapter-2.html","id":"inference","chapter":"2 Statistical Learning","heading":"2.2 Inference","text":"goal understand relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), need concern exact form \\(\\hat f\\). form \\(\\hat f\\) can used identify:predictors associated responsethe direction (positive negative) form (simple complex) relationship response predictor","code":""},{"path":"chapter-2.html","id":"assessing-model-accuracy","chapter":"2 Statistical Learning","heading":"2.3 Assessing Model Accuracy","text":"one statistical learning approach performs better approaches possible data sets. , care needs taken choose approach use given data set produce best results. number important concepts arise selecting statistical learning approach specific data set:Measuring quality fitThe bias-variance trade-","code":""},{"path":"chapter-2.html","id":"in-the-regression-setting","chapter":"2 Statistical Learning","heading":"2.3.1 In the Regression Setting","text":"regression setting, commonly used quality fit measure training data mean squared error \\(\\mathit{MSE}\\), given \\[\n\\mathit{MSE}_{\\mathrm{training}} = \\frac 1 n \\sum_{= 1}^n (y_i - \\hat f(x_i))^2,\n\\tag{2.3}\n\\]\\(\\hat f(x_i)\\) represents prediction \\(\\hat f\\) gives \\(\\)th observation. predicted responses close true responses \\(\\mathit{MSE}\\) small; predicted responses far true responses \\(\\mathit{MSE}\\) large. generally really care value accurately predicting data already seen particularly useful.goal assess accuracy predictions apply method previously unseen test data, can compute mean squared error test observations, given \\[\n\\mathit{MSE}_{\\mathrm{testing}} = \\frac 1 n \\sum_{= 1}^n (y_0 - \\hat f(x_0))^2,\n\\tag{2.4}\n\\]\\((x_0, y_0)\\) previously unseen test observation used train statistical learning model. want choose model gives lowest test \\(\\mathit{MSE}\\) minimizing distance \\(\\hat f(x_0)\\) \\(y_0\\). test data set available can simply evaluate Equation (2.4) choose statistical learning model \\(\\mathit{MSE}\\) smallest. test data set available can use cross-validation, method estimating test \\(\\mathit{MSE}\\) using training data set.expected test \\(\\mathit{MSE}\\) given value \\(x_0\\) can always decomposed sum three fundamental quantities: variance \\(\\hat f(x_0)\\), squared bias \\(\\hat f(x_0)\\), variance error term \\(\\epsilon\\), written \\[\nE \\bigl(y_0 - \\hat f(x_0) \\bigr)^2 = \\mathrm{Var}(\\hat f(x_0)) +\n                                     [\\mathrm{Bias(\\hat f(x_0))}]^2 +\n                                     \\mathrm{Var}(\\epsilon),\n\\tag{2.5}\n\\]notation \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) defines expected test \\(\\mathit{MSE}\\) \\(x_0\\). overall expected test \\(\\mathit{MSE}\\) given averaging \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) possible values \\(x_0\\) test data set.variance \\(\\hat f\\) refers amount \\(\\hat f\\) change estimated using different training set. bias \\(\\hat f\\) refers error introduced approximating real-life problem much simpler model. general, models become flexible, variance increase bias decrease. relative rate change variance bias determines whether test \\(\\mathit{MSE}\\) increases decreases. variance bias can change different rates different data sets, challenge lies finding model variance bias lowest.","code":""},{"path":"chapter-2.html","id":"in-the-classification-setting","chapter":"2 Statistical Learning","heading":"2.3.2 In the Classification Setting","text":"classification setting, commonly used quality fit measure training data error rate, given \\[\n\\frac 1 n \\sum_{= 1}^n (y_i \\ne \\hat y_i).\n\\tag{2.6}\n\\]:\\(\\hat y_i\\) predicted class label \\(\\)th observation using \\(\\hat f\\)\\((y_i \\ne \\hat y_i)\\) indicator variable equals one \\(y_i \\ne \\hat y_i\\) (incorrectly classified) zero \\(y_i = \\hat y_i\\) (correctly classified)test error rate set test observations \\((x_0, y_0)\\) given \\[\n\\frac 1 n \\sum_{= 1}^n (y_0 \\ne \\hat y_0),\n\\tag{2.7}\n\\]\\(\\hat y_0\\) predicted class label results applying classifier test observation predictor \\(x_0\\).bias-variance trade-present classification setting ; variance bias lowest test error rate smallest given data set.","code":""},{"path":"chapter-2.html","id":"exercises","chapter":"2 Statistical Learning","heading":"2.4 Exercises","text":"","code":""},{"path":"chapter-2.html","id":"prerequisites","chapter":"2 Statistical Learning","heading":"Prerequisites","text":"access data sets functions used complete Chapter 2 exercises, load following packages.","code":"\n# library(ISLR2)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(patchwork)"},{"path":"chapter-2.html","id":"conceptual","chapter":"2 Statistical Learning","heading":"Conceptual","text":"Exercise 2.1  parts () (d), indicate whether generally expect performance flexible statistical learning method better worse inflexible method. Justify answer.sample size \\(n\\) extremely large, number predictors \\(p\\) small.Answer. Better. flexible model generally able better estimate true \\(f\\) avoid overfitting extremely large sample size small number predictors. exception true \\(f\\) linear, inflexible model generally perform better; however, real world relationships linear, lower bias flexible model generally lead better quality fit.number predictors \\(p\\) extremely large, number observations \\(n\\) small.Answer. Worse. flexible model generally lead overfitting training data number predictors large sample size small. inflexible model less likely lead overfitting scenario, generally better job giving accurate predictions new observations flexible overfit model.relationship predictors response highly non-linear.Answer. Better. flexible model generally able fit highly non-linear relationship better inflexible model relative rate decrease bias tends much greater relative increase variance \\(f\\) highly non-linear. left right plots Figure 2.12 Page 36 book demonstrate nicely.variance error terms, .e. \\(\\sigma^2 = \\mathrm{Var}(\\epsilon)\\), extremely high.Answer. Worse. flexible model generally lead overfitting training data variance error terms extremely high. \\(Y\\) partly function \\(\\epsilon\\), variance error terms extremely high variance \\(Y\\) also extremely high, mainly due random error. flexible model tries find patterns noise likely pick patterns caused random chance rather true properties unknown function \\(f\\). bias inflexible model preferable situation, give stable predictions long run, likely preferable making essentially random predictions flexible model give circumstances.Exercise 2.2  Explain whether scenario classification regression problem, indicate whether interested inference prediction. Finally, provide \\(n\\) \\(p\\).collect set data top 500 firms US. firm record profit, number employees, industry, CEO salary. interested understanding factors affect CEO salary.Answer. Regression, inference, \\(n = 500\\), \\(p = 3\\).considering launching new product wish know whether success failure. collect data 20 similar products previously launched. product recorded whether success failure, price charged product, marketing budget, competition price, ten variables.Answer. Classification, prediction, \\(n = 20\\), \\(p = 13\\).interested predicting % change USD/Euro exchange rate relation weekly changes world stock markets. Hence collect weekly data 2012. week record % change USD/Euro, % change US market, % change British market, % change German market.Answer. Regression, prediction, \\(n = 52\\), \\(p = 3\\).Exercise 2.3  now revisit bias-variance decomposition.Provide sketch typical (squared) bias, variance, training error, test error, Bayes (irreducible) error curves, single plot, go less flexible statistical learning methods towards flexible approaches. \\(x\\)-axis represent amount flexibility method, \\(y\\)-axis represent values curve. five curves. Make sure label one.Answer.Explain five curves shape displayed part ().Answer. value measure part () changes different rates different directions go less flexible statistical learning methods towards flexible approaches. Broadly, curve different shape. Specifically:Bias refers error introduced approximating real-life problem much simpler model. general, bias decrease models become flexible, flexibility lead better approximations real-life problem, reducing error.Variance refers amount \\(\\hat f\\) change estimated using different training set. general, variance increase models become flexible, flexible model pick patterns training data better may differ training sets.Training error refers quality fit model training data. general, training error steadily increase models become flexible, flexibility allow model closely follow training data.Test error refers quality fit trained model test data. general, test error U-shaped bias-variance trade-.Irreducible error refers random error independent \\(X\\). irreducible error independent \\(X\\), remains stable regardless model flexibility.Exercise 2.4  now think real-life applications statistical learning.Describe three real-life applications classification might useful. Describe response, well predictors. goal application inference prediction? Explain answer.Answer.Predicting whether customer likely purchase items discounted web store. predictor whether likely purchase items; predictors purchase history discounted full price items, purchase frequency. goal prediction order determine whether show discounts customer less frequently.Predicting whether customer likely purchase items discounted web store. predictor whether likely purchase items; predictors purchase history discounted full price items, purchase frequency. goal prediction order determine whether show discounts customer less frequently.Understanding demographic groups enjoy Star Trek. response whether someone Trekkie; predictors various demographic variables age, gender identity, ethnicity, . goal inference whether Star Trek appeals specific groups enjoyable diverse audience.Understanding demographic groups enjoy Star Trek. response whether someone Trekkie; predictors various demographic variables age, gender identity, ethnicity, . goal inference whether Star Trek appeals specific groups enjoyable diverse audience.Understanding kind people like raisin cookies. response whether someone likes raisin cookies; predictors levels psychopathy sadism. goal inference whether people like raisin cookies secretly evil.Understanding kind people like raisin cookies. response whether someone likes raisin cookies; predictors levels psychopathy sadism. goal inference whether people like raisin cookies secretly evil.Describe three real-life applications regression might useful. Describe response, well predictors. goal application inference prediction? Explain answer.Answer.Predicting temperature tomorrow live. response temperature degrees Celsius; predictors average temperature live past seven days, latitude, longitude, altitude live, month year. goal prediction order decide outfit might wear tomorrow.Predicting temperature tomorrow live. response temperature degrees Celsius; predictors average temperature live past seven days, latitude, longitude, altitude live, month year. goal prediction order decide outfit might wear tomorrow.Understanding aspects video game lead people playing longer. response minutes spent playing game; predictors game genre, whether game original sequel, whether game singleplayer, multiplayer, , game developer. goal inference determine makes game engaging.Understanding aspects video game lead people playing longer. response minutes spent playing game; predictors game genre, whether game original sequel, whether game singleplayer, multiplayer, , game developer. goal inference determine makes game engaging.Predicting long take injury heal. response time heal; predictors injury severity, age, indicators physical health. goal prediction order give patients idea time recovery.Predicting long take injury heal. response time heal; predictors injury severity, age, indicators physical health. goal prediction order give patients idea time recovery.Describe three real-life applications cluster analysis might useful.Answer.Identifying whether brain activity similar within individuals. predictor variable repeated measure functional connectivity. goal inference order see whether brain activity recordings cluster together individual.Identifying whether brain activity similar within individuals. predictor variable repeated measure functional connectivity. goal inference order see whether brain activity recordings cluster together individual.Identifying whether patients depression can classified subgroups. predictor variables responses depression inventory. goal inference order understand whether depression inventory measures multiple types depression.Identifying whether patients depression can classified subgroups. predictor variables responses depression inventory. goal inference order understand whether depression inventory measures multiple types depression.Identifying whether species families animals identified biologists aligns species’ genomes. predictor variable species genome. goal understanding whether clusters identified observation align clusters identified genetics.Identifying whether species families animals identified biologists aligns species’ genomes. predictor variable species genome. goal understanding whether clusters identified observation align clusters identified genetics.Exercise 2.5  advantages disadvantages flexible (versus less flexible) approach regression classification? circumstances might flexible approach preferred less flexible approach? might less flexible approach preferred?Answer. advantage flexible approach less bias, model better approximates reality, typically better prediction. disadvantage higher variance potential overfitting. flexible model might preferred goal statistical learning prediction. less flexible approach might preferred goal statistical learning inference, simpler models tend easier understand.Exercise 2.6  Describe differences parametric non-parametric statistical learning approach. advantages parametric approach regression classification (opposed non-parametric approach)? disadvantages?Answer. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. advantage easier estimate set parameters fit entirely arbitrary function \\(f\\). disadvantage model choose usually match true unknown form \\(f\\), can lead poor estimates far true \\(f\\).Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly. advantage becomes easier accurately fit wider range possible shapes \\(f\\). However, disadvantage accurately requires much larger number observations since problem estimating \\(f\\) small number parameters.Exercise 2.7  table provides training data set containing six observations, three predictors, one qualitative response variable.Suppose wish use data set make prediction \\(Y\\) \\(X1 = X2 = X3 = 0\\) using K-nearest neighbours.Compute Euclidean distance observation test point, \\(X1 = X2 = X3 = 0\\).Answer.equation Euclidean distance given \\[\nd(p, q) = \\sqrt{ \\sum_{= 1}^n (q_i - p_i)^2},\n\\tag{2.8}\n\\]\\(d(p, q)\\) represents distance points \\(p\\) \\(q\\) \\(n\\) dimensional Euclidean space, \\(q_i\\) \\(p_i\\) represent Euclidean vectors, starting origin \\(n\\) dimensional Euclidean space.can computed R like .prediction \\(K = 1\\)? ?Answer. \\(K = 1\\) prediction test point whatever training observation closest test point. Observation 5 (Green) closest prediction test point Green.prediction \\(K = 3\\)? ?Answer. \\(K = 3\\) prediction test point given class highest estimated probability based three points training data closest test point. observations 5 (Green), 6 (Red), 2 (Red) closest, results probabilities 1/3 Green class 2/3 Red class. prediction test point Red.Bayes decision boundary problem highly non-linear, expect best value \\(K\\) large small? ?Answer. expect smaller value K give best predictions. small. \\(K = 1\\) KNN decision boundary overly flexible, large value \\(K\\) sufficiently flexible.","code":"\neuclidean_distance <- function(q, p) sqrt(sum((q - p)^2))\n\nobs_test <- c( 0, 0, 0)\n\nobs_train <- list(\n  obs_01 = c( 0, 3, 0),\n  obs_02 = c( 2, 0, 0),\n  obs_03 = c( 0, 1, 3),\n  obs_04 = c( 0, 1, 2),\n  obs_05 = c(-1, 0, 0),\n  obs_06 = c( 1, 1, 1)\n)\n\nlapply(obs_train, function(x) euclidean_distance(obs_test, x))\n#> $obs_01\n#> [1] 3\n#> \n#> $obs_02\n#> [1] 2\n#> \n#> $obs_03\n#> [1] 3.162278\n#> \n#> $obs_04\n#> [1] 2.236068\n#> \n#> $obs_05\n#> [1] 1\n#> \n#> $obs_06\n#> [1] 1.732051"},{"path":"chapter-2.html","id":"applied","chapter":"2 Statistical Learning","heading":"Applied","text":"Exercise 2.8  exercise relates ISLR2::College data set. contains number \nvariables 777 different universities colleges US. help page ?ISLR2::College data set contains description data set 18 variables measures.Load ISLR2::College data set R.Explore college data.Produce numerical summary variables data set.Answer.Table 2.1: Data summaryVariable type: factorVariable type: numericProduce scatterplot matrix first ten columns variables data.Answer.Produce side--side boxplots Outstate versus Private.Answer.Create new qualitative variable, called Elite, binning Top10perc variable. going divide universities two groups based whether proportion students coming top 10% high school classes exceeds 50%. See many elite universities , produce side--side boxplots Outstate versus Elite.Answer.Produce histograms differing numbers bins quantitative variables.Answer.Continue exploring data, provide brief summary discover.Answer. Passing one.Exercise 2.9  exercise involves Auto data set.predictors quantitative, qualitative?Answer.variables name quantitative.range quantitative predictor?range quantitative predictor?mean standard deviation quantitative\npredictor?mean standard deviation quantitative\npredictor?Answer.Now remove 10th 85th observations. range, mean, standard deviation predictor subset data remains?Answer.Using full data set, investigate predictors graphically, using scatterplots tools choice. Create plots highlighting relationships among predictors. Comment findings.Answer.plots show relationships expect different variables based laws physics. interesting observations:appear two clusters cars based weight acceleration plotThe relationship weight mpg nonlinearMost cars even number cylindersSuppose wish predict gas mileage (mpg) basis variables. plots suggest variables might useful predicting mpg? Justify answer.Answer.plots suggest mpg nonlinear relationships least variables data set (didn’t explore ). particular, weight horsepower show clear relationships might suitable making predictions, long can deal heteroskedasticity.Exercise 2.10  exercise involves Boston housing data set.begin, load Boston data set. many rows data set? many columns? rows columns represent?Answer.columns represent variables might related value houses area. row represents suburb Boston.Make pairwise scatterplots predictors (columns) \ndata set. Describe findings.Answer.explored scatterplots shown . general aren’t many easily interpretable relationships variables data set. pairs clusters data, overall trend; others floor effects communities value zero (e.g., crime).predictors associated per capita crime rate?\n, explain relationship.Answer.plots illustrative relationship crime variables dataset. less, crime seems occur specific range variable associated .census tracts Boston appear particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment range predictor.Answer.plots already get features data. range three predictors show interesting patterns . Crime per capita wide range, almost none others lot comparison. Tax similarly wide range. Although pupil-teacher ratios skinnier range, differences minimum maximum quite meaningful classroom—teacher minimum theoretically twice much time student compared teacher maximum.many census tracts data set bound Charles river?Answer.median pupil-teacher ratio among towns data set?Answer.census tract Boston lowest median value owner-occupied homes? values predictors census tract, values compare overall ranges predictors? Comment findings.Answer.suburb lowest median value owner-occupied homes values predictors listed .comparison range values:crim upper endzn minimumindus around two-thirds upThe suburb bound rivernox upper endrm near middleage maximumdis almost minimumrad maximumtax near maximumptratio near maximumlstat around two-thirds upmedv minimumIt unsurprising suburb lowest median value owner-occupied homes. older suburb small residential lots, high traffic density resultant increase pollution crime, land taken businesses, high taxes. characteristics attractive home buyers, likely isn’t much demand homes suburb.data set, many census tracts average seven rooms per dwelling? eight rooms per dwelling? Comment census tracts average eight rooms per dwelling.Answer.Several communities seem near , given shared values many variables. low tax rates, low crime, smaller lots, medium high pollution, high number rooms per dwelling.","code":"\n# Make sure to preserve the row names with college names in them because they\n# might be useful later\ncollege <- as_tibble(ISLR2::College, rownames = NA)\nglimpse(college)\n#> Rows: 777\n#> Columns: 18\n#> $ Private     <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes~\n#> $ Apps        <dbl> 1660, 2186, 1428, 417, 193, 587, 353, ~\n#> $ Accept      <dbl> 1232, 1924, 1097, 349, 146, 479, 340, ~\n#> $ Enroll      <dbl> 721, 512, 336, 137, 55, 158, 103, 489,~\n#> $ Top10perc   <dbl> 23, 16, 22, 60, 16, 38, 17, 37, 30, 21~\n#> $ Top25perc   <dbl> 52, 29, 50, 89, 44, 62, 45, 68, 63, 44~\n#> $ F.Undergrad <dbl> 2885, 2683, 1036, 510, 249, 678, 416, ~\n#> $ P.Undergrad <dbl> 537, 1227, 99, 63, 869, 41, 230, 32, 3~\n#> $ Outstate    <dbl> 7440, 12280, 11250, 12960, 7560, 13500~\n#> $ Room.Board  <dbl> 3300, 6450, 3750, 5450, 4120, 3335, 57~\n#> $ Books       <dbl> 450, 750, 400, 450, 800, 500, 500, 450~\n#> $ Personal    <dbl> 2200, 1500, 1165, 875, 1500, 675, 1500~\n#> $ PhD         <dbl> 70, 29, 53, 92, 76, 67, 90, 89, 79, 40~\n#> $ Terminal    <dbl> 78, 30, 66, 97, 72, 73, 93, 100, 84, 4~\n#> $ S.F.Ratio   <dbl> 18.1, 12.2, 12.9, 7.7, 11.9, 9.4, 11.5~\n#> $ perc.alumni <dbl> 12, 16, 30, 37, 2, 11, 26, 37, 23, 15,~\n#> $ Expend      <dbl> 7041, 10527, 8735, 19016, 10922, 9727,~\n#> $ Grad.Rate   <dbl> 60, 56, 54, 59, 15, 55, 63, 73, 80, 52~\nskim(college)\ncollege %>% \n  select(1:10) %>% \n  ggpairs()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\ncollege %>% \n  ggplot(aes(y = Outstate)) +\n    geom_boxplot() +\n    facet_wrap(~Private) +\n    labs(title = \"Private universities receive more out of state tuition\")\ncollege %>% \n  mutate(Elite = if_else(Top10perc > 50, \"Yes\", \"No\") %>% as_factor()) %>% \n  ggplot(aes(y = Outstate)) +\n    geom_boxplot() +\n    facet_wrap(~Elite) +\n    labs(title = \"Elite universities receive more out of state tuition\")\ngghist <- function(x, binwidth) {\n  ggplot(college, aes(x = {{ x }})) +\n    geom_histogram(binwidth = binwidth)\n}\n\ngghist(Enroll, 300) + gghist(Enroll, 200) + gghist(Enroll, 100) +\n  plot_annotation(\n    title = \"College enrollment follows a power law distribution\"\n  )\n\ngghist(PhD, 20) + gghist(PhD, 10) + gghist(PhD, 5) +\n  plot_annotation(\n    title = \"The majority of college faculty have a PhD\"\n  )\n\ngghist(Books, 100) + gghist(Books, 50) + gghist(Books, 25) +\n  plot_annotation(\n    title = \"Books cost way too much at colleges\"\n  )\nauto <- as_tibble(ISLR2::Auto)\nglimpse(auto)\n#> Rows: 392\n#> Columns: 9\n#> $ mpg          <dbl> 18, 15, 18, 16, 17, 15, 14, 14, 14, 1~\n#> $ cylinders    <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8~\n#> $ displacement <dbl> 307, 350, 318, 304, 302, 429, 454, 44~\n#> $ horsepower   <int> 130, 165, 150, 150, 140, 198, 220, 21~\n#> $ weight       <int> 3504, 3693, 3436, 3433, 3449, 4341, 4~\n#> $ acceleration <dbl> 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9~\n#> $ year         <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~\n#> $ origin       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ name         <fct> chevrolet chevelle malibu, buick skyl~\nauto %>% \n  pivot_longer(!name, \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value),\n    mean = mean(value),\n    sd   = sd(value)\n  )\n#> # A tibble: 8 x 5\n#>   predictor      min    max    mean      sd\n#>   <chr>        <dbl>  <dbl>   <dbl>   <dbl>\n#> 1 acceleration     8   24.8   15.5    2.76 \n#> 2 cylinders        3    8      5.47   1.71 \n#> 3 displacement    68  455    194.   105.   \n#> 4 horsepower      46  230    104.    38.5  \n#> 5 mpg              9   46.6   23.4    7.81 \n#> 6 origin           1    3      1.58   0.806\n#> 7 weight        1613 5140   2978.   849.   \n#> 8 year            70   82     76.0    3.68\nauto %>% \n  slice(-10:-85) %>% \n  pivot_longer(!name, \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value),\n    mean = mean(value),\n    sd   = sd(value)\n  )\n#> # A tibble: 8 x 5\n#>   predictor       min    max    mean      sd\n#>   <chr>         <dbl>  <dbl>   <dbl>   <dbl>\n#> 1 acceleration    8.5   24.8   15.7    2.69 \n#> 2 cylinders       3      8      5.37   1.65 \n#> 3 displacement   68    455    187.    99.7  \n#> 4 horsepower     46    230    101.    35.7  \n#> 5 mpg            11     46.6   24.4    7.87 \n#> 6 origin          1      3      1.60   0.820\n#> 7 weight       1649   4997   2936.   811.   \n#> 8 year           70     82     77.1    3.11\nggscatter <- function(x, y) {\n  ggplot(auto, aes({{ x }}, {{ y }})) +\n    geom_point()\n}\n\nggscatter(weight, acceleration) + ggscatter(weight, mpg) +\n  plot_annotation(\n    title = \"Lighter cars are more fuel efficient and accelerate faster\"\n  )\n\nggscatter(cylinders, displacement) +\n  labs(\n    title = \"Cars with more cylinders have higher displacement\"\n  )\nggscatter(weight, mpg) + ggscatter(acceleration, mpg) +\n  ggscatter(horsepower, mpg) + ggscatter(displacement, mpg)\nboston <- as_tibble(ISLR2::Boston)\nglimpse(boston)\n#> Rows: 506\n#> Columns: 13\n#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.0690~\n#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5,~\n#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, ~\n#> $ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, ~\n#> $ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, ~\n#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, ~\n#> $ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.~\n#> $ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, ~\n#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 31~\n#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, ~\n#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43,~\n#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, ~\nggscatter <- function(x, y) {\n  ggplot(boston, aes({{ x }}, {{ y }})) +\n    geom_point()\n}\n\nggscatter(chas, crim) +\n  labs(\n    title = \"Crime occurs more often away from the Charles River\",\n    subtitle = \"Or, most suburbs are away from the Charles River\"\n  )\n\nggscatter(zn, nox) +\n  labs(\n    title = paste0(\n      \"Higher nitrogen oxide concentration in suburbs with all residential \\n\",\n      \"land zoning below 25,000 sq.ft.\")\n  )\n\nggscatter(ptratio, medv) +\n  labs(\n    title = paste0(\n      \"Higher pupil to teacher ratios are related to less \\n\",\n      \"valuable owner-occupied homes\"\n    )\n  )\nggscatter(crim, rad) +\n  labs(\n    title = \"Suburbs with better access to radial highways have more crime\"\n  )\n\nggscatter(crim, tax) +\n  labs(\n    title = \"Suburbs with higher property tax rates have more crime\"\n  )\nboston %>% \n  select(crim, tax, ptratio) %>% \n  pivot_longer(everything(), \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value)\n  )\n#> # A tibble: 3 x 3\n#>   predictor       min   max\n#>   <chr>         <dbl> <dbl>\n#> 1 crim        0.00632  89.0\n#> 2 ptratio    12.6      22  \n#> 3 tax       187       711\nboston %>% \n  mutate(bound = if_else(chas == 1, \"Yes\", \"No\")) %>% \n  group_by(bound) %>% \n  summarise(\n    n = n()\n  )\n#> # A tibble: 2 x 2\n#>   bound     n\n#>   <chr> <int>\n#> 1 No      471\n#> 2 Yes      35\nboston %>% \n  summarise(\n    median_ptratio = median(ptratio)\n  )\n#> # A tibble: 1 x 1\n#>   median_ptratio\n#>            <dbl>\n#> 1           19.0\nboston %>% \n  arrange(desc(medv)) %>% \n  tail(1) %>% \n  glimpse()\n#> Rows: 1\n#> Columns: 13\n#> $ crim    <dbl> 67.9208\n#> $ zn      <dbl> 0\n#> $ indus   <dbl> 18.1\n#> $ chas    <int> 0\n#> $ nox     <dbl> 0.693\n#> $ rm      <dbl> 5.683\n#> $ age     <dbl> 100\n#> $ dis     <dbl> 1.4254\n#> $ rad     <int> 24\n#> $ tax     <dbl> 666\n#> $ ptratio <dbl> 20.2\n#> $ lstat   <dbl> 22.98\n#> $ medv    <dbl> 5\nboston %>% \n  pivot_longer(everything(), \"predictor\") %>% \n  group_by(predictor) %>% \n  summarise(\n    min  = min(value),\n    max  = max(value)\n  )\n#> # A tibble: 13 x 3\n#>    predictor       min     max\n#>    <chr>         <dbl>   <dbl>\n#>  1 age         2.9     100    \n#>  2 chas        0         1    \n#>  3 crim        0.00632  89.0  \n#>  4 dis         1.13     12.1  \n#>  5 indus       0.46     27.7  \n#>  6 lstat       1.73     38.0  \n#>  7 medv        5        50    \n#>  8 nox         0.385     0.871\n#>  9 ptratio    12.6      22    \n#> 10 rad         1        24    \n#> 11 rm          3.56      8.78 \n#> 12 tax       187       711    \n#> 13 zn          0       100\nboston %>% \n  filter(rm > 7) %>% \n  count()\n#> # A tibble: 1 x 1\n#>       n\n#>   <int>\n#> 1    64\nboston %>% \n  filter(rm > 8)\n#> # A tibble: 13 x 13\n#>      crim    zn indus  chas   nox    rm   age   dis   rad\n#>     <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <int>\n#>  1 0.121      0  2.89     0 0.445  8.07  76    3.50     2\n#>  2 1.52       0 19.6      1 0.605  8.38  93.9  2.16     5\n#>  3 0.0201    95  2.68     0 0.416  8.03  31.9  5.12     4\n#>  4 0.315      0  6.2      0 0.504  8.27  78.3  2.89     8\n#>  5 0.527      0  6.2      0 0.504  8.72  83    2.89     8\n#>  6 0.382      0  6.2      0 0.504  8.04  86.5  3.22     8\n#>  7 0.575      0  6.2      0 0.507  8.34  73.3  3.84     8\n#>  8 0.331      0  6.2      0 0.507  8.25  70.4  3.65     8\n#>  9 0.369     22  5.86     0 0.431  8.26   8.4  8.91     7\n#> 10 0.612     20  3.97     0 0.647  8.70  86.9  1.80     5\n#> 11 0.520     20  3.97     0 0.647  8.40  91.5  2.29     5\n#> 12 0.578     20  3.97     0 0.575  8.30  67    2.42     5\n#> 13 3.47       0 18.1      1 0.718  8.78  82.9  1.90    24\n#> # ... with 4 more variables: tax <dbl>, ptratio <dbl>,\n#> #   lstat <dbl>, medv <dbl>"},{"path":"chapter-3.html","id":"chapter-3","chapter":"3 Linear Regression","heading":"3 Linear Regression","text":"Chapter 3 (ordinary least squares polynomial) linear regression, parametric supervised statistical learning method. Many advanced statistical learning approaches generalizations extensions linear regression, good grasp linear regression important understanding later methods book.also discusses K-nearest neighbours regression (KNN regression), non-parametric supervised statistical learning method.","code":""},{"path":"chapter-3.html","id":"exercises-1","chapter":"3 Linear Regression","heading":"3.1 Exercises","text":"","code":""},{"path":"chapter-3.html","id":"prerequisites-1","chapter":"3 Linear Regression","heading":"Prerequisites","text":"access data sets functions used complete Chapter 3 exercises, load following packages.","code":"\nlibrary(ISLR2)\nlibrary(tidyverse)\n# library(skimr)\n# library(GGally)\n# library(patchwork)"},{"path":"chapter-3.html","id":"conceptual-1","chapter":"3 Linear Regression","heading":"Conceptual","text":"Exercise 3.1  Describe null hypotheses p-values given Table 3.4 correspond. Explain conclusions can draw based p-values. explanation phrased terms sales, TV, radio, newspaper, rather terms coefficients linear model.Table 3.4Answer.null hypotheses p-values Table 3.4 correspond coefficient term equal zero, \\(\\beta_i = 0\\), use test whether term associated response variable. can use p-values table infer whether coefficients sufficiently far zero can confident non-zero, thus associated response variable.Based p-values Table 3.4, can conclude :$1000 increase TV advertising budget associated average increase product sales 46 units holding advertising budgets radio newspaper fixed.$1000 increase radio advertising budget associated average increase product sales 189 units holding advertising budgets TV newspaper fixed.$1000 increase newspaper advertising budget associated change product sales.Data note: Advertising data sales thousands units advertising budgets thousands dollars.Exercise 3.2  Carefully explain differences KNN classifier KNN regression methods.Answer.Given value \\(K\\) test observation \\(x_0\\), KNN classifier KNN regression first identify \\(K\\) training observations closest \\(x_0\\), represented \\(\\mathcal N_0\\). KNN classifier \\(x_0\\) discrete class, KNN regression \\(x_0\\) continuous value. methods diverge \\(\\mathcal N_0\\).KNN classifier estimates conditional probability test observation \\(x_0\\) belongs class \\(j\\) fraction training observations \\(\\mathcal N_0\\) whose response value \\(y_i\\) equals \\(j\\):\\[\n\\mathrm{Pr}(Y = j|X = x_0) =  \\frac{1}{K} \\sum_{\\\\mathcal N_0} (y_i = j).\n\\tag{3.1}\n\\]KNN regression, hand, estimates \\(f(x_0)\\) using average training observations \\(\\mathcal N_0\\):\\[\n\\hat f(x_0) = \\frac{1}{K} \\sum_{\\{\\mathcal N}_0} y_i.\n\\tag{3.2}\n\\]can seen Equations (3.1) (3.2), two main differences KNN classifier KNN regression methods:right side equation, KNN classifier uses indicator function \\((y_i = j)\\) determine whether response value \\(y_i\\) equals \\(j\\) (equals 1, 0). summed get numerator conditional probability class (denominator \\(K\\)). KNN regression, hand, simply uses response value \\(y_i\\) , since ’s continuous value. also summed get numerator fraction (\\(K\\) denominator), although makes sense think average training observations \\(\\mathcal N_0\\).left hand side equation, KNN classifier estimate conditional probability test observation \\(x_0\\) belongs class \\(j\\). can use probability assign \\(x_0\\) class \\(j\\) highest probability, choose (although don’t , probabilities equally useful). KNN regression estimate form \\(f(x_0)\\), corresponds prediction point test observation \\(x_0\\) since \\(\\hat Y = \\hat f(X)\\).Thus, main difference two methods KNN classifier method used predict class observation likely belongs based classes \\(K\\) nearest neighbours, whereas KNN regression method used predict value observation response variable based response values \\(K\\) nearest neighbours.Exercise 3.3  Suppose data set five predictors, \\(X_1 =\\) GPA, \\(X_2 =\\) IQ, \\(X_3 =\\) Level (1 College 0 High School), \\(X_4 =\\) Interaction GPA IQ, \\(X_5 =\\) Interaction GPA Level. response starting salary graduation (thousands dollars). Suppose use least squares fit model, get \\(\\hat \\beta_0 = 50\\), \\(\\hat \\beta_1 = 20\\), \\(\\hat \\beta_2 = 0.07\\), \\(\\hat \\beta_3 = 35\\), \\(\\hat \\beta_4 = 0.01\\), \\(\\hat \\beta_5 = −10\\).answer correct, ?fixed value IQ GPA, high school graduates earn , average, college graduates.Answer.fixed value IQ GPA, college graduates earn , average, high school graduates.Answer.fixed value IQ GPA, high school graduates earn , average, college graduates provided GPA high enough.Answer.fixed value IQ GPA, college graduates earn , average, high school graduates provided GPA high enough.Answer.Predict salary college graduate IQ 110 GPA 4.0.Answer.True false: Since coefficient GPA/IQ interaction term small, little evidence interaction effect. Justify answer.Answer.","code":"\nread_csv(file.path(\"data\", \"advertising.csv\")) %>% \n  lm(sales ~ TV + radio + newspaper, data = .) %>% \n  summary()\n#> New names:\n#> * `` -> ...1\n#> Rows: 200 Columns: 5\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> dbl (5): ...1, TV, radio, newspaper, sales\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> \n#> Call:\n#> lm(formula = sales ~ TV + radio + newspaper, data = .)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -8.8277 -0.8908  0.2418  1.1893  2.8292 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  2.938889   0.311908   9.422   <2e-16 ***\n#> TV           0.045765   0.001395  32.809   <2e-16 ***\n#> radio        0.188530   0.008611  21.893   <2e-16 ***\n#> newspaper   -0.001037   0.005871  -0.177     0.86    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.686 on 196 degrees of freedom\n#> Multiple R-squared:  0.8972, Adjusted R-squared:  0.8956 \n#> F-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16"}]
