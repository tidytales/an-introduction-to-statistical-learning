[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"book self study second edition book Introduction Statistical Learning Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.","code":""},{"path":"chapter-1.html","id":"chapter-1","chapter":"1 Introduction","heading":"1 Introduction","text":"Chapter 1 provides brief overview statistical learning, refers vast set tools understanding data. tools statistical learning can classified either supervised unsupervised.Supervised statistical learning involves building statistical model predicting output (called response dependent variable) based one inputs (called features, predictors, independent variables). typical supervised learning setting access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations, response \\(Y\\) also measured \\(n\\) observations. goal predict \\(Y\\) using \\(X_1, X_2, \\dots , X_p\\). response \\(Y\\) can quantitative (continuous, numerical) qualitative (categorical, non-numerical). goal predict numerical value, call regression problem. goal predict non-numerical value call classification problem. Supervised statistical learning can used exploratory confirmatory data analysis.Unsupervised statistical learning involves cases one inputs used learn relationships structure data absence output. typical unsupervised learning setting () access set \\(p\\) features \\(X_1, X_2, \\dots , X_p,\\) measured \\(n\\) observations. goal discover interesting things measurements \\(X_1, X_2, \\dots , X_p\\). goal partition observations subgroups based (dis)similarity, call clustering problem. Unsupervised statistical learning typically used exploratory data analysis; possible check work unsupervised statistical learning (don’t know true answer since problem unsupervised), used confirmatory data analysis.Semi-supervised statistical learning involves building statistical model predicting output based one inputs subset observations predictor measurements response measurement, wish incorporate observations model. typical semi-supervised learning setting access set \\(n\\) observations. \\(m\\) observations, \\(m < n\\), predictor measurements response measurement; remaining \\(n - m\\) observations predictor measurements response measurement. goal predict \\(Y\\) incorporating \\(m\\) observations response measurements available well \\(n - m\\) observations . topic beyond scope book explored .","code":""},{"path":"chapter-1.html","id":"notation-and-simple-matrix-algebra","chapter":"1 Introduction","heading":"1.1 Notation and Simple Matrix Algebra","text":"Chapter 1 also discusses notation conventions used textbook, starting Page 9. Briefly:\\(n\\) represents number observations sample\\(p\\) represents number variables available data set\\(x_{ij}\\) represents value \\(j\\)th variable \\(\\)th observation, \\(= 1, 2, \\dots, n\\) \\(j = 1, 2, \\dots, p\\).\\(\\mathbf X\\) represents \\(n \\times p\\) matrix whose \\((,j)\\)th element \\(x_{ij}\\)\\(x_i\\) represents rows matrix \\(\\mathbf X\\) \\(x_i\\) vector length \\(p\\) containing \\(p\\) variable measurements \\(\\)th observation.\\(\\mathbf x_j\\) represents columns matrix \\(\\mathbf X\\) \\(\\mathbf x_j\\) vector length \\(n\\) containing \\(n\\) observation measurements \\(j\\)th variable.\\(y_i\\) represents \\(\\)th observation response variable.\\(\\mathbf y\\) represents vector length \\(n\\) containing set response variable measurements predictions.","code":""},{"path":"chapter-1.html","id":"data-sets-used-in-labs-and-exercises","chapter":"1 Introduction","heading":"1.2 Data Sets Used in Labs and Exercises","text":"book uses data sets ISLR2 package (available CRAN) one data set included part base R distribution labs exercises.","code":""},{"path":"chapter-1.html","id":"book-website","chapter":"1 Introduction","heading":"1.3 Book Website","text":"website Introduction Statistical Learning located https://www.statlearning.org. contains number additional resources may useful.","code":""},{"path":"chapter-2.html","id":"chapter-2","chapter":"2 Statistical Learning","heading":"2 Statistical Learning","text":"Chapter 2 formalizes concept statistical learning introducing general statistical model used modelling relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), can written \\[\nY = f(X) + \\epsilon.\n\\tag{2.1}\n\\]:\\(Y\\) represents response variable data set\\(X\\) represents set variables data set\\(X_p\\) represents \\(p\\)th variable data set\\(f(\\dots)\\) represents fixed unknown function input(s)\\(\\epsilon\\) represents random error term independent \\(X\\) mean zeroThe goal statistical learning estimate \\(f\\). two main reasons estimating \\(f\\): prediction inference. Depending whether ultimate goal prediction, inference, combination two, different methods estimating \\(f\\) may appropriate. general, trade-prediction accuracy model interpretability. Models make accurate predictions tend less interpretable, models interpretable tend make less accurate predictions (although always case, due potential overfitting highly flexible models).methods use estimate \\(f\\) can characterized either parametric non-parametric. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly.","code":""},{"path":"chapter-2.html","id":"prediction","chapter":"2 Statistical Learning","heading":"2.1 Prediction","text":"error term \\(\\epsilon\\) averages zero, general statistical model predicting \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\) can written \\[\n\\hat Y = \\hat f(X).\n\\tag{2.2}\n\\]:\\(\\hat Y\\) represents resulting prediction \\(Y\\)\\(\\hat f\\) represents estimate \\(f\\)goal predict, typically need concern exact form \\(\\hat f\\) provided accurately predicts \\(Y\\). accuracy \\(\\hat Y\\) prediction \\(Y\\) depends two sources error: reducible error irreducible error. error model attributable \\(\\hat f\\) reducible can potentially improve accuracy \\(\\hat f\\) estimating \\(f\\) using appropriate statistical learning technique. However, error model attributable \\(\\epsilon\\) irreducible \\(Y\\) also function \\(\\epsilon\\), \\(\\epsilon\\) independent \\(X\\), matter well estimate \\(f\\), variability associated \\(\\epsilon\\) still present model. variability may come unmeasured variables useful predicting \\(Y\\), unmeasurable variation. Irreducible error places (often unknowable) upper bound accuracy prediction \\(Y\\).","code":""},{"path":"chapter-2.html","id":"inference","chapter":"2 Statistical Learning","heading":"2.2 Inference","text":"goal understand relationship \\(Y\\) \\(X = (X_1, X_2, \\dots, X_p)\\), need concern exact form \\(\\hat f\\). form \\(\\hat f\\) can used identify:predictors associated responsethe direction (positive negative) form (simple complex) relationship response predictor","code":""},{"path":"chapter-2.html","id":"assessing-model-accuracy","chapter":"2 Statistical Learning","heading":"2.3 Assessing Model Accuracy","text":"one statistical learning approach performs better approaches possible data sets. , care needs taken choose approach use given data set produce best results. number important concepts arise selecting statistical learning approach specific data set:Measuring quality fitThe bias-variance trade-","code":""},{"path":"chapter-2.html","id":"in-the-regression-setting","chapter":"2 Statistical Learning","heading":"2.3.1 In the Regression Setting","text":"regression setting, commonly used quality fit measure training data mean squared error \\(\\mathit{MSE}\\), given \\[\n\\mathit{MSE}_{\\mathrm{training}} = \\frac 1 n \\sum_{= 1}^n (y_i - \\hat f(x_i))^2,\n\\tag{2.3}\n\\]\\(\\hat f(x_i)\\) represents prediction \\(\\hat f\\) gives \\(\\)th observation. predicted responses close true responses \\(\\mathit{MSE}\\) small; predicted responses far true responses \\(\\mathit{MSE}\\) large. generally really care value accurately predicting data already seen particularly useful.goal assess accuracy predictions apply method previously unseen test data, can compute mean squared error test observations, given \\[\n\\mathit{MSE}_{\\mathrm{testing}} = \\frac 1 n \\sum_{= 1}^n (y_0 - \\hat f(x_0))^2,\n\\tag{2.4}\n\\]\\((x_0, y_0)\\) previously unseen test observation used train statistical learning model. want choose model gives lowest test \\(\\mathit{MSE}\\) minimizing distance \\(\\hat f(x_0)\\) \\(y_0\\). test data set available can simply evaluate Equation (2.4) choose statistical learning model \\(\\mathit{MSE}\\) smallest. test data set available can use cross-validation, method estimating test \\(\\mathit{MSE}\\) using training data set.expected test \\(\\mathit{MSE}\\) given value \\(x_0\\) can always decomposed sum three fundamental quantities: variance \\(\\hat f(x_0)\\), squared bias \\(\\hat f(x_0)\\), variance error term \\(\\epsilon\\), written \\[\nE \\bigl(y_0 - \\hat f(x_0) \\bigr)^2 = \\mathrm{Var}(\\hat f(x_0)) +\n                                     [\\mathrm{Bias(\\hat f(x_0))}]^2 +\n                                     \\mathrm{Var}(\\epsilon),\n\\tag{2.5}\n\\]notation \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) defines expected test \\(\\mathit{MSE}\\) \\(x_0\\). overall expected test \\(\\mathit{MSE}\\) given averaging \\(E \\bigl(y_0 - \\hat f(x_0) \\bigr)^2\\) possible values \\(x_0\\) test data set.variance \\(\\hat f\\) refers amount \\(\\hat f\\) change estimated using different training set. bias \\(\\hat f\\) refers error introduced approximating real-life problem much simpler model. general, models become flexible, variance increase bias decrease. relative rate change variance bias determines whether test \\(\\mathit{MSE}\\) increases decreases. variance bias can change different rates different data sets, challenge lies finding model variance bias lowest.","code":""},{"path":"chapter-2.html","id":"in-the-classification-setting","chapter":"2 Statistical Learning","heading":"2.3.2 In the Classification Setting","text":"classification setting, commonly used quality fit measure training data error rate, given \\[\n\\frac 1 n \\sum_{= 1}^n (y_i \\ne \\hat y_i).\n\\tag{2.6}\n\\]:\\(\\hat y_i\\) predicted class label \\(\\)th observation using \\(\\hat f\\)\\((y_i \\ne \\hat y_i)\\) indicator variable equals one \\(y_i \\ne \\hat y_i\\) (incorrectly classified) zero \\(y_i = \\hat y_i\\) (correctly classified)test error rate set test observations \\((x_0, y_0)\\) given \\[\n\\frac 1 n \\sum_{= 1}^n (y_0 \\ne \\hat y_0),\n\\tag{2.7}\n\\]\\(\\hat y_0\\) predicted class label results applying classifier test observation predictor \\(x_0\\).bias-variance trade-present classification setting ; variance bias lowest test error rate smallest given data set.","code":""},{"path":"chapter-2.html","id":"exercises","chapter":"2 Statistical Learning","heading":"2.4 Exercises","text":"","code":""},{"path":"chapter-2.html","id":"prerequisites","chapter":"2 Statistical Learning","heading":"Prerequisites","text":"access data sets functions used complete Chapter 2 exercises, load following packages.","code":"\nlibrary(ISLR2)\nlibrary(tidyverse)\nlibrary(skimr)"},{"path":"chapter-2.html","id":"conceptual","chapter":"2 Statistical Learning","heading":"Conceptual","text":"Exercise 2.1  parts () (d), indicate whether generally expect performance flexible statistical learning method better worse inflexible method. Justify answer.sample size \\(n\\) extremely large, number predictors \\(p\\) small.Answer. Better. flexible model generally able better estimate true \\(f\\) avoid overfitting extremely large sample size small number predictors. exception true \\(f\\) linear, inflexible model generally perform better; however, real world relationships linear, lower bias flexible model generally lead better quality fit.number predictors \\(p\\) extremely large, number observations \\(n\\) small.Answer. Worse. flexible model generally lead overfitting training data number predictors large sample size small. inflexible model less likely lead overfitting scenario, generally better job giving accurate predictions new observations flexible overfit model.relationship predictors response highly non-linear.Answer. Better. flexible model generally able fit highly non-linear relationship better inflexible model relative rate decrease bias tends much greater relative increase variance \\(f\\) highly non-linear. left right plots Figure 2.12 Page 36 book demonstrate nicely.variance error terms, .e. \\(\\sigma^2 = \\mathrm{Var}(\\epsilon)\\), extremely high.Answer. Worse. flexible model generally lead overfitting training data variance error terms extremely high. \\(Y\\) partly function \\(\\epsilon\\), variance error terms extremely high variance \\(Y\\) also extremely high, mainly due random error. flexible model tries find patterns noise likely pick patterns caused random chance rather true properties unknown function \\(f\\). bias inflexible model preferable situation, give stable predictions long run, likely preferable making essentially random predictions flexible model give circumstances.Exercise 2.2  Explain whether scenario classification regression problem, indicate whether interested inference prediction. Finally, provide \\(n\\) \\(p\\).collect set data top 500 firms US. firm record profit, number employees, industry, CEO salary. interested understanding factors affect CEO salary.Answer. Regression, inference, \\(n = 500\\), \\(p = 3\\).considering launching new product wish know whether success failure. collect data 20 similar products previously launched. product recorded whether success failure, price charged product, marketing budget, competition price, ten variables.Answer. Classification, prediction, \\(n = 20\\), \\(p = 13\\).interested predicting % change USD/Euro exchange rate relation weekly changes world stock markets. Hence collect weekly data 2012. week record % change USD/Euro, % change US market, % change British market, % change German market.Answer. Regression, prediction, \\(n = 52\\), \\(p = 3\\).Exercise 2.3  now revisit bias-variance decomposition.Provide sketch typical (squared) bias, variance, training error, test error, Bayes (irreducible) error curves, single plot, go less flexible statistical learning methods towards flexible approaches. \\(x\\)-axis represent amount flexibility method, \\(y\\)-axis represent values curve. five curves. Make sure label one.Answer.Explain five curves shape displayed part ().Answer. value measure part () changes different rates different directions go less flexible statistical learning methods towards flexible approaches. Broadly, curve different shape. Specifically:Bias refers error introduced approximating real-life problem much simpler model. general, bias decrease models become flexible, flexibility lead better approximations real-life problem, reducing error.Variance refers amount \\(\\hat f\\) change estimated using different training set. general, variance increase models become flexible, flexible model pick patterns training data better may differ training sets.Training error refers quality fit model training data. general, training error steadily increase models become flexible, flexibility allow model closely follow training data.Test error refers quality fit trained model test data. general, test error U-shaped bias-variance trade-.Irreducible error refers random error independent \\(X\\). irreducible error independent \\(X\\), remains stable regardless model flexibility.Exercise 2.4  now think real-life applications statistical learning.Describe three real-life applications classification might useful. Describe response, well predictors. goal application inference prediction? Explain answer.Answer.Predicting whether customer likely purchase items discounted web store. predictor whether likely purchase items; predictors purchase history discounted full price items, purchase frequency. goal prediction order determine whether show discounts customer less frequently.Predicting whether customer likely purchase items discounted web store. predictor whether likely purchase items; predictors purchase history discounted full price items, purchase frequency. goal prediction order determine whether show discounts customer less frequently.Understanding demographic groups enjoy Star Trek. response whether someone Trekkie; predictors various demographic variables age, gender identity, ethnicity, . goal inference whether Star Trek appeals specific groups enjoyable diverse audience.Understanding demographic groups enjoy Star Trek. response whether someone Trekkie; predictors various demographic variables age, gender identity, ethnicity, . goal inference whether Star Trek appeals specific groups enjoyable diverse audience.Understanding kind people like raisin cookies. response whether someone likes raisin cookies; predictors levels psychopathy sadism. goal inference whether people like raisin cookies secretly evil.Understanding kind people like raisin cookies. response whether someone likes raisin cookies; predictors levels psychopathy sadism. goal inference whether people like raisin cookies secretly evil.Describe three real-life applications regression might useful. Describe response, well predictors. goal application inference prediction? Explain answer.Answer.Predicting temperature tomorrow live. response temperature degrees Celsius; predictors average temperature live past seven days, latitude, longitude, altitude live, month year. goal prediction order decide outfit might wear tomorrow.Predicting temperature tomorrow live. response temperature degrees Celsius; predictors average temperature live past seven days, latitude, longitude, altitude live, month year. goal prediction order decide outfit might wear tomorrow.Understanding aspects video game lead people playing longer. response minutes spent playing game; predictors game genre, whether game original sequel, whether game singleplayer, multiplayer, , game developer. goal inference determine makes game engaging.Understanding aspects video game lead people playing longer. response minutes spent playing game; predictors game genre, whether game original sequel, whether game singleplayer, multiplayer, , game developer. goal inference determine makes game engaging.Predicting long take injury heal. response time heal; predictors injury severity, age, indicators physical health. goal prediction order give patients idea time recovery.Predicting long take injury heal. response time heal; predictors injury severity, age, indicators physical health. goal prediction order give patients idea time recovery.Describe three real-life applications cluster analysis might useful.Answer.Identifying whether brain activity similar within individuals. predictor variable repeated measure functional connectivity. goal inference order see whether brain activity recordings cluster together individual.Identifying whether brain activity similar within individuals. predictor variable repeated measure functional connectivity. goal inference order see whether brain activity recordings cluster together individual.Identifying whether patients depression can classified subgroups. predictor variables responses depression inventory. goal inference order understand whether depression inventory measures multiple types depression.Identifying whether patients depression can classified subgroups. predictor variables responses depression inventory. goal inference order understand whether depression inventory measures multiple types depression.Identifying whether species families animals identified biologists aligns species’ genomes. predictor variable species genome. goal understanding whether clusters identified observation align clusters identified genetics.Identifying whether species families animals identified biologists aligns species’ genomes. predictor variable species genome. goal understanding whether clusters identified observation align clusters identified genetics.Exercise 2.5  advantages disadvantages flexible (versus less flexible) approach regression classification? circumstances might flexible approach preferred less flexible approach? might less flexible approach preferred?Answer. advantage flexible approach less bias, model better approximates reality, typically better prediction. disadvantage higher variance potential overfitting. flexible model might preferred goal statistical learning prediction. less flexible approach might preferred goal statistical learning inference, simpler models tend easier understand.Exercise 2.6  Describe differences parametric non-parametric statistical learning approach. advantages parametric approach regression classification (opposed non-parametric approach)? disadvantages?Answer. Parametric methods involve two-step model-based approach: First make assumption functional form \\(f\\) (e.g., assume \\(f\\) linear). Second fit (train) model training data order estimate parameters. advantage easier estimate set parameters fit entirely arbitrary function \\(f\\). disadvantage model choose usually match true unknown form \\(f\\), can lead poor estimates far true \\(f\\).Non-parametric methods make assumptions functional form \\(f\\). Instead try find estimate \\(f\\) gets close data points without rough wiggly. advantage becomes easier accurately fit wider range possible shapes \\(f\\). However, disadvantage accurately requires much larger number observations since problem estimating \\(f\\) small number parameters.Exercise 2.7  table provides training data set containing six observations, three predictors, one qualitative response variable.Suppose wish use data set make prediction \\(Y\\) \\(X1 = X2 = X3 = 0\\) using K-nearest neighbours.Compute Euclidean distance observation test point, \\(X1 = X2 = X3 = 0\\).Answer.equation Euclidean distance given \\[\nd(p, q) = \\sqrt{ \\sum_{= 1}^n (q_i - p_i)^2},\n\\tag{2.8}\n\\]\\(d(p, q)\\) represents distance points \\(p\\) \\(q\\) \\(n\\) dimensional Euclidean space, \\(q_i\\) \\(p_i\\) represent Euclidean vectors, starting origin \\(n\\) dimensional Euclidean space.can computed R like .prediction \\(K = 1\\)? ?Answer. \\(K = 1\\) prediction test point whatever training observation closest test point. Observation 5 (Green) closest prediction test point Green.prediction \\(K = 3\\)? ?Answer. \\(K = 3\\) prediction test point given class highest estimated probability based three points training data closest test point. observations 5 (Green), 6 (Red), 2 (Red) closest, results probabilities 1/3 Green class 2/3 Red class. prediction test point Red.Bayes decision boundary problem highly non-linear, expect best value \\(K\\) large small? ?Answer. expect smaller value K give best predictions. small. \\(K = 1\\) KNN decision boundary overly flexible, large value \\(K\\) sufficiently flexible.","code":"\neuclidean_distance <- function(q, p) sqrt(sum((q - p)^2))\n\nobs_test <- c( 0, 0, 0)\n\nobs_train <- list(\n  obs_01 = c( 0, 3, 0),\n  obs_02 = c( 2, 0, 0),\n  obs_03 = c( 0, 1, 3),\n  obs_04 = c( 0, 1, 2),\n  obs_05 = c(-1, 0, 0),\n  obs_06 = c( 1, 1, 1)\n)\n\nlapply(obs_train, function(x) euclidean_distance(obs_test, x))\n#> $obs_01\n#> [1] 3\n#> \n#> $obs_02\n#> [1] 2\n#> \n#> $obs_03\n#> [1] 3.162278\n#> \n#> $obs_04\n#> [1] 2.236068\n#> \n#> $obs_05\n#> [1] 1\n#> \n#> $obs_06\n#> [1] 1.732051"},{"path":"chapter-2.html","id":"applied","chapter":"2 Statistical Learning","heading":"Applied","text":"Exercise 2.8  exercise relates ISLR2::College data set. contains number \nvariables 777 different universities colleges US. help page ?ISLR2::College data set contains description data set 18 variables measures.Load ISLR2::College data set R.Explore college data.Produce numerical summary variables data set.Table 2.1: Data summaryVariable type: factorVariable type: numericProduce scatterplot matrix first ten columns variables data.Produce scatterplot matrix first ten columns variables data.Produce side--side boxplots Outstate versus Private.Produce side--side boxplots Outstate versus Private.Create new qualitative variable, called Elite, binning Top10perc variable. going divide universities two groups based whether proportion students coming top 10% high school classes exceeds 50%. See many elite universities , produce side--side boxplots Outstate versus Elite.Create new qualitative variable, called Elite, binning Top10perc variable. going divide universities two groups based whether proportion students coming top 10% high school classes exceeds 50%. See many elite universities , produce side--side boxplots Outstate versus Elite.Produce histograms differing numbers bins quantitative variables.Produce histograms differing numbers bins quantitative variables.Continue exploring data, provide brief summary discover.Continue exploring data, provide brief summary discover.","code":"\n# Make sure to preserve the row names with college names in them because they\n# might be useful later\ncollege <- as_tibble(College, rownames = NA)\nstr(college)\n#> tibble [777 x 18] (S3: tbl_df/tbl/data.frame)\n#>  $ Private    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ Apps       : num [1:777] 1660 2186 1428 417 193 ...\n#>  $ Accept     : num [1:777] 1232 1924 1097 349 146 ...\n#>  $ Enroll     : num [1:777] 721 512 336 137 55 158 103 489 227 172 ...\n#>  $ Top10perc  : num [1:777] 23 16 22 60 16 38 17 37 30 21 ...\n#>  $ Top25perc  : num [1:777] 52 29 50 89 44 62 45 68 63 44 ...\n#>  $ F.Undergrad: num [1:777] 2885 2683 1036 510 249 ...\n#>  $ P.Undergrad: num [1:777] 537 1227 99 63 869 ...\n#>  $ Outstate   : num [1:777] 7440 12280 11250 12960 7560 ...\n#>  $ Room.Board : num [1:777] 3300 6450 3750 5450 4120 ...\n#>  $ Books      : num [1:777] 450 750 400 450 800 500 500 450 300 660 ...\n#>  $ Personal   : num [1:777] 2200 1500 1165 875 1500 ...\n#>  $ PhD        : num [1:777] 70 29 53 92 76 67 90 89 79 40 ...\n#>  $ Terminal   : num [1:777] 78 30 66 97 72 73 93 100 84 41 ...\n#>  $ S.F.Ratio  : num [1:777] 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...\n#>  $ perc.alumni: num [1:777] 12 16 30 37 2 11 26 37 23 15 ...\n#>  $ Expend     : num [1:777] 7041 10527 8735 19016 10922 ...\n#>  $ Grad.Rate  : num [1:777] 60 56 54 59 15 55 63 73 80 52 ...\nskim(college)"}]
