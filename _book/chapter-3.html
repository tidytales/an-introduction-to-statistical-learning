<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Linear Regression | Self-Study: An Introduction to Statistical Learning</title>
<meta name="author" content="Michael McCarthy">
<meta name="description" content="Chapter 3 is about (ordinary least squares and polynomial) linear regression, a parametric supervised statistical learning method. Many of the more advanced statistical learning approaches are...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 3 Linear Regression | Self-Study: An Introduction to Statistical Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="Chapter 3 is about (ordinary least squares and polynomial) linear regression, a parametric supervised statistical learning method. Many of the more advanced statistical learning approaches are...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Linear Regression | Self-Study: An Introduction to Statistical Learning">
<meta name="twitter:description" content="Chapter 3 is about (ordinary least squares and polynomial) linear regression, a parametric supervised statistical learning method. Many of the more advanced statistical learning approaches are...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Self-Study: An Introduction to Statistical Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled"><li><a class="" href="index.html"><span class="header-section-number">Chapter 3</span> Linear Regression</a></li></ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/tidytales/an-introduction-to-statistical-learning">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-3" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Linear Regression<a class="anchor" aria-label="anchor" href="#chapter-3"><i class="fas fa-link"></i></a>
</h1>
<p>Chapter 3 is about (ordinary least squares and polynomial) linear regression, a parametric supervised statistical learning method. Many of the more advanced statistical learning approaches are generalizations or extensions of linear regression, so having a good grasp on linear regression is important for understanding later methods in the book.</p>
<p>It also discusses K-nearest neighbours regression (KNN regression), a non-parametric supervised statistical learning method.</p>
<div id="exercises-1" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-1"><i class="fas fa-link"></i></a>
</h2>
<div id="prerequisites-1" class="section level3 unnumbered">
<h3>Prerequisites<a class="anchor" aria-label="anchor" href="#prerequisites-1"><i class="fas fa-link"></i></a>
</h3>
<p>To access the data sets and functions used to complete the Chapter 3 exercises, load the following packages.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="co"># library(skimr)</span>
<span class="co"># library(GGally)</span>
<span class="co"># library(patchwork)</span></code></pre></div>
</div>
<div id="conceptual-1" class="section level3 unnumbered">
<h3>Conceptual<a class="anchor" aria-label="anchor" href="#conceptual-1"><i class="fas fa-link"></i></a>
</h3>
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 3.1  </strong></span>Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.</p>
<div class="inline-table"><table class="table table-sm">
<caption>Table 3.4</caption>
<thead><tr class="header">
<th>Term</th>
<th>Coefficient</th>
<th>Std. Error</th>
<th>t-statistic</th>
<th>p-value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>intercept</td>
<td>2.939</td>
<td>0.3119</td>
<td>9.420</td>
<td>&lt; .001</td>
</tr>
<tr class="even">
<td>TV</td>
<td>0.046</td>
<td>0.0014</td>
<td>32.81</td>
<td>&lt; .0001</td>
</tr>
<tr class="odd">
<td>radio</td>
<td>0.189</td>
<td>0.0086</td>
<td>21.89</td>
<td>&lt; .0001</td>
</tr>
<tr class="even">
<td>newspaper</td>
<td>-0.001</td>
<td>0.0059</td>
<td>-0.18</td>
<td>.8599</td>
</tr>
</tbody>
</table></div>
<p><em>Answer</em>.</p>
<p>The null hypotheses to which the p-values in Table 3.4 correspond is that the coefficient for each term is equal to zero, <span class="math inline">\(\beta_i = 0\)</span>, which we use to test whether a term is associated with the response variable. We can use the p-values in the table to infer whether the coefficients are sufficiently far from zero that we can be confident they are non-zero, and thus associated with the response variable.</p>
<p>Based on the p-values in Table 3.4, we can conclude that:</p>
<ul>
<li>A $1000 increase in the TV advertising budget is associated with an average increase in product sales by 46 units while holding the advertising budgets for radio and newspaper fixed.</li>
<li>A $1000 increase in the radio advertising budget is associated with an average increase in product sales by 189 units while holding the advertising budgets for TV and newspaper fixed.</li>
<li>A $1000 increase in the newspaper advertising budget is not associated with a change in product sales.</li>
</ul>
<p>Data note: For the Advertising data sales are in thousands of units and advertising budgets in thousands of dollars.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 3.2  </strong></span>Carefully explain the differences between the KNN classifier and KNN regression methods.</p>
<p><em>Answer</em>.</p>
<p>Given a value for <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, both the KNN classifier and KNN regression first identify the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\mathcal N_0\)</span>. For the KNN classifier <span class="math inline">\(x_0\)</span> would be a discrete class, and for KNN regression <span class="math inline">\(x_0\)</span> would be a continuous value. The methods then diverge in what they do with <span class="math inline">\(\mathcal N_0\)</span>.</p>
<p>The KNN classifier then estimates the conditional probability that the test observation <span class="math inline">\(x_0\)</span> belongs to class <span class="math inline">\(j\)</span> as the fraction of training observations in <span class="math inline">\(\mathcal N_0\)</span> whose response value <span class="math inline">\(y_i\)</span> equals <span class="math inline">\(j\)</span>:</p>
<p><span class="math display" id="eq:knn-classification">\[
\mathrm{Pr}(Y = j|X = x_0) =  \frac{1}{K} \sum_{i \in \mathcal N_0} I(y_i = j).
\tag{3.1}
\]</span></p>
<p>KNN regression, on the other hand, then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training observations in <span class="math inline">\(\mathcal N_0\)</span>:</p>
<p><span class="math display" id="eq:knn-regression">\[
\hat f(x_0) = \frac{1}{K} \sum_{i \in {\mathcal N}_0} y_i.
\tag{3.2}
\]</span></p>
<p>As can be seen in Equations <a href="chapter-3.html#eq:knn-classification">(3.1)</a> and <a href="chapter-3.html#eq:knn-regression">(3.2)</a>, there are two main differences between the KNN classifier and KNN regression methods:</p>
<ul>
<li>On the right side of the equation, the KNN classifier uses an indicator function <span class="math inline">\(I(y_i = j)\)</span> to determine whether a response value <span class="math inline">\(y_i\)</span> equals <span class="math inline">\(j\)</span> (if so then it equals 1, if not 0). These are summed to get the numerator of the conditional probability for each class (where the denominator is <span class="math inline">\(K\)</span>). KNN regression, on the other hand, simply uses the response value <span class="math inline">\(y_i\)</span> as is, since it’s a continuous value. These are also summed to get the numerator of the fraction (again with <span class="math inline">\(K\)</span> as the denominator), although it makes more sense to think of this as the average of all the training observations in <span class="math inline">\(\mathcal N_0\)</span>.</li>
<li>On the left hand side of the equation, for the KNN classifier we estimate the conditional probability that a test observation <span class="math inline">\(x_0\)</span> belongs to a class <span class="math inline">\(j\)</span>. We can then use this probability to assign <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> with the highest probability, if we so choose (although we don’t have to, the probabilities are equally useful). For KNN regression we estimate the form of <span class="math inline">\(f(x_0)\)</span>, which corresponds to our prediction point for our test observation <span class="math inline">\(x_0\)</span> since <span class="math inline">\(\hat Y = \hat f(X)\)</span>.</li>
</ul>
<p>Thus, the main difference between the two methods is that the KNN classifier method is used to predict what class an observation likely belongs to based on the classes of its <span class="math inline">\(K\)</span> nearest neighbours, whereas the KNN regression method is used to predict what value an observation will have on a response variable based on the response values of its <span class="math inline">\(K\)</span> nearest neighbours.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 3.3  </strong></span>Suppose we have a data set with five predictors, <span class="math inline">\(X_1 =\)</span> GPA, <span class="math inline">\(X_2 =\)</span> IQ, <span class="math inline">\(X_3 =\)</span> Level (1 for College and 0 for High School), <span class="math inline">\(X_4 =\)</span> Interaction between GPA and IQ, and <span class="math inline">\(X_5 =\)</span> Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get <span class="math inline">\(\hat \beta_0 = 50\)</span>, <span class="math inline">\(\hat \beta_1 = 20\)</span>, <span class="math inline">\(\hat \beta_2 = 0.07\)</span>, <span class="math inline">\(\hat \beta_3 = 35\)</span>, <span class="math inline">\(\hat \beta_4 = 0.01\)</span>, <span class="math inline">\(\hat \beta_5 = −10\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Which answer is correct, and why?</li>
</ol>
<ol style="list-style-type: lower-roman">
<li><p>For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.</p></li>
<li><p>For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.</p></li>
<li><p>For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.</p></li>
<li><p>For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.</p></li>
</ol>
<p><em>Answer</em>. The fourth description is correct. The coefficient for Level is positive, so when it is included in the model (i.e., when Level is not zero) then predictions for starting salary will be higher; however, because the coefficient for the interaction between GPA and level is negative, this will only be true provided that college graduate GPA is high enough.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The predicted salary for this college graduate is $137,100.</p>
<p><span class="math display">\[
\begin{align}
\hat y &amp;= 50 + 20(4) + 0.07(110) + 35(1) + 0.01(4*110) - 10(4*1) \\
       &amp;= 50 + 80 + 7.7 + 35 + 4.4 - 40 \\
       &amp;= 137.1
\end{align}
\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.</li>
</ol>
<p><em>Answer</em>. False. The size of the coefficient does not provide evidence for or against an interaction effect. We use the p-value associated with the coefficient to determine whether it is sufficiently far from zero that we can be confident it is non-zero, and thus associated with the response variable. If our estimates are precise enough we can find a small coefficient and be confident that it is non-zero. But we cannot make a decision about this based on the coefficient alone.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-4" class="exercise"><strong>Exercise 3.4  </strong></span>I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. <span class="math inline">\(Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose that the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is linear,
i.e. <span class="math inline">\(Y = \beta_0 + \beta_1X + \epsilon\)</span>. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.</li>
</ol>
<p><em>Answer</em>.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Answer (a) using test rather than training RSS.</li>
</ol>
<p><em>Answer</em>.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Suppose that the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.</li>
</ol>
<p><em>Answer</em>.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>Answer (c) using test rather than training RSS.</li>
</ol>
<p><em>Answer</em>.</p>
</div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="empty"></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Self-Study: An Introduction to Statistical Learning</strong>" was written by Michael McCarthy. It was last built on 2021-12-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
