<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Statistical Learning | Self-Study: An Introduction to Statistical Learning</title>
<meta name="author" content="Michael McCarthy">
<meta name="description" content="Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between \(Y\) and \(X = (X_1, X_2, \dots, X_p)\), which...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 2 Statistical Learning | Self-Study: An Introduction to Statistical Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between \(Y\) and \(X = (X_1, X_2, \dots, X_p)\), which...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2 Statistical Learning | Self-Study: An Introduction to Statistical Learning">
<meta name="twitter:description" content="Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between \(Y\) and \(X = (X_1, X_2, \dots, X_p)\), which...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Self-Study: An Introduction to Statistical Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled"><li><a class="" href="index.html"><span class="header-section-number">Chapter 2</span> Statistical Learning</a></li></ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/tidytales/an-introduction-to-statistical-learning">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-2" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Statistical Learning<a class="anchor" aria-label="anchor" href="#chapter-2"><i class="fas fa-link"></i></a>
</h1>
<p>Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span>, which can be written as</p>
<p><span class="math display" id="eq:gsm">\[
Y = f(X) + \epsilon.
\tag{2.1}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> represents the response variable in our data set</li>
<li>
<span class="math inline">\(X\)</span> represents the set of variables in our data set</li>
<li>
<span class="math inline">\(X_p\)</span> represents the <span class="math inline">\(p\)</span>th variable in our data set</li>
<li>
<span class="math inline">\(f(\dots)\)</span> represents a fixed but unknown function of its input(s)</li>
<li>
<span class="math inline">\(\epsilon\)</span> represents a random error term which is independent of <span class="math inline">\(X\)</span> and has mean zero</li>
</ul>
<p>The goal of statistical learning is to estimate <span class="math inline">\(f\)</span>. There are two main reasons for estimating <span class="math inline">\(f\)</span>: <em>prediction</em> and <em>inference</em>. Depending on whether our ultimate goal is prediction, inference, or some combination of the two, different methods for estimating <span class="math inline">\(f\)</span> may be appropriate. In general, there is a trade-off between prediction accuracy and model interpretability. Models that make more accurate predictions tend to be less interpretable, and models that are more interpretable tend to make less accurate predictions (although this is not always the case, due to the potential for <em>overfitting</em> in highly flexible models).</p>
<p>The methods we use to estimate <span class="math inline">\(f\)</span> can be characterized as either <em>parametric</em> or <em>non-parametric</em>. Parametric methods involve a two-step model-based approach: First we make an assumption about the functional form of <span class="math inline">\(f\)</span> (e.g., we could assume <span class="math inline">\(f\)</span> is linear). Second we <em>fit</em> (train) the model to our training data in order to estimate the parameters. Non-parametric methods do not make any assumptions about the functional form of <span class="math inline">\(f\)</span>. Instead they try to find an estimate of <span class="math inline">\(f\)</span> that gets close to the data points without being too rough or wiggly.</p>
<div id="prediction" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Prediction<a class="anchor" aria-label="anchor" href="#prediction"><i class="fas fa-link"></i></a>
</h2>
<p>Because the error term <span class="math inline">\(\epsilon\)</span> averages to zero, the general statistical model for predicting <span class="math inline">\(Y\)</span> from <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span> can be written as</p>
<p><span class="math display" id="eq:gsm-p">\[
\hat Y = \hat f(X).
\tag{2.2}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(\hat Y\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>
</li>
<li>
<span class="math inline">\(\hat f\)</span> represents our estimate for <span class="math inline">\(f\)</span>
</li>
</ul>
<p>When our goal is only to predict, we do not typically need to concern ourselves with the exact form of <span class="math inline">\(\hat f\)</span> provided that it accurately predicts <span class="math inline">\(Y\)</span>. The accuracy of <span class="math inline">\(\hat Y\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two sources of error: <em>reducible error</em> and <em>irreducible error</em>. The error in our model attributable to <span class="math inline">\(\hat f\)</span> is <em>reducible</em> because we can potentially improve the accuracy of <span class="math inline">\(\hat f\)</span> for estimating <span class="math inline">\(f\)</span> by using a more appropriate statistical learning technique. However, the error in our model attributable to <span class="math inline">\(\epsilon\)</span> is <em>irreducible</em> because <span class="math inline">\(Y\)</span> is also a function of <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(\epsilon\)</span> is independent of <span class="math inline">\(X\)</span>, so no matter how well we estimate <span class="math inline">\(f\)</span>, the variability associated with <span class="math inline">\(\epsilon\)</span> will still be present in our model. This variability may come from unmeasured variables that are useful for predicting <span class="math inline">\(Y\)</span>, or from unmeasurable variation. Irreducible error places an (often unknowable) upper bound on the accuracy of our prediction for <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="inference" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Inference<a class="anchor" aria-label="anchor" href="#inference"><i class="fas fa-link"></i></a>
</h2>
<p>When our goal is to understand the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span>, we do need to concern ourselves with the exact form of <span class="math inline">\(\hat f\)</span>. Here the form of <span class="math inline">\(\hat f\)</span> can be used to identify:</p>
<ul>
<li>Which predictors are associated with the response</li>
<li>the direction (positive or negative) and form (simple or complex) of the relationship between the response and each predictor</li>
</ul>
</div>
<div id="assessing-model-accuracy" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Assessing Model Accuracy<a class="anchor" aria-label="anchor" href="#assessing-model-accuracy"><i class="fas fa-link"></i></a>
</h2>
<p>No one statistical learning approach performs better than all other approaches on all possible data sets. Because of this, care needs to be taken to choose which approach to use for any given data set to produce the best results. A number of important concepts arise when selecting a statistical learning approach for a specific data set:</p>
<ul>
<li>Measuring the quality of fit</li>
<li>The bias-variance trade-off</li>
</ul>
<div id="in-the-regression-setting" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> In the Regression Setting<a class="anchor" aria-label="anchor" href="#in-the-regression-setting"><i class="fas fa-link"></i></a>
</h3>
<p>In the regression setting, the most commonly used quality of fit measure for training data is the mean squared error <span class="math inline">\(\mathit{MSE}\)</span>, given by</p>
<p><span class="math display" id="eq:mse-training">\[
\mathit{MSE}_{\mathrm{training}} = \frac 1 n \sum_{i = 1}^n (y_i - \hat f(x_i))^2,
\tag{2.3}
\]</span></p>
<p>where <span class="math inline">\(\hat f(x_i)\)</span> represents the prediction that <span class="math inline">\(\hat f\)</span> gives for the <span class="math inline">\(i\)</span>th observation. When predicted responses are very close to true responses the <span class="math inline">\(\mathit{MSE}\)</span> will be small; When predicted responses are very far from true responses the <span class="math inline">\(\mathit{MSE}\)</span> will be large. We generally do not really care about this value because accurately predicting data we have already seen is not particularly useful.</p>
<p>when our goal is to assess the accuracy of predictions when we apply our method to previously unseen <em>test data</em>, we can compute the mean squared error for test observations, given by</p>
<p><span class="math display" id="eq:mse-testing">\[
\mathit{MSE}_{\mathrm{testing}} = \frac 1 n \sum_{i = 1}^n (y_0 - \hat f(x_0))^2,
\tag{2.4}
\]</span></p>
<p>where <span class="math inline">\((x_0, y_0)\)</span> is a previously unseen test observation not used to train the statistical learning model. We want to choose the model that gives the lowest <em>test</em> <span class="math inline">\(\mathit{MSE}\)</span> by minimizing the distance between <span class="math inline">\(\hat f(x_0)\)</span> and <span class="math inline">\(y_0\)</span>. When a test data set is available then we can simply evaluate Equation <a href="chapter-2.html#eq:mse-testing">(2.4)</a> and choose the statistical learning model where <span class="math inline">\(\mathit{MSE}\)</span> is the smallest. When a test data set is not available then we can use <em>cross-validation</em>, which is a method for estimating test <span class="math inline">\(\mathit{MSE}\)</span> using the training data set.</p>
<p>The expected test <span class="math inline">\(\mathit{MSE}\)</span> for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: the <em>variance</em> of <span class="math inline">\(\hat f(x_0)\)</span>, the squared <em>bias</em> of <span class="math inline">\(\hat f(x_0)\)</span>, and the variance of the error term <span class="math inline">\(\epsilon\)</span>, written as</p>
<p><span class="math display" id="eq:expected-test-mse">\[
E \bigl(y_0 - \hat f(x_0) \bigr)^2 = \mathrm{Var}(\hat f(x_0)) +
                                     [\mathrm{Bias(\hat f(x_0))}]^2 +
                                     \mathrm{Var}(\epsilon),
\tag{2.5}
\]</span></p>
<p>where the notation <span class="math inline">\(E \bigl(y_0 - \hat f(x_0) \bigr)^2\)</span> defines the expected test <span class="math inline">\(\mathit{MSE}\)</span> at <span class="math inline">\(x_0\)</span>. The overall expected test <span class="math inline">\(\mathit{MSE}\)</span> is given by averaging <span class="math inline">\(E \bigl(y_0 - \hat f(x_0) \bigr)^2\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test data set.</p>
<p>The variance of <span class="math inline">\(\hat f\)</span> refers to the amount by which <span class="math inline">\(\hat f\)</span> would change if we estimated it using a different training set. The bias of <span class="math inline">\(\hat f\)</span> refers to the error that is introduced by approximating a real-life problem with a much simpler model. In general, as models become more flexible, the variance will increase and the bias will decrease. The relative rate of change of the variance and bias determines whether the test <span class="math inline">\(\mathit{MSE}\)</span> increases or decreases. Because the variance and bias can change at different rates in different data sets, the challenge lies in finding a model for which both the variance and bias are lowest.</p>
</div>
<div id="in-the-classification-setting" class="section level3" number="2.3.2">
<h3>
<span class="header-section-number">2.3.2</span> In the Classification Setting<a class="anchor" aria-label="anchor" href="#in-the-classification-setting"><i class="fas fa-link"></i></a>
</h3>
<p>In the classification setting, the most commonly used quality of fit measure for training data is the <em>error rate</em>, given by</p>
<p><span class="math display" id="eq:er-training">\[
\frac 1 n \sum_{i = 1}^n I(y_i \ne \hat y_i).
\tag{2.6}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(\hat y_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat f\)</span>
</li>
<li>
<span class="math inline">\(I(y_i \ne \hat y_i)\)</span> is an indicator variable that equals one if <span class="math inline">\(y_i \ne \hat y_i\)</span> (incorrectly classified) and zero if <span class="math inline">\(y_i = \hat y_i\)</span> (correctly classified)</li>
</ul>
<p>The <em>test error rate</em> for a set of test observations <span class="math inline">\((x_0, y_0)\)</span> is given by</p>
<p><span class="math display" id="eq:er-testing">\[
\frac 1 n \sum_{i = 1}^n I(y_0 \ne \hat y_0),
\tag{2.7}
\]</span></p>
<p>where <span class="math inline">\(\hat y_0\)</span> is the predicted class label that results from applying the classifier to the test observation with predictor <span class="math inline">\(x_0\)</span>.</p>
<p>The bias-variance trade-off is present in the classification setting too; when variance and bias are lowest then the test error rate will be at its smallest for a given data set.</p>
</div>
</div>
<div id="exercises" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises"><i class="fas fa-link"></i></a>
</h2>
<div id="prerequisites" class="section level3 unnumbered">
<h3>Prerequisites<a class="anchor" aria-label="anchor" href="#prerequisites"><i class="fas fa-link"></i></a>
</h3>
<p>To access the data sets and functions used to complete the Chapter 2 exercises, load the following packages.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># library(ISLR2)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/skimr/">skimr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggobi.github.io/ggally/">GGally</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span></code></pre></div>
</div>
<div id="conceptual" class="section level3 unnumbered">
<h3>Conceptual<a class="anchor" aria-label="anchor" href="#conceptual"><i class="fas fa-link"></i></a>
</h3>
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 2.1  </strong></span>For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.</p>
<ol style="list-style-type: lower-alpha">
<li>The sample size <span class="math inline">\(n\)</span> is extremely large, and the number of predictors <span class="math inline">\(p\)</span> is small.</li>
</ol>
<p><em>Answer</em>. Better. A flexible model will generally be able to better estimate the the true <span class="math inline">\(f\)</span> and avoid overfitting when we have an extremely large sample size and a small number of predictors. The only exception would be if the true <span class="math inline">\(f\)</span> is linear, then an inflexible model would generally perform better; however, most real world relationships are not linear, so the lower bias of a flexible model will generally lead to a better quality of fit.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>The number of predictors <span class="math inline">\(p\)</span> is extremely large, and the number of observations <span class="math inline">\(n\)</span> is small.</li>
</ol>
<p><em>Answer</em>. Worse. A flexible model will generally lead to overfitting of our training data when the number of predictors is large and the sample size is small. An inflexible model is less likely to lead to overfitting in this scenario, so it will generally do a better job of giving accurate predictions on new observations than the flexible but overfit model.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>The relationship between the predictors and response is highly non-linear.</li>
</ol>
<p><em>Answer</em>. Better. A flexible model will generally be able to fit a highly non-linear relationship better than an inflexible model because the relative rate of decrease in bias tends to be much greater than the relative increase in variance when <span class="math inline">\(f\)</span> is highly non-linear. The left and right plots in Figure 2.12 on Page 36 of the book demonstrate this nicely.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>The variance of the error terms, i.e. <span class="math inline">\(\sigma^2 = \mathrm{Var}(\epsilon)\)</span>, is extremely high.</li>
</ol>
<p><em>Answer</em>. Worse. A flexible model will generally lead to overfitting of our training data when the variance of the error terms is extremely high. Because <span class="math inline">\(Y\)</span> is partly a function of <span class="math inline">\(\epsilon\)</span>, when the variance of the error terms is extremely high then the variance of <span class="math inline">\(Y\)</span> will also be extremely high, mainly due to random error. A flexible model that tries to find patterns in this noise is more likely to pick up on patterns that are caused by random chance rather than true properties of the unknown function <span class="math inline">\(f\)</span>. The bias of an inflexible model is preferable in this situation, as it will give more stable predictions in the long run, which is likely preferable to making the essentially random predictions a flexible model would give in these circumstances.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 2.2  </strong></span>Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry, and the CEO salary. We are interested in understanding which factors affect CEO salary.</li>
</ol>
<p><em>Answer</em>. Regression, inference, <span class="math inline">\(n = 500\)</span>, <span class="math inline">\(p = 3\)</span>.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>We are considering launching a new product and wish to know whether it will be a <em>success</em> or <em>failure</em>. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.</li>
</ol>
<p><em>Answer</em>. Classification, prediction, <span class="math inline">\(n = 20\)</span>, <span class="math inline">\(p = 13\)</span>.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.</li>
</ol>
<p><em>Answer</em>. Regression, prediction, <span class="math inline">\(n = 52\)</span>, <span class="math inline">\(p = 3\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 2.3  </strong></span>We now revisit the bias-variance decomposition.</p>
<ol style="list-style-type: lower-alpha">
<li>Provide a sketch of a typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The <span class="math inline">\(x\)</span>-axis should represent the amount of flexibility in the method, and the <span class="math inline">\(y\)</span>-axis should represent the values for each curve. There should be five curves. Make sure to label each one.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="inline-figure"><img src="inst/images/chapter-02_exercise-3a.png" width="100%"></div>
<ol start="2" style="list-style-type: lower-alpha">
<li>Explain why each of the five curves has the shape displayed in part (a).</li>
</ol>
<p><em>Answer</em>. The value of each measure in part (a) changes at different rates and in different directions as we go from less flexible statistical learning methods towards more flexible approaches. Broadly, this is why each curve has a different shape. Specifically:</p>
<ul>
<li>Bias refers to the error that is introduced by approximating a real-life problem with a much simpler model. In general, bias will decrease as models become more flexible, because more flexibility will lead to better approximations of a real-life problem, reducing error.</li>
<li>Variance refers to the amount by which <span class="math inline">\(\hat f\)</span> would change if we estimated it using a different training set. In general, variance will increase as models become more flexible, because a more flexible model will pick up patterns in the training data better which may differ between training sets.</li>
<li>Training error refers to the quality of fit of the model to the training data. In general, training error will steadily increase as models become more flexible, because more flexibility will allow the model to closely follow the training data.</li>
<li>Test error refers to the quality of fit of the trained model to the test data. In general, test error will be U-shaped because of the bias-variance trade-off.</li>
<li>Irreducible error refers to random error that is independent of <span class="math inline">\(X\)</span>. Because irreducible error is independent of <span class="math inline">\(X\)</span>, it remains stable regardless of model flexibility.</li>
</ul>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-4" class="exercise"><strong>Exercise 2.4  </strong></span>You will now think of some real-life applications for statistical learning.</p>
<ol style="list-style-type: lower-alpha">
<li>Describe three real-life applications in which <em>classification</em> might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</li>
</ol>
<p><em>Answer</em>.</p>
<ul>
<li><p>Predicting whether or not a customer is likely to purchase more if items are discounted on a web store. The predictor would be whether or not they are likely to purchase more items; the predictors would be purchase history of discounted and full price items, and purchase frequency. The goal would be prediction in order to determine whether to show discounts to a customer more or less frequently.</p></li>
<li><p>Understanding what demographic groups enjoy Star Trek. The response would again be whether or not someone is a Trekkie; the predictors would be various demographic variables such as age, gender identity, ethnicity, and so on. The goal would be inference about whether Star Trek appeals to specific groups or is enjoyable to a diverse audience.</p></li>
<li><p>Understanding what kind of people like raisin cookies. The response would be whether or not someone likes raisin cookies; the predictors would be levels of psychopathy and sadism. The goal would be inference about whether people who like raisin cookies are secretly evil.</p></li>
</ul>
<ol start="2" style="list-style-type: lower-alpha">
<li>Describe three real-life applications in which <em>regression</em> might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</li>
</ol>
<p><em>Answer</em>.</p>
<ul>
<li><p>Predicting the temperature tomorrow where you live. The response would be temperature in degrees Celsius; the predictors would be average temperature where you live in the past seven days, the latitude, longitude, and altitude of where you live, and the month of the year. The goal would be prediction in order to decide what outfit you might wear tomorrow.</p></li>
<li><p>Understanding what aspects of a video game lead to people playing it longer. The response would be minutes spent playing a game; the predictors would be game genre, whether the game is an original or sequel, whether the game is singleplayer, multiplayer, or both, and the game developer. The goal would be inference to determine what makes a game engaging.</p></li>
<li><p>Predicting how long it would take for an injury to heal. The response would be time to heal; the predictors would be injury severity, age, and other indicators of physical health. The goal would be prediction in order to give patients an idea the time to recovery.</p></li>
</ul>
<ol start="3" style="list-style-type: lower-alpha">
<li>Describe three real-life applications in which <em>cluster analysis</em> might be useful.</li>
</ol>
<p><em>Answer</em>.</p>
<ul>
<li><p>Identifying whether brain activity is more similar within or between individuals. The predictor variable would a repeated measure of functional connectivity. The goal would be inference in order to see whether brain activity recordings cluster together by individual.</p></li>
<li><p>Identifying whether patients with depression can be classified into subgroups. The predictor variables would be responses on a depression inventory. The goal would be inference in order to understand whether the depression inventory measures multiple types of depression.</p></li>
<li><p>Identifying whether species in families of animals identified by biologists aligns with those species’ genomes. The predictor variable would be species genome. The goal would be understanding whether clusters identified through observation align with clusters identified through genetics.</p></li>
</ul>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-5" class="exercise"><strong>Exercise 2.5  </strong></span>What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?</p>
<p><em>Answer</em>. The advantage of a very flexible approach is less bias, so the model better approximates reality, and typically better prediction. The disadvantage is higher variance and the potential for overfitting. A more flexible model might be preferred if the goal of statistical learning is prediction. A less flexible approach might be preferred if the goal of statistical learning is inference, as simpler models tend to be easier to understand.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-6" class="exercise"><strong>Exercise 2.6  </strong></span>Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?</p>
<p><em>Answer</em>. Parametric methods involve a two-step model-based approach: First we make an assumption about the functional form of <span class="math inline">\(f\)</span> (e.g., we could assume <span class="math inline">\(f\)</span> is linear). Second we <em>fit</em> (train) the model to our training data in order to estimate the parameters. The advantage of this is that it is easier to estimate a set of parameters than it is to fit an entirely arbitrary function <span class="math inline">\(f\)</span>. The disadvantage is that the model we choose will usually not match the true unknown form of <span class="math inline">\(f\)</span>, which can lead to poor estimates if we are too far from the true <span class="math inline">\(f\)</span>.</p>
<p>Non-parametric methods do not make any assumptions about the functional form of <span class="math inline">\(f\)</span>. Instead they try to find an estimate of <span class="math inline">\(f\)</span> that gets close to the data points without being too rough or wiggly. The advantage of this is that it becomes easier to accurately fit a wider range of possible shapes for <span class="math inline">\(f\)</span>. However, the disadvantage of this is that doing so accurately requires a much larger number of observations since the problem of estimating <span class="math inline">\(f\)</span> to a small number of parameters.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-7" class="exercise"><strong>Exercise 2.7  </strong></span>The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Obs.</th>
<th><span class="math inline">\(X_1\)</span></th>
<th><span class="math inline">\(X_2\)</span></th>
<th><span class="math inline">\(X_3\)</span></th>
<th><span class="math inline">\(Y\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>Red</td>
</tr>
<tr class="even">
<td>2</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>Red</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0</td>
<td>1</td>
<td>3</td>
<td>Red</td>
</tr>
<tr class="even">
<td>4</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>Green</td>
</tr>
<tr class="odd">
<td>5</td>
<td>-1</td>
<td>0</td>
<td>0</td>
<td>Green</td>
</tr>
<tr class="even">
<td>6</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>Red</td>
</tr>
</tbody>
</table></div>
<p>Suppose we wish to use this data set to make a prediction for <span class="math inline">\(Y\)</span> when <span class="math inline">\(X1 = X2 = X3 = 0\)</span> using K-nearest neighbours.</p>
<ol style="list-style-type: lower-alpha">
<li>Compute the Euclidean distance between each observation and the test point, <span class="math inline">\(X1 = X2 = X3 = 0\)</span>.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The equation for Euclidean distance is given by</p>
<p><span class="math display" id="eq:euclidean-distance">\[
d(p, q) = \sqrt{ \sum_{i = 1}^n (q_i - p_i)^2},
\tag{2.8}
\]</span></p>
<p>where <span class="math inline">\(d(p, q)\)</span> represents the distance between points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in <span class="math inline">\(n\)</span> dimensional Euclidean space, and <span class="math inline">\(q_i\)</span> and <span class="math inline">\(p_i\)</span> represent Euclidean vectors, starting from the origin of the <span class="math inline">\(n\)</span> dimensional Euclidean space.</p>
<p>This can be computed in R like so.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">euclidean_distance</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">q</span>, <span class="va">p</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">q</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>

<span class="va">obs_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>

<span class="va">obs_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
  obs_01 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">3</span>, <span class="fl">0</span><span class="op">)</span>,
  obs_02 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">2</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>,
  obs_03 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span>,
  obs_04 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>,
  obs_05 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>,
  obs_06 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>
<span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">obs_train</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">euclidean_distance</span><span class="op">(</span><span class="va">obs_test</span>, <span class="va">x</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; $obs_01</span>
<span class="co">#&gt; [1] 3</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $obs_02</span>
<span class="co">#&gt; [1] 2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $obs_03</span>
<span class="co">#&gt; [1] 3.162278</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $obs_04</span>
<span class="co">#&gt; [1] 2.236068</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $obs_05</span>
<span class="co">#&gt; [1] 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $obs_06</span>
<span class="co">#&gt; [1] 1.732051</span></code></pre></div>
<ol start="2" style="list-style-type: lower-alpha">
<li>What is our prediction with <span class="math inline">\(K = 1\)</span>? Why?</li>
</ol>
<p><em>Answer</em>. When <span class="math inline">\(K = 1\)</span> our prediction for the test point will be whatever training observation is closest to the test point. Observation 5 (Green) is closest so our prediction is that the test point is Green.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>What is our prediction with <span class="math inline">\(K = 3\)</span>? Why?</li>
</ol>
<p><em>Answer</em>. When <span class="math inline">\(K = 3\)</span> our prediction for the test point is given by the class with highest estimated probability based on the three points in the training data that are closest to the test point. Here observations 5 (Green), 6 (Red), and 2 (Red) are closest, which results in probabilities of 1/3 for the Green class and 2/3 for the Red class. So our prediction is that the test point is Red.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for <span class="math inline">\(K\)</span> to be large or small? Why?</li>
</ol>
<p><em>Answer</em>. We would expect a smaller value of K to give the best predictions. But not too small. When <span class="math inline">\(K = 1\)</span> the KNN decision boundary will be overly flexible, while with a large value of <span class="math inline">\(K\)</span> it will not be sufficiently flexible.</p>
</div>
</div>
<div id="applied" class="section level3 unnumbered">
<h3>Applied<a class="anchor" aria-label="anchor" href="#applied"><i class="fas fa-link"></i></a>
</h3>
<div class="exercise">
<p><span id="exr:unlabeled-div-8" class="exercise"><strong>Exercise 2.8  </strong></span>This exercise relates to the <code><a href="https://rdrr.io/pkg/ISLR2/man/College.html">ISLR2::College</a></code> data set. It contains a number of
variables for 777 different universities and colleges in the US. The help page <code><a href="https://rdrr.io/pkg/ISLR2/man/College.html">?ISLR2::College</a></code> for the data set contains a description of the data set and the 18 variables it measures.</p>
<ol style="list-style-type: lower-alpha">
<li>Load the <code><a href="https://rdrr.io/pkg/ISLR2/man/College.html">ISLR2::College</a></code> data set into R.</li>
</ol>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Make sure to preserve the row names with college names in them because they</span>
<span class="co"># might be useful later</span>
<span class="va">college</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/College.html">College</a></span>, rownames <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>
<span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">college</span><span class="op">)</span>
<span class="co">#&gt; Rows: 777</span>
<span class="co">#&gt; Columns: 18</span>
<span class="co">#&gt; $ Private     &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes~</span>
<span class="co">#&gt; $ Apps        &lt;dbl&gt; 1660, 2186, 1428, 417, 193, 587, 353, ~</span>
<span class="co">#&gt; $ Accept      &lt;dbl&gt; 1232, 1924, 1097, 349, 146, 479, 340, ~</span>
<span class="co">#&gt; $ Enroll      &lt;dbl&gt; 721, 512, 336, 137, 55, 158, 103, 489,~</span>
<span class="co">#&gt; $ Top10perc   &lt;dbl&gt; 23, 16, 22, 60, 16, 38, 17, 37, 30, 21~</span>
<span class="co">#&gt; $ Top25perc   &lt;dbl&gt; 52, 29, 50, 89, 44, 62, 45, 68, 63, 44~</span>
<span class="co">#&gt; $ F.Undergrad &lt;dbl&gt; 2885, 2683, 1036, 510, 249, 678, 416, ~</span>
<span class="co">#&gt; $ P.Undergrad &lt;dbl&gt; 537, 1227, 99, 63, 869, 41, 230, 32, 3~</span>
<span class="co">#&gt; $ Outstate    &lt;dbl&gt; 7440, 12280, 11250, 12960, 7560, 13500~</span>
<span class="co">#&gt; $ Room.Board  &lt;dbl&gt; 3300, 6450, 3750, 5450, 4120, 3335, 57~</span>
<span class="co">#&gt; $ Books       &lt;dbl&gt; 450, 750, 400, 450, 800, 500, 500, 450~</span>
<span class="co">#&gt; $ Personal    &lt;dbl&gt; 2200, 1500, 1165, 875, 1500, 675, 1500~</span>
<span class="co">#&gt; $ PhD         &lt;dbl&gt; 70, 29, 53, 92, 76, 67, 90, 89, 79, 40~</span>
<span class="co">#&gt; $ Terminal    &lt;dbl&gt; 78, 30, 66, 97, 72, 73, 93, 100, 84, 4~</span>
<span class="co">#&gt; $ S.F.Ratio   &lt;dbl&gt; 18.1, 12.2, 12.9, 7.7, 11.9, 9.4, 11.5~</span>
<span class="co">#&gt; $ perc.alumni &lt;dbl&gt; 12, 16, 30, 37, 2, 11, 26, 37, 23, 15,~</span>
<span class="co">#&gt; $ Expend      &lt;dbl&gt; 7041, 10527, 8735, 19016, 10922, 9727,~</span>
<span class="co">#&gt; $ Grad.Rate   &lt;dbl&gt; 60, 56, 54, 59, 15, 55, 63, 73, 80, 52~</span></code></pre></div>
<ol start="3" style="list-style-type: lower-alpha">
<li>Explore the <code>college</code> data.</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>Produce a numerical summary of the variables in the data set.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://docs.ropensci.org/skimr/reference/skim.html">skim</a></span><span class="op">(</span><span class="va">college</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:unnamed-chunk-3">Table 2.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">college</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">777</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">18</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">17</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table></div>
<p><strong>Variable type: factor</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">Private</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">Yes: 565, No: 212</td>
</tr></tbody>
</table></div>
<p><strong>Variable type: numeric</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Apps</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3001.64</td>
<td align="right">3870.20</td>
<td align="right">81.0</td>
<td align="right">776.0</td>
<td align="right">1558.0</td>
<td align="right">3624.0</td>
<td align="right">48094.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">Accept</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2018.80</td>
<td align="right">2451.11</td>
<td align="right">72.0</td>
<td align="right">604.0</td>
<td align="right">1110.0</td>
<td align="right">2424.0</td>
<td align="right">26330.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">Enroll</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">779.97</td>
<td align="right">929.18</td>
<td align="right">35.0</td>
<td align="right">242.0</td>
<td align="right">434.0</td>
<td align="right">902.0</td>
<td align="right">6392.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">Top10perc</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">27.56</td>
<td align="right">17.64</td>
<td align="right">1.0</td>
<td align="right">15.0</td>
<td align="right">23.0</td>
<td align="right">35.0</td>
<td align="right">96.0</td>
<td align="left">▇▇▂▁▁</td>
</tr>
<tr class="odd">
<td align="left">Top25perc</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">55.80</td>
<td align="right">19.80</td>
<td align="right">9.0</td>
<td align="right">41.0</td>
<td align="right">54.0</td>
<td align="right">69.0</td>
<td align="right">100.0</td>
<td align="left">▂▆▇▅▃</td>
</tr>
<tr class="even">
<td align="left">F.Undergrad</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3699.91</td>
<td align="right">4850.42</td>
<td align="right">139.0</td>
<td align="right">992.0</td>
<td align="right">1707.0</td>
<td align="right">4005.0</td>
<td align="right">31643.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">P.Undergrad</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">855.30</td>
<td align="right">1522.43</td>
<td align="right">1.0</td>
<td align="right">95.0</td>
<td align="right">353.0</td>
<td align="right">967.0</td>
<td align="right">21836.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">Outstate</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10440.67</td>
<td align="right">4023.02</td>
<td align="right">2340.0</td>
<td align="right">7320.0</td>
<td align="right">9990.0</td>
<td align="right">12925.0</td>
<td align="right">21700.0</td>
<td align="left">▃▇▆▂▂</td>
</tr>
<tr class="odd">
<td align="left">Room.Board</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4357.53</td>
<td align="right">1096.70</td>
<td align="right">1780.0</td>
<td align="right">3597.0</td>
<td align="right">4200.0</td>
<td align="right">5050.0</td>
<td align="right">8124.0</td>
<td align="left">▂▇▆▂▁</td>
</tr>
<tr class="even">
<td align="left">Books</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">549.38</td>
<td align="right">165.11</td>
<td align="right">96.0</td>
<td align="right">470.0</td>
<td align="right">500.0</td>
<td align="right">600.0</td>
<td align="right">2340.0</td>
<td align="left">▇▆▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">Personal</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1340.64</td>
<td align="right">677.07</td>
<td align="right">250.0</td>
<td align="right">850.0</td>
<td align="right">1200.0</td>
<td align="right">1700.0</td>
<td align="right">6800.0</td>
<td align="left">▇▃▁▁▁</td>
</tr>
<tr class="even">
<td align="left">PhD</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">72.66</td>
<td align="right">16.33</td>
<td align="right">8.0</td>
<td align="right">62.0</td>
<td align="right">75.0</td>
<td align="right">85.0</td>
<td align="right">103.0</td>
<td align="left">▁▁▅▇▅</td>
</tr>
<tr class="odd">
<td align="left">Terminal</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">79.70</td>
<td align="right">14.72</td>
<td align="right">24.0</td>
<td align="right">71.0</td>
<td align="right">82.0</td>
<td align="right">92.0</td>
<td align="right">100.0</td>
<td align="left">▁▁▃▆▇</td>
</tr>
<tr class="even">
<td align="left">S.F.Ratio</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">14.09</td>
<td align="right">3.96</td>
<td align="right">2.5</td>
<td align="right">11.5</td>
<td align="right">13.6</td>
<td align="right">16.5</td>
<td align="right">39.8</td>
<td align="left">▁▇▂▁▁</td>
</tr>
<tr class="odd">
<td align="left">perc.alumni</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">22.74</td>
<td align="right">12.39</td>
<td align="right">0.0</td>
<td align="right">13.0</td>
<td align="right">21.0</td>
<td align="right">31.0</td>
<td align="right">64.0</td>
<td align="left">▅▇▆▂▁</td>
</tr>
<tr class="even">
<td align="left">Expend</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">9660.17</td>
<td align="right">5221.77</td>
<td align="right">3186.0</td>
<td align="right">6751.0</td>
<td align="right">8377.0</td>
<td align="right">10830.0</td>
<td align="right">56233.0</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">Grad.Rate</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">65.46</td>
<td align="right">17.18</td>
<td align="right">10.0</td>
<td align="right">53.0</td>
<td align="right">65.0</td>
<td align="right">78.0</td>
<td align="right">118.0</td>
<td align="left">▁▅▇▅▁</td>
</tr>
</tbody>
</table></div>
<ol start="2" style="list-style-type: lower-roman">
<li>Produce a scatterplot matrix of the first ten columns or variables of the data.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">college</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://ggobi.github.io/ggally/reference/ggpairs.html">ggpairs</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span>
<span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with</span>
<span class="co">#&gt; `binwidth`.</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-4-1.png" width="672"></div>
<ol start="3" style="list-style-type: lower-roman">
<li>Produce side-by-side boxplots of <code>Outstate</code> versus <code>Private</code>.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">college</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Outstate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">Private</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Private universities receive more out of state tuition"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-5-1.png" width="672"></div>
<ol start="4" style="list-style-type: lower-roman">
<li>Create a new qualitative variable, called <code>Elite</code>, by binning the <code>Top10perc</code> variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. See how many elite universities there are, then produce side-by-side boxplots of <code>Outstate</code> versus <code>Elite</code>.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">college</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Elite <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/if_else.html">if_else</a></span><span class="op">(</span><span class="va">Top10perc</span> <span class="op">&gt;</span> <span class="fl">50</span>, <span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://forcats.tidyverse.org/reference/as_factor.html">as_factor</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">Outstate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">Elite</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Elite universities receive more out of state tuition"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-6-1.png" width="672"></div>
<ol start="22" style="list-style-type: lower-alpha">
<li>Produce some histograms with differing numbers of bins for a few of the quantitative variables.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">gghist</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">binwidth</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">college</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="op">{</span><span class="op">{</span> <span class="va">x</span> <span class="op">}</span><span class="op">}</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>binwidth <span class="op">=</span> <span class="va">binwidth</span><span class="op">)</span>
<span class="op">}</span>

<span class="fu">gghist</span><span class="op">(</span><span class="va">Enroll</span>, <span class="fl">300</span><span class="op">)</span> <span class="op">+</span> <span class="fu">gghist</span><span class="op">(</span><span class="va">Enroll</span>, <span class="fl">200</span><span class="op">)</span> <span class="op">+</span> <span class="fu">gghist</span><span class="op">(</span><span class="va">Enroll</span>, <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://patchwork.data-imaginist.com/reference/plot_annotation.html">plot_annotation</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"College enrollment follows a power law distribution"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-7-1.png" width="672"></div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="fu">gghist</span><span class="op">(</span><span class="va">PhD</span>, <span class="fl">20</span><span class="op">)</span> <span class="op">+</span> <span class="fu">gghist</span><span class="op">(</span><span class="va">PhD</span>, <span class="fl">10</span><span class="op">)</span> <span class="op">+</span> <span class="fu">gghist</span><span class="op">(</span><span class="va">PhD</span>, <span class="fl">5</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://patchwork.data-imaginist.com/reference/plot_annotation.html">plot_annotation</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"The majority of college faculty have a PhD"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-7-2.png" width="672"></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="fu">gghist</span><span class="op">(</span><span class="va">Books</span>, <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> <span class="fu">gghist</span><span class="op">(</span><span class="va">Books</span>, <span class="fl">50</span><span class="op">)</span> <span class="op">+</span> <span class="fu">gghist</span><span class="op">(</span><span class="va">Books</span>, <span class="fl">25</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://patchwork.data-imaginist.com/reference/plot_annotation.html">plot_annotation</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Books cost way too much at colleges"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-7-3.png" width="672"></div>
<ol start="6" style="list-style-type: lower-roman">
<li>Continue exploring the data, and provide a brief summary of what you discover.</li>
</ol>
<p><em>Answer</em>. Passing on this one.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-9" class="exercise"><strong>Exercise 2.9  </strong></span>This exercise involves the <code>Auto</code> data set.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html">Auto</a></span><span class="op">)</span></code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>Which of the predictors are quantitative, and which are qualitative?</li>
</ol>
<p><em>Answer</em>.</p>
<p>All variables but <code>name</code> are quantitative.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">auto</span><span class="op">)</span>
<span class="co">#&gt; Rows: 392</span>
<span class="co">#&gt; Columns: 9</span>
<span class="co">#&gt; $ mpg          &lt;dbl&gt; 18, 15, 18, 16, 17, 15, 14, 14, 14, 1~</span>
<span class="co">#&gt; $ cylinders    &lt;int&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8~</span>
<span class="co">#&gt; $ displacement &lt;dbl&gt; 307, 350, 318, 304, 302, 429, 454, 44~</span>
<span class="co">#&gt; $ horsepower   &lt;int&gt; 130, 165, 150, 150, 140, 198, 220, 21~</span>
<span class="co">#&gt; $ weight       &lt;int&gt; 3504, 3693, 3436, 3433, 3449, 4341, 4~</span>
<span class="co">#&gt; $ acceleration &lt;dbl&gt; 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9~</span>
<span class="co">#&gt; $ year         &lt;int&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~</span>
<span class="co">#&gt; $ origin       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~</span>
<span class="co">#&gt; $ name         &lt;fct&gt; chevrolet chevelle malibu, buick skyl~</span></code></pre></div>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>What is the range of each quantitative predictor?</p></li>
<li><p>What is the mean and standard deviation of each quantitative
predictor?</p></li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">auto</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">!</span><span class="va">name</span>, <span class="st">"predictor"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">predictor</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    min  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    max  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    sd   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>
  <span class="op">)</span>
<span class="co">#&gt; # A tibble: 8 x 5</span>
<span class="co">#&gt;   predictor      min    max    mean      sd</span>
<span class="co">#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;</span>
<span class="co">#&gt; 1 acceleration     8   24.8   15.5    2.76 </span>
<span class="co">#&gt; 2 cylinders        3    8      5.47   1.71 </span>
<span class="co">#&gt; 3 displacement    68  455    194.   105.   </span>
<span class="co">#&gt; 4 horsepower      46  230    104.    38.5  </span>
<span class="co">#&gt; 5 mpg              9   46.6   23.4    7.81 </span>
<span class="co">#&gt; 6 origin           1    3      1.58   0.806</span>
<span class="co">#&gt; 7 weight        1613 5140   2978.   849.   </span>
<span class="co">#&gt; 8 year            70   82     76.0    3.68</span></code></pre></div>
<ol start="4" style="list-style-type: lower-alpha">
<li>Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">auto</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html">slice</a></span><span class="op">(</span><span class="op">-</span><span class="fl">10</span><span class="op">:</span><span class="op">-</span><span class="fl">85</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">!</span><span class="va">name</span>, <span class="st">"predictor"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">predictor</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    min  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    max  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    sd   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>
  <span class="op">)</span>
<span class="co">#&gt; # A tibble: 8 x 5</span>
<span class="co">#&gt;   predictor       min    max    mean      sd</span>
<span class="co">#&gt;   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;</span>
<span class="co">#&gt; 1 acceleration    8.5   24.8   15.7    2.69 </span>
<span class="co">#&gt; 2 cylinders       3      8      5.37   1.65 </span>
<span class="co">#&gt; 3 displacement   68    455    187.    99.7  </span>
<span class="co">#&gt; 4 horsepower     46    230    101.    35.7  </span>
<span class="co">#&gt; 5 mpg            11     46.6   24.4    7.87 </span>
<span class="co">#&gt; 6 origin          1      3      1.60   0.820</span>
<span class="co">#&gt; 7 weight       1649   4997   2936.   811.   </span>
<span class="co">#&gt; 8 year           70     82     77.1    3.11</span></code></pre></div>
<ol start="5" style="list-style-type: lower-alpha">
<li>Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The plots below show relationships we would expect between the different variables based on the laws of physics. Some interesting observations:</p>
<ul>
<li>There appear to be two clusters of cars based on the weight by acceleration plot</li>
<li>The relationship between weight and mpg is nonlinear</li>
<li>Most cars have an even number of cylinders</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ggscatter</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">auto</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="op">{</span><span class="op">{</span> <span class="va">x</span> <span class="op">}</span><span class="op">}</span>, <span class="op">{</span><span class="op">{</span> <span class="va">y</span> <span class="op">}</span><span class="op">}</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span>
<span class="op">}</span>

<span class="fu">ggscatter</span><span class="op">(</span><span class="va">weight</span>, <span class="va">acceleration</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ggscatter</span><span class="op">(</span><span class="va">weight</span>, <span class="va">mpg</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://patchwork.data-imaginist.com/reference/plot_annotation.html">plot_annotation</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Lighter cars are more fuel efficient and accelerate faster"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-12-1.png" width="672"></div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="fu">ggscatter</span><span class="op">(</span><span class="va">cylinders</span>, <span class="va">displacement</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Cars with more cylinders have higher displacement"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-12-2.png" width="672"></div>
<ol start="6" style="list-style-type: lower-alpha">
<li>Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The plots below suggest that mpg has nonlinear relationships with at least a few of the variables in the data set (I didn’t explore them all). In particular, weight and horsepower show very clear relationships that might be suitable for making predictions, so long as we can deal with the heteroskedasticity.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">ggscatter</span><span class="op">(</span><span class="va">weight</span>, <span class="va">mpg</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ggscatter</span><span class="op">(</span><span class="va">acceleration</span>, <span class="va">mpg</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">ggscatter</span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">mpg</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ggscatter</span><span class="op">(</span><span class="va">displacement</span>, <span class="va">mpg</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-13-1.png" width="672"></div>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-10" class="exercise"><strong>Exercise 2.10  </strong></span>This exercise involves the <code>Boston</code> housing data set.</p>
<ol style="list-style-type: lower-alpha">
<li>To begin, load in the <code>Boston</code> data set. How many rows are in this data set? How many columns? What do the rows and columns represent?</li>
</ol>
<p><em>Answer</em>.</p>
<p>The columns represent variables that might be related to the value of houses in an area. Each row represents a suburb in Boston.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html">as_tibble</a></span><span class="op">(</span><span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Boston.html">Boston</a></span><span class="op">)</span>
<span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">boston</span><span class="op">)</span>
<span class="co">#&gt; Rows: 506</span>
<span class="co">#&gt; Columns: 13</span>
<span class="co">#&gt; $ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.0690~</span>
<span class="co">#&gt; $ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5,~</span>
<span class="co">#&gt; $ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, ~</span>
<span class="co">#&gt; $ chas    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~</span>
<span class="co">#&gt; $ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, ~</span>
<span class="co">#&gt; $ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, ~</span>
<span class="co">#&gt; $ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, ~</span>
<span class="co">#&gt; $ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.~</span>
<span class="co">#&gt; $ rad     &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, ~</span>
<span class="co">#&gt; $ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 31~</span>
<span class="co">#&gt; $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, ~</span>
<span class="co">#&gt; $ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43,~</span>
<span class="co">#&gt; $ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, ~</span></code></pre></div>
<ol start="2" style="list-style-type: lower-alpha">
<li>Make some pairwise scatterplots of the predictors (columns) in
this data set. Describe your findings.</li>
</ol>
<p><em>Answer</em>.</p>
<p>I explored some other scatterplots not shown below. In general there aren’t many easily interpretable relationships between variables in this data set. Some pairs have clusters of data, but no overall trend; others have floor effects where most communities have a value of zero (e.g., crime).</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ggscatter</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">boston</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="op">{</span><span class="op">{</span> <span class="va">x</span> <span class="op">}</span><span class="op">}</span>, <span class="op">{</span><span class="op">{</span> <span class="va">y</span> <span class="op">}</span><span class="op">}</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span>
<span class="op">}</span>

<span class="fu">ggscatter</span><span class="op">(</span><span class="va">chas</span>, <span class="va">crim</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Crime occurs more often away from the Charles River"</span>,
    subtitle <span class="op">=</span> <span class="st">"Or, most suburbs are away from the Charles River"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-15-1.png" width="672"></div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="fu">ggscatter</span><span class="op">(</span><span class="va">zn</span>, <span class="va">nox</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span>
      <span class="st">"Higher nitrogen oxide concentration in suburbs with all residential \n"</span>,
      <span class="st">"land zoning below 25,000 sq.ft."</span><span class="op">)</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-15-2.png" width="672"></div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="fu">ggscatter</span><span class="op">(</span><span class="va">ptratio</span>, <span class="va">medv</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span>
      <span class="st">"Higher pupil to teacher ratios are related to less \n"</span>,
      <span class="st">"valuable owner-occupied homes"</span>
    <span class="op">)</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-15-3.png" width="672"></div>
<ol start="3" style="list-style-type: lower-alpha">
<li>Are any of the predictors associated with per capita crime rate?
If so, explain the relationship.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The plots below are illustrative of the relationship with crime and other variables in the dataset. More or less, crime seems to occur in a specific range of the variable its being associated with.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">ggscatter</span><span class="op">(</span><span class="va">crim</span>, <span class="va">rad</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Suburbs with better access to radial highways have more crime"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-16-1.png" width="672"></div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="fu">ggscatter</span><span class="op">(</span><span class="va">crim</span>, <span class="va">tax</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Suburbs with higher property tax rates have more crime"</span>
  <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-statistical-learning_files/figure-html/unnamed-chunk-16-2.png" width="672"></div>
<ol start="4" style="list-style-type: lower-alpha">
<li>Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The plots above already get at these features of the data. The range of the three predictors show some interesting patterns too. Crime per capita has a very wide range, with some having almost none and others having a lot in comparison. Tax has a similarly wide range. Although pupil-teacher ratios have a skinnier range, the differences between the minimum and maximum would be quite meaningful in a classroom—a teacher in the minimum would theoretically have twice as much time for each student compared to a teacher in the maximum.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">crim</span>, <span class="va">tax</span>, <span class="va">ptratio</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span>, <span class="st">"predictor"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">predictor</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    min  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    max  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>
  <span class="op">)</span>
<span class="co">#&gt; # A tibble: 3 x 3</span>
<span class="co">#&gt;   predictor       min   max</span>
<span class="co">#&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1 crim        0.00632  89.0</span>
<span class="co">#&gt; 2 ptratio    12.6      22  </span>
<span class="co">#&gt; 3 tax       187       711</span></code></pre></div>
<ol start="5" style="list-style-type: lower-alpha">
<li>How many of the census tracts in this data set bound the Charles river?</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>bound <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/if_else.html">if_else</a></span><span class="op">(</span><span class="va">chas</span> <span class="op">==</span> <span class="fl">1</span>, <span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">bound</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    n <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span>
  <span class="op">)</span>
<span class="co">#&gt; # A tibble: 2 x 2</span>
<span class="co">#&gt;   bound     n</span>
<span class="co">#&gt;   &lt;chr&gt; &lt;int&gt;</span>
<span class="co">#&gt; 1 No      471</span>
<span class="co">#&gt; 2 Yes      35</span></code></pre></div>
<ol start="6" style="list-style-type: lower-alpha">
<li>What is the median pupil-teacher ratio among the towns in this data set?</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    median_ptratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">ptratio</span><span class="op">)</span>
  <span class="op">)</span>
<span class="co">#&gt; # A tibble: 1 x 1</span>
<span class="co">#&gt;   median_ptratio</span>
<span class="co">#&gt;            &lt;dbl&gt;</span>
<span class="co">#&gt; 1           19.0</span></code></pre></div>
<ol start="7" style="list-style-type: lower-alpha">
<li>Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.</li>
</ol>
<p><em>Answer</em>.</p>
<p>The suburb with the lowest median value of owner-occupied homes and the values of the other predictors for it are listed here.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/arrange.html">arrange</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/desc.html">desc</a></span><span class="op">(</span><span class="va">medv</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; Rows: 1</span>
<span class="co">#&gt; Columns: 13</span>
<span class="co">#&gt; $ crim    &lt;dbl&gt; 67.9208</span>
<span class="co">#&gt; $ zn      &lt;dbl&gt; 0</span>
<span class="co">#&gt; $ indus   &lt;dbl&gt; 18.1</span>
<span class="co">#&gt; $ chas    &lt;int&gt; 0</span>
<span class="co">#&gt; $ nox     &lt;dbl&gt; 0.693</span>
<span class="co">#&gt; $ rm      &lt;dbl&gt; 5.683</span>
<span class="co">#&gt; $ age     &lt;dbl&gt; 100</span>
<span class="co">#&gt; $ dis     &lt;dbl&gt; 1.4254</span>
<span class="co">#&gt; $ rad     &lt;int&gt; 24</span>
<span class="co">#&gt; $ tax     &lt;dbl&gt; 666</span>
<span class="co">#&gt; $ ptratio &lt;dbl&gt; 20.2</span>
<span class="co">#&gt; $ lstat   &lt;dbl&gt; 22.98</span>
<span class="co">#&gt; $ medv    &lt;dbl&gt; 5</span></code></pre></div>
<p>In comparison to the range of those values:</p>
<ul>
<li>
<code>crim</code> is at the upper end</li>
<li>
<code>zn</code> is at the minimum</li>
<li>
<code>indus</code> is around two-thirds up</li>
<li>The suburb does not bound the river</li>
<li>
<code>nox</code> is at the upper end</li>
<li>
<code>rm</code> is near the middle</li>
<li>
<code>age</code> is at the maximum</li>
<li>
<code>dis</code> is almost at the minimum</li>
<li>
<code>rad</code> is at the maximum</li>
<li>
<code>tax</code> is near the maximum</li>
<li>
<code>ptratio</code> is near the maximum</li>
<li>
<code>lstat</code> is around two-thirds up</li>
<li>
<code>medv</code> is at the minimum</li>
</ul>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span>, <span class="st">"predictor"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">predictor</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>
    min  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>,
    max  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span>
  <span class="op">)</span>
<span class="co">#&gt; # A tibble: 13 x 3</span>
<span class="co">#&gt;    predictor       min     max</span>
<span class="co">#&gt;    &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;</span>
<span class="co">#&gt;  1 age         2.9     100    </span>
<span class="co">#&gt;  2 chas        0         1    </span>
<span class="co">#&gt;  3 crim        0.00632  89.0  </span>
<span class="co">#&gt;  4 dis         1.13     12.1  </span>
<span class="co">#&gt;  5 indus       0.46     27.7  </span>
<span class="co">#&gt;  6 lstat       1.73     38.0  </span>
<span class="co">#&gt;  7 medv        5        50    </span>
<span class="co">#&gt;  8 nox         0.385     0.871</span>
<span class="co">#&gt;  9 ptratio    12.6      22    </span>
<span class="co">#&gt; 10 rad         1        24    </span>
<span class="co">#&gt; 11 rm          3.56      8.78 </span>
<span class="co">#&gt; 12 tax       187       711    </span>
<span class="co">#&gt; 13 zn          0       100</span></code></pre></div>
<p>It is unsurprising that this suburb has the lowest median value of owner-occupied homes. It is an older suburb with small residential lots, high traffic and density with a resultant increase in pollution and crime, most of the land is taken up by businesses, and high taxes. Most of these characteristics are not attractive to home buyers, so there likely isn’t much demand for homes in this suburb.</p>
<ol start="8" style="list-style-type: lower-alpha">
<li>In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.</li>
</ol>
<p><em>Answer</em>.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">rm</span> <span class="op">&gt;</span> <span class="fl">7</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html">count</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; # A tibble: 1 x 1</span>
<span class="co">#&gt;       n</span>
<span class="co">#&gt;   &lt;int&gt;</span>
<span class="co">#&gt; 1    64</span></code></pre></div>
<p>Several of these communities seem to be near each other, given the shared values between many of their variables. Most have low tax rates, low crime, smaller lots, medium to high pollution, and a high number of rooms per dwelling.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">boston</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">rm</span> <span class="op">&gt;</span> <span class="fl">8</span><span class="op">)</span>
<span class="co">#&gt; # A tibble: 13 x 13</span>
<span class="co">#&gt;      crim    zn indus  chas   nox    rm   age   dis   rad</span>
<span class="co">#&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;</span>
<span class="co">#&gt;  1 0.121      0  2.89     0 0.445  8.07  76    3.50     2</span>
<span class="co">#&gt;  2 1.52       0 19.6      1 0.605  8.38  93.9  2.16     5</span>
<span class="co">#&gt;  3 0.0201    95  2.68     0 0.416  8.03  31.9  5.12     4</span>
<span class="co">#&gt;  4 0.315      0  6.2      0 0.504  8.27  78.3  2.89     8</span>
<span class="co">#&gt;  5 0.527      0  6.2      0 0.504  8.72  83    2.89     8</span>
<span class="co">#&gt;  6 0.382      0  6.2      0 0.504  8.04  86.5  3.22     8</span>
<span class="co">#&gt;  7 0.575      0  6.2      0 0.507  8.34  73.3  3.84     8</span>
<span class="co">#&gt;  8 0.331      0  6.2      0 0.507  8.25  70.4  3.65     8</span>
<span class="co">#&gt;  9 0.369     22  5.86     0 0.431  8.26   8.4  8.91     7</span>
<span class="co">#&gt; 10 0.612     20  3.97     0 0.647  8.70  86.9  1.80     5</span>
<span class="co">#&gt; 11 0.520     20  3.97     0 0.647  8.40  91.5  2.29     5</span>
<span class="co">#&gt; 12 0.578     20  3.97     0 0.575  8.30  67    2.42     5</span>
<span class="co">#&gt; 13 3.47       0 18.1      1 0.718  8.78  82.9  1.90    24</span>
<span class="co">#&gt; # ... with 4 more variables: tax &lt;dbl&gt;, ptratio &lt;dbl&gt;,</span>
<span class="co">#&gt; #   lstat &lt;dbl&gt;, medv &lt;dbl&gt;</span></code></pre></div>
</div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="empty"></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Self-Study: An Introduction to Statistical Learning</strong>" was written by Michael McCarthy. It was last built on 2021-11-30.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
