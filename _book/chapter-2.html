<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Statistical Learning | Self-Study: An Introduction to Statistical Learning</title>
<meta name="author" content="Michael McCarthy">
<meta name="description" content="Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between \(Y\) and \(X = (X_1, X_2, \dots, X_p)\), which...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 2 Statistical Learning | Self-Study: An Introduction to Statistical Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between \(Y\) and \(X = (X_1, X_2, \dots, X_p)\), which...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2 Statistical Learning | Self-Study: An Introduction to Statistical Learning">
<meta name="twitter:description" content="Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between \(Y\) and \(X = (X_1, X_2, \dots, X_p)\), which...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Self-Study: An Introduction to Statistical Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About</a></li>
<li><a class="" href="chapter-1.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="active" href="chapter-2.html"><span class="header-section-number">2</span> Statistical Learning</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/tidytales/an-introduction-to-statistical-learning">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-2" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Statistical Learning<a class="anchor" aria-label="anchor" href="#chapter-2"><i class="fas fa-link"></i></a>
</h1>
<p>Chapter 2 formalizes the concept of statistical learning by introducing the general statistical model used for modelling the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span>, which can be written as</p>
<p><span class="math display" id="eq:gsm">\[
Y = f(X) + \epsilon.
\tag{2.1}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> represents the response variable in our data set</li>
<li>
<span class="math inline">\(X\)</span> represents the set of variables in our data set</li>
<li>
<span class="math inline">\(X_p\)</span> represents the <span class="math inline">\(p\)</span>th variable in our data set</li>
<li>
<span class="math inline">\(f(\dots)\)</span> represents a fixed but unknown function of its input(s)</li>
<li>
<span class="math inline">\(\epsilon\)</span> represents a random error term which is independent of <span class="math inline">\(X\)</span> and has mean zero</li>
</ul>
<p>The goal of statistical learning is to estimate <span class="math inline">\(f\)</span>. There are two main reasons for estimating <span class="math inline">\(f\)</span>: <em>prediction</em> and <em>inference</em>. Depending on whether our ultimate goal is prediction, inference, or some combination of the two, different methods for estimating <span class="math inline">\(f\)</span> may be appropriate. In general, there is a trade-off between prediction accuracy and model interpretability. Models that make more accurate predictions tend to be less interpretable, and models that are more interpretable tend to make less accurate predictions (although this is not always the case, due to the potential for <em>overfitting</em> in highly flexible models).</p>
<p>The methods we use to estimate <span class="math inline">\(f\)</span> can be characterized as either <em>parametric</em> or <em>non-parametric</em>. Parametric methods involve a two-step model-based approach: First we make an assumption about the functional form of <span class="math inline">\(f\)</span> (e.g., we could assume <span class="math inline">\(f\)</span> is linear). Second we <em>fit</em> (train) the model to our training data in order to estimate the parameters. Non-parametric methods do not make any assumptions about the functional form of <span class="math inline">\(f\)</span>. Instead they try to find an estimate of <span class="math inline">\(f\)</span> that gets close to the data points without being too rough or wiggly.</p>
<div id="prediction" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Prediction<a class="anchor" aria-label="anchor" href="#prediction"><i class="fas fa-link"></i></a>
</h2>
<p>Because the error term <span class="math inline">\(\epsilon\)</span> averages to zero, the general statistical model for predicting <span class="math inline">\(Y\)</span> from <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span> can be written as</p>
<p><span class="math display" id="eq:gsm-p">\[
\hat Y = \hat f(X).
\tag{2.2}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(\hat Y\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>
</li>
<li>
<span class="math inline">\(\hat f\)</span> represents our estimate for <span class="math inline">\(f\)</span>
</li>
</ul>
<p>When our goal is only to predict, we do not typically need to concern ourselves with the exact form of <span class="math inline">\(\hat f\)</span> provided that it accurately predicts <span class="math inline">\(Y\)</span>. The accuracy of <span class="math inline">\(\hat Y\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two sources of error: <em>reducible error</em> and <em>irreducible error</em>. The error in our model attributable to <span class="math inline">\(\hat f\)</span> is <em>reducible</em> because we can potentially improve the accuracy of <span class="math inline">\(\hat f\)</span> for estimating <span class="math inline">\(f\)</span> by using a more appropriate statistical learning technique. However, the error in our model attributable to <span class="math inline">\(\epsilon\)</span> is <em>irreducible</em> because <span class="math inline">\(Y\)</span> is also a function of <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(\epsilon\)</span> is independent of <span class="math inline">\(X\)</span>, so no matter how well we estimate <span class="math inline">\(f\)</span>, the variability associated with <span class="math inline">\(\epsilon\)</span> will still be present in our model. This variability may come from unmeasured variables that are useful for predicting <span class="math inline">\(Y\)</span>, or from unmeasurable variation. Irreducible error places an (often unknowable) upper bound on the accuracy of our prediction for <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="inference" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Inference<a class="anchor" aria-label="anchor" href="#inference"><i class="fas fa-link"></i></a>
</h2>
<p>When our goal is to understand the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1, X_2, \dots, X_p)\)</span>, we do need to concern ourselves with the exact form of <span class="math inline">\(\hat f\)</span>. Here the form of <span class="math inline">\(\hat f\)</span> can be used to identify:</p>
<ul>
<li>Which predictors are associated with the response</li>
<li>the direction (positive or negative) and form (simple or complex) of the relationship between the response and each predictor</li>
</ul>
</div>
<div id="assessing-model-accuracy" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Assessing Model Accuracy<a class="anchor" aria-label="anchor" href="#assessing-model-accuracy"><i class="fas fa-link"></i></a>
</h2>
<p>No one statistical learning approach performs better than all other approaches on all possible data sets. Because of this, care needs to be taken to choose which approach to use for any given data set to produce the best results. A number of important concepts arise when selecting a statistical learning approach for a specific data set:</p>
<ul>
<li>Measuring the quality of fit</li>
<li>The bias-variance trade-off</li>
</ul>
<div id="in-the-regression-setting" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> In the Regression Setting<a class="anchor" aria-label="anchor" href="#in-the-regression-setting"><i class="fas fa-link"></i></a>
</h3>
<p>In the regression setting, the most commonly used quality of fit measure for training data is the mean squared error <span class="math inline">\(\mathit{MSE}\)</span>, given by</p>
<p><span class="math display" id="eq:mse-training">\[
\mathit{MSE}_{\mathrm{training}} = \frac 1 n \sum_{i = 1}^n (y_i - \hat f(x_i))^2,
\tag{2.3}
\]</span></p>
<p>where <span class="math inline">\(\hat f(x_i)\)</span> represents the prediction that <span class="math inline">\(\hat f\)</span> gives for the <span class="math inline">\(i\)</span>th observation. When predicted responses are very close to true responses the <span class="math inline">\(\mathit{MSE}\)</span> will be small; When predicted responses are very far from true responses the <span class="math inline">\(\mathit{MSE}\)</span> will be large. We generally do not really care about this value because accurately predicting data we have already seen is not particularly useful.</p>
<p>when our goal is to assess the accuracy of predictions when we apply our method to previously unseen <em>test data</em>, we can compute the mean squared error for test observations, given by</p>
<p><span class="math display" id="eq:mse-testing">\[
\mathit{MSE}_{\mathrm{testing}} = \frac 1 n \sum_{i = 1}^n (y_0 - \hat f(x_0))^2,
\tag{2.4}
\]</span></p>
<p>where <span class="math inline">\((x_0, y_0)\)</span> is a previously unseen test observation not used to train the statistical learning model. We want to choose the model that gives the lowest <em>test</em> <span class="math inline">\(\mathit{MSE}\)</span> by minimizing the distance between <span class="math inline">\(\hat f(x_0)\)</span> and <span class="math inline">\(y_0\)</span>. When a test data set is available then we can simply evaluate Equation <a href="chapter-2.html#eq:mse-testing">(2.4)</a> and choose the statistical learning model where <span class="math inline">\(\mathit{MSE}\)</span> is the smallest. When a test data set is not available then we can use <em>cross-validation</em>, which is a method for estimating test <span class="math inline">\(\mathit{MSE}\)</span> using the training data set.</p>
<p>The expected test <span class="math inline">\(\mathit{MSE}\)</span> for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: the <em>variance</em> of <span class="math inline">\(\hat f(x_0)\)</span>, the squared <em>bias</em> of <span class="math inline">\(\hat f(x_0)\)</span>, and the variance of the error term <span class="math inline">\(\epsilon\)</span>, written as</p>
<p><span class="math display" id="eq:expected-test-mse">\[
E \bigl(y_0 - \hat f(x_0) \bigr)^2 = \mathrm{Var}(\hat f(x_0)) +
                                     [\mathrm{Bias(\hat f(x_0))}]^2 +
                                     \mathrm{Var}(\epsilon),
\tag{2.5}
\]</span></p>
<p>where the notation <span class="math inline">\(E \bigl(y_0 - \hat f(x_0) \bigr)^2\)</span> defines the expected test <span class="math inline">\(\mathit{MSE}\)</span> at <span class="math inline">\(x_0\)</span>. The overall expected test <span class="math inline">\(\mathit{MSE}\)</span> is given by averaging <span class="math inline">\(E \bigl(y_0 - \hat f(x_0) \bigr)^2\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test data set.</p>
<p>The variance of <span class="math inline">\(\hat f\)</span> refers to the amount by which <span class="math inline">\(\hat f\)</span> would change if we estimated it using a different training set. The bias of <span class="math inline">\(\hat f\)</span> refers to the error that is introduced by approximating a real-life problem with a much simpler model. In general, as models become more flexible, the variance will increase and the bias will decrease. The relative rate of change of the variance and bias determines whether the test <span class="math inline">\(\mathit{MSE}\)</span> increases or decreases. Because the variance and bias can change at different rates in different data sets, the challenge lies in finding a model for which both the variance and bias are lowest.</p>
</div>
<div id="in-the-classification-setting" class="section level3" number="2.3.2">
<h3>
<span class="header-section-number">2.3.2</span> In the Classification Setting<a class="anchor" aria-label="anchor" href="#in-the-classification-setting"><i class="fas fa-link"></i></a>
</h3>
<p>In the classification setting, the most commonly used quality of fit measure for training data is the <em>error rate</em>, given by</p>
<p><span class="math display" id="eq:er-training">\[
\frac 1 n \sum_{i = 1}^n I(y_i \ne \hat y_i).
\tag{2.6}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(\hat y_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat f\)</span>
</li>
<li>
<span class="math inline">\(I(y_i \ne \hat y_i)\)</span> is an indicator variable that equals one if <span class="math inline">\(y_i \ne \hat y_i\)</span> (incorrectly classified) and zero if <span class="math inline">\(y_i = \hat y_i\)</span> (correctly classified)</li>
</ul>
<p>The <em>test error rate</em> for a set of test observations <span class="math inline">\((x_0, y_0)\)</span> is given by</p>
<p><span class="math display" id="eq:er-testing">\[
\frac 1 n \sum_{i = 1}^n I(y_0 \ne \hat y_0),
\tag{2.7}
\]</span></p>
<p>where <span class="math inline">\(\hat y_0\)</span> is the predicted class label that results from applying the classifier to the test observation with predictor <span class="math inline">\(x_0\)</span>.</p>
<p>The bias-variance trade-off is present in the classification setting too; when variance and bias are lowest then the test error rate will be at its smallest for a given data set.</p>
</div>
</div>
<div id="exercises" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises"><i class="fas fa-link"></i></a>
</h2>
<div id="conceptual" class="section level3 unnumbered">
<h3>Conceptual<a class="anchor" aria-label="anchor" href="#conceptual"><i class="fas fa-link"></i></a>
</h3>
<div class="exercise">
<p><span id="exr:unlabeled-div-1" class="exercise"><strong>Exercise 2.1  </strong></span>For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.</p>
<ol style="list-style-type: lower-alpha">
<li>The sample size <span class="math inline">\(n\)</span> is extremely large, and the number of predictors <span class="math inline">\(p\)</span> is small.</li>
</ol>
<p><em>Answer</em>. Better. A flexible model will generally be able to better estimate the the true <span class="math inline">\(f\)</span> and avoid overfitting when we have an extremely large sample size and a small number of predictors. The only exception would be if the true <span class="math inline">\(f\)</span> is linear, then an inflexible model would generally perform better; however, most real world relationships are not linear, so the lower bias of a flexible model will generally lead to a better quality of fit.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>The number of predictors <span class="math inline">\(p\)</span> is extremely large, and the number of observations <span class="math inline">\(n\)</span> is small.</li>
</ol>
<p><em>Answer</em>. Worse. A flexible model will generally lead to overfitting of our training data when the number of predictors is large and the sample size is small. An inflexible model is less likely to lead to overfitting in this scenario, so it will generally do a better job of giving accurate predictions on new observations than the flexible but overfit model.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>The relationship between the predictors and response is highly non-linear.</li>
</ol>
<p><em>Answer</em>. Better. A flexible model will generally be able to fit a highly non-linear relationship better than an inflexible model because the relative rate of decrease in bias tends to be much greater than the relative increase in variance when <span class="math inline">\(f\)</span> is highly non-linear. The left and right plots in Figure 2.12 on Page 36 of the book demonstrate this nicely.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>The variance of the error terms, i.e.Â <span class="math inline">\(\sigma^2 = \mathrm{Var}(\epsilon)\)</span>, is extremely high.</li>
</ol>
<p><em>Answer</em>. Worse. A flexible model will generally lead to overfitting of our training data when the variance of the error terms is extremely high. Because <span class="math inline">\(Y\)</span> is partly a function of <span class="math inline">\(\epsilon\)</span>, when the variance of the error terms is extremely high then the variance of <span class="math inline">\(Y\)</span> will also be extremely high, mainly due to random error. A flexible model that tries to find patterns in this noise is more likely to pick up on patterns that are caused by random chance rather than true properties of the unknown function <span class="math inline">\(f\)</span>. The bias of an inflexible model is preferable in this situation, as it will give more stable predictions in the long run, which is likely preferable to making the essentially random predictions a flexible model would give in these circumstances.</p>
</div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="chapter-1.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-2"><span class="header-section-number">2</span> Statistical Learning</a></li>
<li><a class="nav-link" href="#prediction"><span class="header-section-number">2.1</span> Prediction</a></li>
<li><a class="nav-link" href="#inference"><span class="header-section-number">2.2</span> Inference</a></li>
<li>
<a class="nav-link" href="#assessing-model-accuracy"><span class="header-section-number">2.3</span> Assessing Model Accuracy</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#in-the-regression-setting"><span class="header-section-number">2.3.1</span> In the Regression Setting</a></li>
<li><a class="nav-link" href="#in-the-classification-setting"><span class="header-section-number">2.3.2</span> In the Classification Setting</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#exercises"><span class="header-section-number">2.4</span> Exercises</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#conceptual">Conceptual</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/tidytales/an-introduction-to-statistical-learning/blob/master/02-statistical-learning.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/tidytales/an-introduction-to-statistical-learning/edit/master/02-statistical-learning.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Self-Study: An Introduction to Statistical Learning</strong>" was written by Michael McCarthy. It was last built on 2021-11-15.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
